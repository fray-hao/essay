强化学习（Reinforcement Learning）：
- MDPs：马尔科夫决策过程
- Value Function：价值函数
- Value Iteration：值迭代
- Policy iteration：政策迭代

强化学习是介于监督学习与无监督学习之间的一类算法。

例如训练直升机飞行。使用强化算法，每秒运行10次。直升机上的传感器提供了直升机的方向的位置非常准确的估计。所以在所有时间点，你非常准确知道直升机在哪个地方。你的工作是使用定位作为输入，并输出相对应的控制杆的移动位置的数据。这和监督学习不同，因为我们通常不知道正确的控制动作是什么。更具体的说，如果这架直升机位于某个位置的方向，很难说应该把控制杆移动到哪个精确的位置。因此，很难对这种问题，使用监督学习算法。因为我们不能给出一个训练集，使它的位置作为输入，并且输出是所有正确的控制行为。

取而代之的是强化学习，我们将提供给学习算法不同类型的反馈。基本上是所谓的奖励信号。它会告诉直升机，什么时候它的表现很好，什么时候表现不佳。所以我们要做的是提出所谓的奖励信号。这个奖励信号用来测量直升机是否飞行的好，我们的算法以这个奖励函数为输入，并尝试飞的很好。

强化学习的另一个很好的例子就是考虑做一个玩游戏的程序：下棋。在游戏中的任何阶段，我们不知道最佳的举动是什么。所以很难构成一个下棋的监督学习问题。但我们确实知道的是：如果你让一台电脑下国际象棋游戏，我们知道它赢得了比赛还是失去了比赛，所以我们要做的是给它一个奖励信号，赢得一盘棋，就给它一个积极的奖励，输了时就给它一个惩罚，并希望随着时间的推移，它本身要学会赢越来越多的游戏。

可以把强化学习认为是训练狗，每当你的狗做的好就告诉它“好狗”，每次坏事就告诉它“坏狗”，随着时间的推移，你的狗会越来越多的做好事情。直升机训练方式类似。

强化学习比监督学习更难的原因是，这不是一个一次性决策过程。在监督学习中，如果你有一个分类，预测有人是否患有癌症，你做一个预测，然后就大功告成了。在强化学习中，必须随着时间的推移一直采取行动。因此它被称为渐进的决策过程。具体来说，假设一个程序走到第60步，输了1盘棋，那么它实际上在输了这盘棋之前，它移动了60步，使得它很难让算法来学习。这就是所谓的信贷分配问题。在第60步时输了棋，我们不是很确定它所有的移动中，哪些是正确的移动，哪些是坏的移动。
在直升机训练中的也是同样的问题，直升机坠毁了，我们不能确定到底是哪个操作导致最终坠毁，也许是1分钟前的某个操作，最终导致直升机坠毁，对此我们不能确定。

在强化学习中，我们面对的是渐进的决策过程，我们需要作出许多决定（decision），并且你的决定也许会有长远的后果。
## MDP
强化学习问题的标准化：强化学习是使用MDP，即马氏决策（Markov Decision Process）对问题进行建模的形式体系（formalism）。MDP是一个五元组（five tuple），由五个集合组成，可以表示为$(S,A,{P_{sa}},\gamma,R)$，其中：
- S： set of state。状态的集合。比如在直升机无人驾驶例子中，S就是直升机可能的位置和方向集合
- A： set of actions：比如在直升机无人驾驶例子中，A就是操控直升机可能的动作集合，比如上下左右等。
- $P_{sa}$:state transition distribution。状态转换分布。$P_{sa}$指的是在状态s采取行动a的概率分布。
  $$
  \sum_{s'}P_{sa}(s')=1,P_{sa}(s')\geq0
  $$
- $\gamma$:discount facotr. 折扣因子
  $$0\leq\gamma<1$$
- R:reward function。奖励函数。它与状态集对应，都是实数。R表示在状态s采取行动a时能获得的奖励。

让我举一个MDP的具体的例子：机器人导航任务
> 人工智能：一种现代方法


机器人生活在一个网格世界中，阴暗的单元是障碍，机器人不能通过：
![](https://raw.githubusercontent.com/fray-hao/images/master/20190523135233.png)

比如，我想让机器人到达右上侧的单元，所以，要用+1的奖励来关联那个单元。它下侧的那个单元是要避免的单元，用-1来奖励，去关联那个单元：
![](https://raw.githubusercontent.com/fray-hao/images/master/20190523135611.png)

我们实际上要遍历MDP的那5个元素，要看看对于这个问题，它们是什么。

机器人可以在11个位置中的任何一个中，所以有11个状态的MDP：
$$S=\{s_0,...,s_{10}\}$$
可以往四个方向移动，所以有四个动作：
$$
A=\{N,S,E,W\}
$$
机器人的动力有噪杂（dynamics are noisy）。比如命令一个机器人去北方，由于车轮打滑或逆的行为的核心设计，机器人可能会走偏方向。所以，你命令机器人向前移动1米，它通常移动介于95-105厘米之间。因此，在这个高度简化的网格世界中，我要去建立机器人的随机动态模型。在指挥机器人向北移动时，它有10%的概率向左便宜，10%的概率向右偏移，只有0.8机会会向你指挥的方向移动。具体来说，我们需要写下状态转换概率。例如，我们使用(3,1)的状态，在这个状态下指挥机器人北上时，状态转移概率如下：
$$
\begin{aligned}
    &P_{(3,1)N}((3,2)) = 0.8\\
     &P_{(3,1)N}((2,1)) = 0.1\\
      &P_{(3,1)N}((4,1)) = 0.1\\
       &P_{(3,1)N}((3,3)) = 0\\
      &...
\end{aligned}
$$
我们先不管$\gamma$。来看一下奖励函数。到达(4,3)的奖励是+1，到达(4,2)的奖励是-1。在导航任务中，一个普遍的做法是为其他状态关联一个小的负面奖励（负报酬），可以理解为电池消耗或燃料消耗。对于这样一个小的负面奖励，使机器人可以随意跑动,促使系统计算解决方案不要浪费时间，并且使其尽快达到目标，因为它需要支付燃料消耗费用。
$$
\begin{aligned}
      &  R((4,3))=+1
    \\&R((4,2))=-1
    \\& R(s) = -0.02,\qquad \text{for all other}
\end{aligned}
$$

实际上还存在一个其他的复杂情况，在这个例子中，当得到+1或-1时，处理过程结束，在这之后，没有更多的正的或负的奖励了。这里有许多种方式来建立模型。方式之一是，你可以想象这里存在12个被称为零成本吸收状态（zero cost absorbing states）。每当你得到+1或-1，然后你过渡到这12个状态之一，零成本吸收状态没有回报，你将永远留在该状态。

让我们来看看MDP是如何工作的，它的工作原理如下：
1. 在时间0时，机器人从初始状态$s_0$出发，
2. 取决于你要去的方向，选择一个动作$a_0$,
3. 根据你选择的行动方向，得到一个状态过渡概率$P_{s_0a_0}$，我们的状态按照这个概率转移到下一个状态$s_1$
4. 在状态状态$s_1$下，选择一个新的行动$a_1$,根据$P_{s_1a_1}$的概率，转移到下一个状态$s_3$，依次类推，我们可以把这一过程用下图表示：
$$
s_{\tiny0}\xrightharpoonup{a_0}s_{\tiny1}\xrightharpoonup{a_1}s_{\tiny2}\xrightharpoonup{a_2}s_{\tiny3}\xrightharpoonup{a_3}...
$$
在遍历所有的状态和行动后，整个过程的奖励为：
$$
R(s_{\tiny0},a_{\tiny0})+\gamma R(s_{\tiny1},a_{\tiny1})+\gamma^2 R(s_{\tiny2},a_{\tiny2})+...
$$
如果我们把奖励函数当作只和状态相关，那么可以简化为：
$$
R(s_{\tiny0})+\gamma R(s_{\tiny1})+\gamma^2 R(s_{\tiny2})+...
$$
> $\gamma$是一个小于1的数，$0<\gamma\leq1$,通常是类似0.99的数，它表示在时间1获得的奖励略小于在时间0获得的奖励，然后在时间2获得的奖励又略小于在之前的时间1获得的奖励。$\gamma$可以解释为美元货币的时间价值，因为美元今天的价值比明天美元价值略多，因为美元在银行可以赚一点点的利息，与此相比，明天支付1美元，比今天支付1美元更好。所以换句话说，这种compacted gamma用于weight将来的奖励或亏损要比现在的奖励或亏损的代价小。

## 强化学习的目标
强化学习算法是随着时间的推移而选择行为，我们强化学习的目标就是选择一组行动使得整个奖励函数的期望最大化：
$$
E\bigg[ R(s_{\tiny0})+\gamma R(s_{\tiny1})+\gamma^2 R(s_{\tiny2})+... \bigg]
$$
> 注意奖励的值随着时刻t被$γ^t$的因子衰减。因此为了获得最大的奖励，我们需要尽早获得正向的奖励或者推迟获得负向的奖励。在经济学领域中，R表示获得的金钱，γ可以被自然的解释为“利率”（可以解释为今天的美元比明天的值钱）。
> 
我们会尽量使我们的强化学习算法，计算出一个策略。策略(policy)是指任意一个从状态到行动的映射函数π: S → A。当我们在状态s时执行某个策略π时，我们采取的行动a = π(s)。下面就是一个策略的例子：
![](https://raw.githubusercontent.com/fray-hao/images/master/20190527092950.png)
策略是一个从状态到行动的映射。所以这个策略告诉我们，当你再某种状态下的时候，你应该采取什么样的行动。例如状态为(3,3)时，策略告诉我们应该向左边行动。执行这样的策略将最大限度地提高你的总收益的预期价值。这将最大限度的提高预期的奖励的总和。

那么，如何来计算最优的策略呢？
我们首先定义三个表示符号：$V^\pi,V^*,\pi^*$

对于给定的任何策略$\pi$,定义它的值函数为$V^\pi$，表示状态的值的映射函数。这样$V^\pi(s)$的预期收益是从状态s开始执行策略$\pi$的预期总收益：
$$
V^\pi(s)= E\bigg[ R(s_{\tiny0})+\gamma R(s_{\tiny1})+\gamma^2 R(s_{\tiny2})+...\ \bigg|\ s_{\tiny0}=s,\pi \bigg]
$$
我们来看一下具体的例子：
![](https://raw.githubusercontent.com/fray-hao/images/master/20190527104232.png)
> 这是许多策略中一个比较糟糕的策略，对于许多状态来说，走向的都是-1。

该策略对应的价值函数是：
![](https://raw.githubusercontent.com/fray-hao/images/master/20190527104450.png)
底下两行的状态的值都是负数，因此，在这种期望中，你的总收益将是负数。但是如果是从第一行开始，总收益将是正的。这并不是一个坏的策略。因此，给定任何一个策略，你可以写下这个策略的价值函数。

价值函数$V^\pi$可以改写为：
$$
\begin{aligned}
    V^\pi(s) = E\bigg[\underbrace{R(s_{\tiny0})}_{\text{immediate reward}} +\gamma\Big(\underbrace{  R(s_{\tiny1})+\gamma R(s_{\tiny2})+...}_{\text{future reward}}\Big)\ \bigg|\ s_{\tiny0}=s,\pi \bigg]
\end{aligned}
$$
即使奖励是零状态开始时立即得到的奖励；未来奖励是未来一段时间的选择后得到的奖励，而这个未来奖励实质上是$V^\pi(s_{\tiny1})$值函数。
将$s_{\tiny0}$映射为s,$s_{\tiny1}$映射为s':
$$
s_{\tiny0}\rightarrow s, s_{\tiny1}\rightarrow s'
$$
然后用概率形式来表示这个公式：
$$
V^\pi(s) = R(s)+\gamma\sum_{s'\in S}P_{s\ \pi(s)}(s')V^\pi(s')
$$

>上式表明了$V^π$包含了两部分：第一部分是初始状态下的立即奖励R(s)，第二部分是后续状态下的总奖励。我们更详细地考察下第二部分，这个总奖励可以被写成：
$$
E_{s'\sim P_{s\pi(s)}}\big[V^\pi(s')\big]
$$
其中s′是初始状态s之后的下一个状态，因此整个第二部分是指初始状态为s′下的后续所有奖励之和。

这个方程被称为贝尔曼方程（Bellman's equation）,是解决MDP问题时将要经常用到的主要方程之一。为我们提供了一种解决策略价值函数的封闭形式的方式。假定有一个固定的策略，那么贝尔曼方程就是在价值函数上施加了线性约束集。对于一个给定的状态的价值函数值= 某个常数（立即奖励）+其他值的线性函数。所以，可以在MDP中，为每一个状态写下方程，然后为价值函数施加上线性约束。最后，通过求解线性系统方程就可以解出价值函数$V^\pi(s)$。
以（3,l）状态为例,它有一个固定的策略，有一个朝上北的行动。对于这项策略，贝尔曼告诉我们：
$$
V^\pi((3,1)) = R((3,1))+\gamma\bigg[ 0.8V^\pi((3,1))+0.1V^\pi((4,1))+0.1V^\pi((2,1))  \bigg]
$$
要解开$V^\pi((3,1))$,需要先解开$V^\pi((3,2)),V^\pi((4,1)),V^\pi((3,1)),V^\pi((2,1))$，这些都是未知数。
(3,1)只是机器人网格世界中的一个状态。还有其它10个状态，这11个状态总共有11个未知变量。因此想解出11个状态的价值函数，将有11个限制。通过这11个线性方程和11个未知量，可以解出这个线性系统的方程组，得到一个明确的$V^\pi$
上面就是具体策略的价值函数如何解开的讲解。接下来我们学习最优价值函数（optimal value function）$V^*(s)$：
$$
V^*(s) = \max_\pi V^\pi(s)
$$
最优价值函数等于所有策略p中最大的$V\pi(s)$。话句话说，对于任何给定的状态s，最优价值函数就是在所有可能的策略p中选择最好的预期。
对于最优价值函数，也有对应版本的贝尔曼方程：
$$
V^*(s) = R(s)+\max_a\gamma\sum_{s'}P_{sa}(s')V^*(s')
$$

而这也引出了$\pi^*$的定义。比如说，在状态s时，我想知道采用什么样的行动。因为在状态s时，将立即得到R(s),所以选择的的最佳行动是能够使价值函数最大化的第二部分，即：
$$
\pi^*=\arg\max_a\sum_{s'}P_{sa}(s')V^*(s')
$$
> $\gamma$应为总是一个正数，只是一个恒定的规模（constant scale），不会影响$\arg\max_a$

从上面的三个定义可以知道，如果能够计算出$V^*$，就能够计算出最优价值函数，并且计算出最佳策略。我们知道$V^\pi$是一个线性方程组，可以解开$V^\pi$,但是，这里有指数级别的策略（每一个状态都可以有4中策略，那就是$11^4$种策略），所以，不能穷尽策略的组合，然后求出最大的$V^*$。

## 价值迭代
求解MDP的第一个方法称为价值迭代(value iteration)，步骤如下：
1. 对每一个状态s，初始化V(s) :=0
   $$V(s)=0\qquad \forall s$$
2. 对于每个状态s,更新V(s)：
    $$
    V(s):= R(s)+ \max_a\gamma\sum_{s'}P_{sa}(s')V(s')
    $$
步骤2，需要重复直到收敛。最终V(s)将收敛到$V^*(s)$。步骤2可以看做是不断地用贝尔曼等式去更新价值函数。第2步的循环可以有两种不同的更新方式。第一种方法，我们可以为每个状态s计算V(s)，然后一次性把新的V(s)值替换旧值，这种更新称为是同步(synchronous)的。这种情况下，这个算法可以认为是实现了一个贝尔曼算子(Bellman backup operator)，贝尔曼算子将当前价值函数映射到更接近最优值的一个估计。另一种更新方法称为异步(asynchronous)更新，这种情况下每次计算出V(s)后就立刻进行更新。

不管是同步还是异步的更新，价值迭代都会使得V不断收敛到V*。如我们的示例，计算出$V^*$:
![](https://raw.githubusercontent.com/fray-hao/images/master/20190528091213.png)
得到$V^*$后我们通过$\pi^*$就能求出最佳策略:
![](https://raw.githubusercontent.com/fray-hao/images/master/20190527092950.png)
我们来解释下为什么在(3,1)时选择向西行动。
向西时：
$$
\begin{aligned}
    &W:\sum_{s'}P_{sa}(s')V^*(s')
    \\&=0.8\times0.75+0.1\times0.69+0.1\times0.71\quad (0.1\times\text{0.71表示向南走，会撞墙，而原地不动的概率})
    \\& = 0.74
\end{aligned}
$$
向北时：
$$
\begin{aligned}
    &N:\sum_{s'}P_{sa}(s')V^*(s')
    \\&=0.8\times0.69+0.1\times0.75+0.1\times0.49
    \\& = 0.676
\end{aligned}
$$
向西行动比向北行动得到的预期总收益更高，这就是为什么在这种状态下的最佳行动是向西行动
> MDP能很好的找到最短路线与最冒险路线之间的平衡，采取的是折中方案
## 策略迭代
求解MDP的第二个方法称为策略迭代(policy iteration)，步骤如下：
1. 随机初始化一个策略π
2. 重复如下操作直到收敛:
   a. 令$V := V^π$
   b. 对于每个状态s，更新π(s):  
   $$
   \pi(s):=\arg\max_a\sum_{s'}P_{sa}(s')V(s')
   $$

在第2个步骤中，a解决的问题是： 采取的现行策略$\pi$,并且解出Bellman方程，获得$V^{\pi}$。b是更新策略，话句话说， 假设a中解出的价值函数就是最优的值，那么就更新策略。

> 算法的第2步不断地根据当前策略求出价值函数，然后根据当前价值函数更新策略。其中第2步中的(a)可以用之前提到的**求解方程组**获得。

在算法经过一定次数的迭代后，V会不断收敛到$V^*$，π会不断收敛到$π^*$。

价值迭代和策略迭代都是用于求解MDP的常规算法，目前没有一个统一的定论说哪个更好。对于规模较小的MDP，由于状态有限，线性方程组解起来比较快，策略迭代收敛更快。对于规模较大的MDP，由于求解方程组的开销太大，价值迭代性能更好。因此在实践中，通常选取的算法是价值迭代 。
### 未知的状态转换概率和奖励函数
到目前为止，我们都是在已知状态转换概率和奖励函数的情况下讨论MDP算法。但在实际情况中，通常状态集合、行动集合和折扣因子是已知的，但状态转换概率和奖励函数很可能是未知的。
例如，你试图飞直升机。你不可能提前（in advance）知道你的直升机将过渡到什么状态，并在一定状态下采取什么行动。因为直升机动力很noisy。你真的不知道最终会达到什么状态。因此，要做的标准的事是尝试从数据来估计状态转移概率。
MDP有五元组，状态空间（state space）S是我们定义的，动作A是我们能采取的行动的集合，贴现因子$\gamma$由当前与未来的多少而定，我们自己进行权衡，奖励函数也通常都知道，特殊情况下可能不知道。状态转移概率通常是未知的。
在状态转移概率是未知的情况下，通常的做法是估计数据，想象一下机器人在走廊上漫游，就像那个网格的例子。你让机器人在MDP中采取行动，并且你会估计你的状态转移概率：
$$
P_{sa}(s')=\frac{\text{\#times took we action a in state s and go to s'}}{\text{\#times we took action in state s}}
$$
>状态s下采取a行动并到达s'的次数/状态s下采取a行动的次数

在某一状态s下如果没有采取过行动a，那么这个比率是“0/0”，以防万一，我们可以用$\frac{1}{|s|}$进行估计，也就是所有状态出现的平均值。 

如果我们实验运行的次数足够多，分子和分母可以用所有实验中出现次数的累加值，这样算出来的比率更接近状态转换概率的真实情况。
类似的，如果奖励函数R未知，我们可以把R(s)的值用状态s下的平均奖励作近似估计。
在建立了模型参数后，我们就可以用价值迭代或者策略迭代求解MDP问题了。将两者结合起来，下面描述了在状态转换概率未知的情况下求解MDP问题的步骤：
1. 随机初始化一个策略π
2. 重复如下操作直到收敛:
   a. 用策略π进行若干次MDP实验
   b. 根据实验数据，更新$P_{sa}$的值。
   c. 通过价值迭代求解贝尔曼方程，得到新的$V$
   d. 根据得到的V，执行策略迭代中第2步中的(b)，从而更新策略：
    $$
        \pi(s):=\arg\max_a\sum_{s'}P_{sa}(s')V(s')
    $$

我们发现对于这个特定的算法，有一个很简单的优化可以使得这个算法运行更快。在算法内层循环中的(c)步，价值迭代默认是把V初始化成0，如果将V初始化成上一轮迭代得到的V值，这样能获得一个更好的迭代起始点，因而收敛地更快。

## 总结
- 强化学习的一个常见模型是马尔可夫决策过程(MDP)，MDP由一个五元组构成，MDP的目标是找到最优价值函数
- 当MDP是有限状态的情况下，可以用价值迭代或策略迭代两种方法求解。对于规模较小的MDP，策略迭代收敛更快；对于规模较大的MDP，价值迭代性能更好。因此在实践中，通常选取的算法是价值迭代
- 当MDP的状态转换概率和奖励函数未知的情况下，可以进行多次实验并根据实验结果给出近似的预估

https://www.jianshu.com/p/ab32fac9183c

----
