强化学习（Reinforcement Learning）：
- MDPs：马尔科夫决策过程
- Value Function：价值函数
- Value Iteration：值迭代
- Policy iteration：政策迭代

强化学习是介于监督学习与无监督学习之间的一类算法。

例如训练直升机飞行。使用强化算法，每秒运行10次。直升机上的传感器提供了直升机的方向的位置非常准确的估计。所以在所有时间点，你非常准确知道直升机在哪个地方。你的工作是使用定位作为输入，并输出相对应的控制杆的移动位置的数据。这和监督学习不同，因为我们通常不知道正确的控制动作是什么。更具体的说，如果这架直升机位于某个位置的方向，很难说应该把控制杆移动到哪个精确的位置。因此，很难对这种问题，使用监督学习算法。因为我们不能给出一个训练集，使它的位置作为输入，并且输出是所有正确的控制行为。

取而代之的是强化学习，我们将提供给学习算法不同类型的反馈。基本上是所谓的奖励信号。它会告诉直升机，什么时候它的表现很好，什么时候表现不佳。所以我们要做的是提出所谓的奖励信号。这个奖励信号用来测量直升机是否飞行的好，我们的算法以这个奖励函数为输入，并尝试飞的很好。

强化学习的另一个很好的例子就是考虑做一个玩游戏的程序：下棋。在游戏中的任何阶段，我们不知道最佳的举动是什么。所以很难构成一个下棋的监督学习问题。但我们确实知道的是：如果你让一台电脑下国际象棋游戏，我们知道它赢得了比赛还是失去了比赛，所以我们要做的是给它一个奖励信号，赢得一盘棋，就给它一个积极的奖励，输了时就给它一个惩罚，并希望随着时间的推移，它本身要学会赢越来越多的游戏。

可以把强化学习认为是训练狗，每当你的狗做的好就告诉它“好狗”，每次坏事就告诉它“坏狗”，随着时间的推移，你的狗会越来越多的做好事情。直升机训练方式类似。

强化学习比监督学习更难的原因是，这不是一个一次性决策过程。在监督学习中，如果你有一个分类，预测有人是否患有癌症，你做一个预测，然后就大功告成了。在强化学习中，必须随着时间的推移一直采取行动。因此它被称为渐进的决策过程。具体来说，假设一个程序走到第60步，输了1盘棋，那么它实际上在输了这盘棋之前，它移动了60步，使得它很难让算法来学习。这就是所谓的信贷分配问题。在第60步时输了棋，我们不是很确定它所有的移动中，哪些是正确的移动，哪些是坏的移动。
在直升机训练中的也是同样的问题，直升机坠毁了，我们不能确定到底是哪个操作导致最终坠毁，也许是1分钟前的某个操作，最终导致直升机坠毁，对此我们不能确定。

在强化学习中，我们面对的是渐进的决策过程，我们需要作出许多决定（decision），并且你的决定也许会有长远的后果。

强化学习问题的标准化：强化学习是使用MDP，即马氏决策（Markov Decision Process）对问题进行建模的形式体系（formalism）。MDP是一个五元组（five tuple），由五个集合组成，可以表示为$(S,A,{P_{sa}},\gamma,R)$，其中：
- S： set of state。状态的集合。比如在直升机无人驾驶例子中，S就是直升机可能的位置和方向集合
- A： set of actions：比如在直升机无人驾驶例子中，A就是操控直升机可能的动作集合，比如上下左右等。
- $P_{sa}$:state transition distribution。状态转换分布。$P_{sa}$指的是在状态s采取行动a的概率分布。
  $$
  \sum_{s'}P_{sa}(s')=1,P_{sa}(s')\geq0
  $$
- $\gamma$:discount facotr. 折扣因子
  $$0\leq\gamma<1$$
- R:reward function。奖励函数。它与状态集对应，都是实数。R表示在状态s采取行动a时能获得的奖励。

让我举一个MDP的具体的例子：机器人导航任务
> 人工智能：一种现代方法


机器人生活在一个网格世界中，阴暗的单元是障碍，机器人不能通过：
![](https://raw.githubusercontent.com/fray-hao/images/master/20190523091741.png)



https://www.jianshu.com/p/ab32fac9183c

https://blog.csdn.net/zhongyoubing/article/details/78465322