Principal Component Analysis(PCA)是一个非常强大的降低维度的算法

上一篇笔记中，介绍了因子分析模型，因子分析模型使用d维子空间的隐含变量z来拟合训练数据，所以实际上因子分析模型是一种数据降维的方法，它基于一个概率模型，使用EM算法来估计参数。

本篇主要介绍PCA(Principal Components Analysis, 主成分分析)，也是一种降维方法，但是该方法比较直接，只需计算特征向量就可以进行降维了。

PCA算法的目的和因子分析非常相似，但是相比于因子分析，PCA更为直接。

这个算法也是无监督学习算法，给定m个无训练样本$\{x^{(1)},...,x^{(i)}\}\quad x^{(i)}\in \mathbb{R}^n$

我们希望得到一个维度更低的数据集合，也就是将训练样本的维度降为k维度的数据（k<n）。
为什么要这样做呢？让我们来看一下例子。想象一下给定了包含了很多未知测量数据的训练集合。可能是关于人的一些测量数据。收集数据的人，经常会用厘米和英尺两种形式，保存人的身高数据。因为一些舍入问题，这两个数据并不会完全匹配，如果将其视为二维数据，这些点会离某条直线很近，但是因为舍入，它们并不完全落在直线上。我们的数据集合看起来是这样的，但是你真正应该关心的应该是这条轴，这个坐标轴附近的数据只是噪声。如果你能将数据的维度从二维降到一维，那么就可以避免数据中的噪声。
![](https://raw.githubusercontent.com/fray-hao/images/master/20190428092312.png)
再比如：自动飞行。想象一下，假设需要对飞行员进行一些问卷调查和实际的飞行测试。其中一个坐标轴表示飞行员的技术，另外一个轴表示可能表示它们多喜欢飞行。这里可能存在着一堆数据，如果能将这些数据从二维降到一维，那么也许你会得到一个更好的称为“松散飞行员资质”的衡量标准。

让我们先来看算法。
## 预处理
首先，在运行PCA之前，需要先对数据进行预处理。预处理步骤如下：
1). 令$
\mu = \frac{1}{m}\sum_{i=1}^mx^{(i)}
$

2). 使用$x^{(i)}-\mu$来替代$x^{(i)}$

3). 令$\sigma_j^2=\frac{1}{m}\sum_i(x_j^{(i)})^2$

4). 使用$x_j^{(i)}/\sigma_j$来替代$x_j^{(i)}$

步骤1-2将数据的均值变为0，当已经知道数据的均值为0的时候，可以省略这两步。
步骤3-4将数据的每个维度的方差变为1，从而使得每个维度都在同一个尺度下被衡量，不会造成某些维度因数值较大而又大的影响的情况。当预先知道数据处于同一尺度下时，可以忽略3-4步，比如图像处理中，已经预知了图像的每个像素都在0-255范围内，因而没有必要归一化了。
## 定义

如何找到数据的主轴（主方向呢）。首先，让我们讲一个特殊的例子，之后我们会正式提出算法。

这个案例的训练样本数为5个，均值大概为0
![](https://raw.githubusercontent.com/fray-hao/images/master/20190430130750.png)

主轴的方向大概是正45度角。所以，我们希望算法能够求出这个方向u，它是数据投影的最佳的方向，是数据随之变化的真正的主轴。让我们看看怎么样正规化。

假设找到一条用于投影的轴：
![](https://raw.githubusercontent.com/fray-hao/images/master/20190430131107.png)
我希望用它来捕获数据的变化，之后我令所有的数据都对这条轴进行投影，从而得到一些点。
![](https://raw.githubusercontent.com/fray-hao/images/master/20190430131224.png)
你会注意到，这些点，也就是训练样本与它们在轴上的投影的方差很大。相反，如果选择了一个不同的方向，例如一个最差的方向进行投影，都投影到这条线上，那么会发现所有的数据点的投影的方差比之前小得多。这次所有点彼此之间离得很近
![](https://raw.githubusercontent.com/fray-hao/images/master/20190430131657.png)

我们正式总结一下这个现象。我们希望找到一个方向向量u，使得我将数据投影到这个方向上时，这些数据之间离得越远越好。换句话说，找到一个方向，使得所有数据在该方向上的投影尽可能的分散，使方差尽可能的大。

我希望找到方向u。如果向量u的模等于1,即$||u||=1$，那么向量$x^{(i)}$在u上的投影应该是：${x^{(i)}}^Tu$

那么，我们需要找的U：
$$
\begin{aligned}
    \max_{u:||u||=1}&\frac{1}{m}\sum_{i=1}^m({x^{(i)}}^Tu)^2\sum_{i=1}^m
    \\&=\frac{1}{m}\sum_{i=1}^m({x^{(i)}}u^T)({x^{(i)}}^Tu)
    \\& = u^T\bigg[ \frac{1}{m}\sum_{i=1}^m{x^{(i)}}{x^{(i)}}^T \bigg]u
\end{aligned}
$$
所以，我希望使这个式子最大化，同时满足约束：$||u||=1$。这意味着u是中间这个矩阵的主特征向量。

> $Au=\lambda u$,那么u就是矩阵A的一个特征向量。而$\lambda$被称为特征值。所以主特征向量就是对应着最大的特征值的特征向量。

令矩阵：
$$
\varSigma = \frac{1}{m}\sum_{i=1}^m{x^{(i)}}{x^{(i)}}^T 
$$
在满足$u^Tu=1$的条件下，最大化$u^T\varSigma u$:
$$
\begin{aligned}
    &\max u^T\varSigma u
    \\& s.t. \ u^Tu=1
\end{aligned}
$$
这是带约束的优化问题，可以用拉格朗日方程来求解该最大化问题。则：
$$
\mathcal{L}(u,\lambda) = u^T\varSigma u -\lambda(u^Tu-1)
$$

相对于U对L求导：
$$
\begin{aligned}
    \nabla_u\mathcal{L} &= \nabla_u( u^T\varSigma u -\lambda(u^Tu-1))
    \\&=\nabla_uu^T\varSigma u-\lambda\nabla u^Tu
    \\&=\nabla_u tr(u^T\varSigma u)- \lambda\nabla tr(u^Tu)
    \\&= (\nabla_u tr(u^T\varSigma u))^T-\lambda(\nabla tr(u^Tu))^T
    \\&={(\varSigma u)^T}^T-\lambda {u^T}^T
    \\&= \varSigma u -\lambda u
\end{aligned}
$$

令导数为0：$
 \varSigma u -\lambda u \overset{set}{=} 0$。这意味着$\varSigma u=\lambda u$。所以，能够求解这个优化约束问题的解的u一定是$\varSigma$的一个特征向量。

![](https://raw.githubusercontent.com/fray-hao/images/master/20190506082706.png)
PCA有多种解释，这种子空间方差最大化只是其中一种，另一种比较常见的理解方法就是最小化原始点到投影点的距离的平方和。

## PCA的应用

PCA主要应用在这几个领域：

- 数据可视化：通常情况下数据集合的维度很高。当有人给你一个50维的数据集合时，你很难理解并观察，因为你不能将它画出来。通过情况下，如果想将一个高纬度数据显示出来，你需要将数据投影到2维空间或3维空间中，这样可以画出一个数据的三维视图，这样可以令高纬数据可视化。
- 压缩：
- 学习算法： 降低维度，却不丢失太多信息。通过这种方法将数据压缩成低纬的形式，从而可以让学习算法运行的更快。
- 减少过拟合的可能性： 因为维度越多，特征越多，越容易过拟合。而PCA可以减少特征数量，从而减少过拟合的可能。
- 异常检测：假设给你一个数据集合，之后可以运行PCA，找到数据大致位于哪个子空间中，之后如果想发现未来样本中的异常值，你需要观察当前样本，是否距离子空间非常远。这个不是最好的异常检测算法，但有时候这就足够了
- 匹配/距离计算：假设想进行人脸识别，输入图片的大小是100*100。你将每张人脸的图片表示成10000维的向量，维度非常高。每张图片就对应着10000维空间中的点。因为在这个10000维的空间中，不是所有的点都对应着有效的人脸图片，有很多维度的值都对应着无意义的随机噪声（例如，一些不属于人脸的东西），所以，大多有效的数据都处于一个低纬的子空间中。所以，可以认为有一些轴（通常50）能够真正衡量脸部的形状，但是另外的轴，我们不敢兴趣，它们只是随机噪声。在脸部识别应用中，我给你一张照片，然后问你那一张脸看起来和这张图片最像。实现这个功能的关键步骤是：衡量这两张脸的差异时，并不会直接使用欧几里得距离作为相似度判断标准，而是分别将两张图片投影到50维度的子空间中（称为特征脸），之后求出这两个点在子空间中的距离。这样做的时候，在原空间中看起来非常远的脸，投影到子空间中后，看起来非常近。   

PCA通常是有效的，但是也不能滥用，要考虑数据样本是否需要降维

http://ufldl.stanford.edu/wiki/index.php/%E4%B8%BB%E6%88%90%E5%88%86%E5%88%86%E6%9E%90

https://blog.csdn.net/u012409883/article/details/17091097

https://blog.csdn.net/u012409883/article/details/17091225

https://blog.csdn.net/stdcoutzyx/article/details/37568225

https://blog.csdn.net/mofushaohua_ln/article/details/79118663