本文对应公开课的第13个视频，这个视频仍然和EM算法非常相关，第12个视频讲解了EM算法的基础，本视频则是在讲EM算法的应用。本视频的主要内容包括混合高斯模型(Mixture of Gaussian, MoG)的EM推导、混合贝叶斯模型(Mixture of Naive Bayes，MoNB)的EM推导、因子分析模型(Factor Analysis Model)及其EM求解

## 回顾上节内容
我们讲到了无监督学习，这是一类机器学习问题。你有一个包含了m个无标记样本的训练集合
$$
\begin{Bmatrix}
x^{(1)},x^{(2)},...,^{(m)}
\end{Bmatrix}
$$
问题是：如果给你一个看起来像这样的数据集合：
![](https://raw.githubusercontent.com/fray-hao/images/master/20190408100349.png)
我们希望对数据的概率密度P(x)进行建模。对于上面的数据，我们认为它是由两个高斯分布生成的，我们介绍了一个算法可以拟合出混合高斯模型。
我们将x的概率密度P(x)进行建模：
$$
P(x) = \sum_zP(x|z)P(z)
$$
这里隐含的随机变量z代表你的数据来自于哪个高斯分布。所以，z服从以$\phi$为参数的多项式分布：
$$
z\sim Multinomial(\phi)
$$
$x|z = j$服从均值为$\mu_j$,方差为$\varSigma_j$的高斯分布：
$$
x|z = j \sim N(\mu_j,\varSigma_j)
$$

在上一节课的一开始，我们介绍了一个凭空想象出来的具体的算法用于拟合出这个问题的参数：$\phi,\mu,\varSigma$

在上一节课的后半部分，我介绍了EM算法。它的目标是选取参数，使得相对于$\theta$的对数似然性最大化：
$$
\max_\theta\sum_i\log P(x^{(i)})
$$
因为我们引入了随机变量z，所以上面公式应该等于：
$$
\max_\theta\sum_{z^{(i)}}P(x^{(i)},z^{(i)};\theta)
$$
然后，使用jenson不等式，在E-step中，我们将概率分布$Q_i$定义为：
$$
Q_i(z^{(i)}) = P(z^{(i)}|x^{(i)};\theta)
$$
> 它是一个给定$x^{(i)}$,以$\theta$为参数的后验概率分布。

在M-step中，我们将$\theta$的值更新为使这个式子最大化的值：

$$
 \theta:=\arg\max_\theta \sum_i\sum_{z^{(i)}}Q_i(z^{(i)})\log\frac{P(x^{(i)},z^{(i)};\theta)}{Q_i(z^{(i)})}
$$

用图形来表示，对数似然函数如下：
![](https://raw.githubusercontent.com/fray-hao/images/master/20190409085610.png)
通常很难直接将其最大化。E-step做的是确定概率分布$Q_i$,在图中它表示对数似然函数的下界：
![](https://raw.githubusercontent.com/fray-hao/images/master/20190409085905.png)

这个下界函数实际上就是argmax的右半部分：
![](https://raw.githubusercontent.com/fray-hao/images/master/20190409090017.png)
https://blog.csdn.net/stdcoutzyx/article/details/27368507

## EM算法的第二种理解方式
我一会儿要做的是将EM算法应用到混合高斯模型中。我会针对混合高斯模型，对E-step和M-step进行求解。但是，在那之前，我想再说一些关于EM一般形式的事情

事实证明，有另外一种理解EM算法的方式

定义一个优化目标：
$$
J(\Theta,Q) = \sum_i\sum_{z^{(i)}}Q_i(z^{(i)})\log\frac{P(x^{(i)},z^{(i)};\theta)}{Q_i(z^{(i)})}
$$
> 它就是M-step中的argmax中的式子，也就是对数函数的下界。
利用Jenson不等式，我们可以证明：
$$
l(\Theta)\geq J(\Theta,Q)
$$
换句话说，我们证明了：对于任意的$\theta$和Q，对数似然函数是$J(\Theta,Q)$的上界

联系之前所学的知识，EM算法实际上是相对于函数J的坐标上升算法。E-step是相对于Q进行最大化，而在M-step中是相对于$\theta$进行最大化

这是EM算法的另一种理解，这可以帮助我们理解为什么EM算法是收敛的。因为每一次迭代后，J函数都会单调上升。

## EM算法在混合高斯模型上的应用
接下来，我要将一般化的EM算法流程应用到混合高斯模型上。

我们用MoG作为混合高斯模型（Mixture of Gaussian）的简称。

在E-step，我们会求出概率分布Q，它是隐含变量z的概率分布：
$$
Q_i(z^{(i)}=j) =P(z^{(i)}=j|x,\phi,\mu,\varSigma)
$$
> 这是一个后验概率

通过贝叶斯公式：
$$
Q_i(z^{(i)}=j) = \frac{P(x^{(i)}|z^{(i)}=j)P(z^{(i)}=j)}{\sum_kP(x^{(i)}|z^{(i)}=k)P(z^{(i)}=k)}
$$

- $P(x^{(i)}|z^{(i)}=j)$服从均值为$\mu_j$,协方差为$\varSigma_j$的高斯分布。所以为了计算这一项，只需将参数为$\mu_j$,$\varSigma_j$的高斯密度函数代进来
- $P(z^{(i)}=j)$服从参数为$\phi_j$的多项式分布，所以这一项就等于$\phi_j$
- 分母可以进行同样的代入，最终求出Q

为了简便，我们令：
$$
\omega^{(i)}_j = Q_i(z^{(i)}=j)
$$

在M-step中，相对于$\omega,\mu,\varSigma$参数令式子最大化：
$$
\begin{aligned}
&\max_{\omega,\mu,\varSigma}\sum_{i=1}^m\sum_{z^{(i)}}Q_i(z^{(i)})\log\frac{P(x^{(i)},z^{(i)};\omega,\mu,\varSigma)}{Q_i(z^{(i)})}
\\&= \max_{\omega,\mu,\varSigma}\sum_{i=1}^m\sum_{j=1}^kQ_i(z^{(i)}=j)\log\frac{P(x^{(i)}|z^{(i)}=j;\mu,\varSigma)P(z^{(i)}=j|\omega)}{Q_i(z^{(i)})}
\\& = \max_{\omega,\mu,\varSigma}\sum_{i=1}^m\sum_{j=1}^kw_j^{(i)}\log\frac{\frac{1}{(2\pi)^{n/2}|\varSigma_j|^{1/2}}\exp\bigg( -\frac{1}{2}(x^{(i)}-\mu_j)^T\varSigma_j^{(-1)}(x^{(i)}-\mu_j)  \bigg)*\phi_j}{w_j^{(i)}}
\end{aligned}
$$
上面的公式中，第一个等号是将$p(x,z)$展开为$p(x|z)*p(z)$,第二个等号是将$p(x|z)$和$p(z)$的密度函数展开。

对于上面公式的结果，通过求偏导数然后使偏导为0来获得极大值时各参数的值。
例如，对于参数$\mu$，偏导公式为：
![](https://raw.githubusercontent.com/fray-hao/images/master/20190410090719.png)
将公式设为0，可以得到$\mu_j$的迭代公式：
![](https://raw.githubusercontent.com/fray-hao/images/master/20190410090932.png)

对于$\phi_j$,去除和$\phi_j$无关的项后，要求偏导的函数为：
$$
\sum_{i=1}^m\sum_{j=1}^kw_j^{(i)}\log\phi_j
$$
但是，因为$\phi_j$是多项式分布的概率值，所以$\phi_j$有一个约束条件，即$\sum_j\phi_j=1$。对于有约束的优化问题，可以对拉格朗日函数进行求导，拉格朗日函数如下：
$$
\mathcal{L}(\phi) =\sum_{i=1}^m\sum_{j=1}^kw_j^{(i)}\log\phi_j+\beta(\sum_{j=1}^k\phi_j-1) 
$$

对拉格朗日求偏导：
$$
\frac{\partial}{\partial\phi_j}\mathcal{L}(\phi) = \sum_{i=1}^m\frac{w_j^{(i)}}{\phi_j}+\beta
$$

令偏导等于0：
$$
\frac{\partial}{\partial\phi_j}\mathcal{L}(\phi)\overset{set}{=}0
$$
求得：
$$
\phi_j = \frac{\sum_{i=1}^mw_j^{(i)}}{-\beta}
$$
所以：
$$
\phi_j \propto \sum_{i=1}^mw_j^{(i)}
$$
> ∝ 表示正比于

又因为$\varSigma_j\phi_j=1$.所以可得：
$$
-\beta = \sum_{i=1}^m\sum_{j=1}^k w_j^{(i)}=m
$$
所以：
$$
    \phi_j:=\frac{1}{m}\sum_{i=1}^m w_j^{(i)}
$$

---

## 混合朴素贝叶斯模型(MoNB)

有人给你很多文本，你希望将它们聚类成若干一致的主题。例如，将相同事件或相同主题的新闻进行聚合。那么，如何在文本聚类中应用EM算法呢？

文本聚类可以看做是一个混合朴素贝叶斯（mixture of naive bayes ）模型

我们以前讲过贝叶斯模型的两种事件模型：多值伯努利事件模型与多项式事件模型。今天，我们将用到的是多值伯努利事件模型。

给定一个包含m个样本的训练集合：
$$
\begin{Bmatrix}
x^{(1)},x^{(2)},...,x^{(m)}
\end{Bmatrix}
$$
所以，我们给定了m个文本，每个文本都是一个n维的0，1向量
$$
x^{(i)}\in \{0,1\}^n
$$
指示函数表示某个词语在某文档中是否出现：
$$
x_j^{(i)} = \mathcal{I}\{\text{词语j是否在文档i中出现}\}
$$
我们要对隐含随机变量进行建模。假设我们将文本分为两类，则：
$$
z^{(i)}\in \{ 0,1\}
$$

> 如果要分为多个类，可以取多个值

对于混合朴素贝叶斯，我们假设$z^{(i)}$服从参数为$\phi$的伯努利分布：
$$
z^{(i)}\sim Bernoulli(\phi)
$$
所以，每一个文本按照某个概率属于类1或类2。

又由于贝叶斯模型的独立性假设，我们还有如下计算公式：
$$
P(x^{(i)}|z^{(i)}) = \Pi_{j=1}^nP(x_j^{(i)}|z^{(i)})
$$
更具体的说：
$$
\begin{aligned}
&\phi_{j|z=1} = P(x_j^{(i)}=1|z^{(i)}=1)
\\& 
\phi_{j|z=0} = P(x_j^{(i)}=1|z^{(i)}=0)
\end{aligned}
$$

> 如果将上面列出的公式中的z替换为y，就得出了以前讲过的朴素贝叶斯公式

参数定义完后，就可以将其代入到EM计算框架中了。

对于E-step，有：
$$
w^{(i)} = P(z^{(i)}=1|x^{(i)};\phi_j|z,\phi_z) = \frac{P(x^{(i)}|z^{(i)}=1)P(z^{(i)}=1)}{\sum_{j=0}^1P(x^{(i)}|z^{(i)}=j)P(z^{(i)}=j)}
$$

> $w^{(i)}$是一个权值，表示我们相信这个文档来自于哪个类。注意它与MoG算法中$w_j^{(i)}$的区别，之所以没有下标，是因为z是二值的，另一个参数可以由$1-w^{(i)}$得到。

在M-step中，会得到：
$$
\begin{aligned}
&\phi_{j|z=1} = \frac{\sum_{i=1}^m w^{(i)}\mathcal{I}\{x_j^{(i)}=1\}}{\sum_{i=1}^m w^{(i)}}
\\&\phi_{j|z=0} = \frac{\sum_{i=1}^m (1-w^{(i)})\mathcal{I}\{x_j^{(i)}=1\}}{\sum_{i=1}^m (1-w^{(i)})}
\\& \phi_z=\frac{\sum_{i=1}^m w^{(i)}}{m}
\end{aligned}
$$

## 因子分析模型（factor analysis model）



https://blog.csdn.net/stdcoutzyx/article/details/27368507

