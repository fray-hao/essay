本文对应公开课的第13个视频，这个视频仍然和EM算法非常相关，第12个视频讲解了EM算法的基础，本视频则是在讲EM算法的应用。本视频的主要内容包括混合高斯模型(Mixture of Gaussian, MoG)的EM推导、混合贝叶斯模型(Mixture of Naive Bayes，MoNB)的EM推导、因子分析模型(Factor Analysis Model)及其EM求解

## 回顾上节内容
我们讲到了无监督学习，这是一类机器学习问题。你有一个包含了m个无标记样本的训练集合$\begin{Bmatrix}
    x^{(1)},x^{(2)},...,^{(m)}
\end{Bmatrix}$
问题是：如果给你一个看起来像这样的数据集合：
![](https://raw.githubusercontent.com/fray-hao/images/master/20190408100349.png)
我们希望对数据的概率密度P(x)进行建模。对于上面的数据，我们认为它是由两个高斯分布生成的，我们介绍了一个算法可以拟合出混合高斯模型。
我们将x的概率密度P(x)进行建模：
$$
P(x) = \sum_zP(x|z)P(z)
$$
这里隐含的随机变量z代表你的数据来自于哪个高斯分布。所以，z服从以$\phi$为参数的多项式分布：
$$
z\text{\textasciitilde}Multinomial(\phi)
$$
$x|z = j$服从均值为$\mu_j$,方差为$\varSigma_j$的高斯分布：
$$
x|z = j \text{\textasciitilde}N(\mu_j,\varSigma_j)
$$

在上一节课的一开始，我们介绍了一个凭空想象出来的具体的算法用于拟合出这个问题的参数：$\phi,\mu,\varSigma$

在上一节课的后半部分，我介绍了EM算法。它的目标是选取参数，使得相对于$\theta$的对数似然性最大化
https://blog.csdn.net/stdcoutzyx/article/details/27368507




