## 1.判别学习算法与生成学习算法
>分类算法源于解决这样的问题：给定一个训练集合，给它一个类似逻辑回归的算法。逻辑回归的工作方式是观察这组数据，并尝试找到一条直线

以肿瘤诊断为例，分类算法是根据样本将肿瘤分为良性和恶性。而生成学习算法（generative learing algorithm）是分别对良性肿瘤和恶性肿瘤建模，当你需要对一个新的样本进行诊断时，你分别匹配这两个模型，然后判断它能更好地匹配哪个模型来预测新的癌症样本。

形式化（formally）的描述：

- 判别学习算法（discriminative learing algorithm）：
  - 学习的是p(y|x)。也就是给定x输出y。即线性回归
  - 学习预测$h_\theta(x) = \{0,1\}$。即逻辑回归

与判别学习算法不同的是，生成学习算法用来对p(x|y),记载给定所属类的情况下，对特征出现的概率进行建模，

在给定了样本所属类的情况下，生成模型对样本特征建立概率模型：
![](https://raw.githubusercontent.com/fray-hao/images/master/20190221103257.png)
出于技术上的考虑，也会对p(y)建模。

建立模型后，根据贝叶斯公式（ Bayes rule）:
$$
p(y=1|x) = \frac{p(x|y=1)p(y=1)}{p(x)}
$$

得到后验概率。其中：
$p(x)=p(x|y=1)p(y=1)+p(x|y=0)p(y=0)$

>常见的生成模型有：隐马尔可夫模型HMM、朴素贝叶斯模型、高斯混合模型GMM、LDA等

## 2.高斯判别分析(Gaussian Discriminant Analysis)
让我们学习一个生成学习算法:GDA。
首先，高斯判别分析的两个假设：
- 输入特征$x\in\mathbb{R}^n$,且是连续值
- p(x|y)属于多项高斯分布

### 2.1 多项高斯分布（The multivariate normal distribution）
当一个随机变量z满足多元高斯分布时：
$ 
z \text{\textasciitilde}N(\vec{\mu},\sum)
$

> $\vec{\mu}$为均值（mean）
> $\sum$为协方差矩阵。$\sum = E[(x-\mu)(x-\mu)^T]$
> |Σ|表示矩阵Σ的行列式(determinant)
所以z的概率密度函数如下:
$
p(z)=\frac{1}{(2\pi)^{\frac{n}{2}}|\sum|^{\frac{1}{2}}}exp(-\frac{1}{2}(x-\mu)^T(\sum)^{-1}(x-\mu))
$

接下来我们结合图像看一下多项高斯分布的例子。
![](https://raw.githubusercontent.com/fray-hao/images/master/20190221154910.png)
协方差矩阵反对角线元素的值控制图像起伏的方向。
![](https://raw.githubusercontent.com/fray-hao/images/master/20190221165125.png)
协方差矩阵对角线元素的值控制图像起伏的程度.
![](https://raw.githubusercontent.com/fray-hao/images/master/20190221170225.png)
均值决定图像中心的位置

### 2.2 高斯判别分析模型
如下面的图：
![](https://raw.githubusercontent.com/fray-hao/images/master/20190221170658.png)
在高斯判别分析算法（Gaussian discriminant analysis algorithm）中，会根据不同类别的样本拟合出各自的高斯分布
![](https://raw.githubusercontent.com/fray-hao/images/master/20190221174526.png)
之后，这两个高斯分布的密度函数可以定义出两个类别的隔离器（separator）

这就是高斯判别分析模型。对于模型p(y)。它是一个服从伯努利分布的随机变量，以$\phi$为参数：

$p(y) = \phi^y(1-\phi)^{1-y}$

用高斯分布对p(x|y)建模：

$$
\begin{aligned}
 &p(x|y=0) = \frac{1}{(2\pi)^{\frac{n}{2}}|\sum|^{\frac{1}{2}}}exp(-\frac{1}{2}(x-\mu_0)^T(\sum)^{-1}(x-\mu_0))   
 \\&  p(x|y=1) = \frac{1}{(2\pi)^{\frac{n}{2}}|\sum|^{\frac{1}{2}}}exp(-\frac{1}{2}(x-\mu_1)^T(\sum)^{-1}(x-\mu_1))  
\end{aligned}
$$

这个模型的参数为 $\phi,\mu_0,\mu_1,\sum$

对这些参数作极大(对数)似然估计：

$$
\begin{aligned}
l(\phi,\mu_0,\mu_1,\sum)   &= \log \Pi_{i=1}^m p(x^{(i)},y^{(i)})
\\& = \log \Pi_{i=1}^m p(x^{(i)}|y^{(i)};\mu_0,\mu_1,\sum)p(y^{(i)};\phi)
\end{aligned}
$$

这个公式被称为joint likelihood（联合似然性）

> 对于判别学习算法，我们的参数似然性公式定义如下:
> $ l(\theta)=p(y^{(i)}|x^{(i)};\theta)$
> 这个公式被称为conditional likelihood（条件似然性）.因此，对于生成学习算法，我们对参数的联合似然函数作极大似然估计。对于判别学习算法，我们对参数的条件似然函数作极大似然估计 

对联合似然函数作极大似然估计，得到参数的值为：

$$
\begin{aligned}
&\phi = \frac{\sum_iy^{(i)}}{m} = \frac{\sum_{i=1}^m\mathbb{1}\{y^{(i)}=1\}}{m} \qquad  \footnotesize \# 所有训练样本中标签为1的训练样本所占的比例。
\\& \mu_0 = \frac{\sum_{i=1}^m\mathbb{1}\{y^{(i)}=0\}x^{(i)}}{\sum_{i=1}^m\mathbb{1}\{y^{(i)}=0\}}   \qquad  \footnotesize \# 分子表示是对所有标签为0的x^{(i)}的求和;分母是所有标签为0的样本的数目
\\& \mu_1 = \frac{\sum_{i=1}^m\mathbb{1}\{y^{(i)}=1\}x^{(i)}}{\sum_{i=1}^m\mathbb{1}\{y^{(i)}=1\}}   \qquad  \footnotesize \# 分子表示是对所有标签为1的x^{(i)}的求和;分母是所有标签为1的样本的数目
\\& \sum  = \frac{1}{m}\sum_{i=1}^m(x^{i}-\mu_{y^{(i)}})(x^{i}-\mu_{y^{(i)}})^T
\end{aligned}
$$
>$\phi$是训练样本中结果y=1占有的比例。
$\mu_0$是y=0的样本中特征均值。
$\mu_1$是y=1的样本中特征均值。
$\sum$是样本协方差均值。

得到参数值之后， 对于一个新样本x，预测最可能的y：
$$
\begin{aligned}
 \arg\max\limits_y p(y|x) &= \arg \max\limits_y \frac{p(x|y)p(y)}{p(x)} \quad \footnotesize \#根据贝叶斯公式
\\& = \arg \max\limits_yp(x|y)p(y) \quad \footnotesize \#因为p(x)是独立于y的。
\end{aligned}
$$

>如果p(y)是均匀分布的。换句话说，如果取每种类型的概率相同，那么最后求的就是使p(x|y)最大的那个y，即$\arg \max\limits_yp(x|y)$。 当然这并不常见。

> 例如： $\min (x-5)^2=0$,则$\arg \min\limits_x (x-5)^2=5$。因为5是使得整个式子最小的x的值。

实际上，高斯判别分析和logistic回归之间是由关系的。例如，我们为训练样本建立了不同的高斯函数，根据贝叶斯公式，给定x情况下求y=1的概率密度函数类似于逻辑回归中的sigmoid函数。如下图：

![](https://raw.githubusercontent.com/fray-hao/images/master/20190221190449.png)


### 2.3  生成学习算法的优点与缺陷

假设x|y服从高斯分布，那么一定可以推出p(y=1|x)的后验分布函数是一个logistic函数。但是，反过来并不成立。因为，假设x|y服从泊松分布：

$
x|y=1 \quad\text{\textasciitilde} Passion(\lambda_0)
$
$
x|y=0 \quad\text{\textasciitilde} Passion(\lambda_1)
$

这同样意味着p(y=1|x)是一个logistic函数。实际上，还有许多其他假设可以使得p(y=1|x)是一个logistic函数。

这说明， x|y服从高斯分布的假设，要比y|x服从logistic分布的假设更强。
更加普适性的是，假设x|y的分布属于指数分布簇:
$$
\begin{aligned}
&x|y=1 \qquad \text{\textasciitilde} ExpFamily(\eta_0)
\\&    x|y=0 \qquad \text{\textasciitilde} ExpFamily(\eta_1)
\end{aligned}
$$
这蕴含着p(y=1|x)是一个logistic函数。这意味着x|y可以是伽马分布、$\beta$分布等，这样所得的后验分布p(y=1|x)是一个logistic函数。这显示了logistic函数的鲁棒性。

 那么该如何权衡高斯判别分析模型和logistic回归模型呢？

GDA做出了一个更强的假设，即x|y服从高斯分布，一旦这个假设正确或者近似正确，那么GDA算法的表现将会比logistic回归要好。

因为这个算法利用了更多数据的信息，算法知道数据服从高斯分布。

相反地，如果不确定x|y的分布情况，那么logistic回归将会表现的更好。

例如，你预先假设数据服从高斯分布，但是事实上数据服从泊松分布，那么逻辑回归函数仍然会获得不错的效果。因为如果数据服从泊松分布，p(y=1|x)仍然是一个logistic函数，所以会获得不错的效果，但是如果你假设数据服从高斯分布，那么算法的效果，可能就不会那么好。

事实证明，使用生成学习算法的真正的好处通常在于它需要更少的数据。即使不知数据是否服从高斯分布，通常情况下它的效果也出奇的好。而logistic回归的弱假设使得logistic回归算法具有更好的鲁棒性，对不确定的数据分布依然能够得到一个较好的结果，但与高斯判别分析相比，为了拟合出模型，它需要更多的样本