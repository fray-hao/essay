假设：
$$
\begin{aligned}
    &x|y=1\text{  \textasciitilde}ExpFamily(\eta_1)
    \\&x|y=0\text{  \textasciitilde}ExpFamily(\eta_0)
\end{aligned}
$$
实际上朴素贝叶斯属于这样的模型，也就是属于指数分布族。因为满足这些条件时，会得到一个逻辑后验分布（logistic posterior）。
因此使用朴素贝叶斯模型时，最后用到的还是线性分类器：
$$
h_\theta(x) = \frac{1}{1+e^{-\theta^Tx}}
$$


无论是感知器算法还是逻辑回归还是刚才所介绍的朴素贝叶斯模型（朴素贝叶斯算法是前置假设是多项式分布的多项式模型，所以也属于逻辑回归模型），其最终结果反映在数据上都是一条直线或一个超平面，但如果数据并不是线性可分的话，这些模型的性能会变差。针对该问题，涌现出很多对非线性可分数据进行分类的算法，神经网络是其中最早出现的一种。

对于逻辑回归模型，可以将其表示为下图所示：
![](https://i.loli.net/2019/02/28/5c7735d2d5266.png)
其中，$x_i$是输入的特征向量的各个分量，sigmoid是计算单元，output是函数输出。sigmoid计算单元有参数θ，其函数形式为：
$$g(\theta^Tx)=\frac{1}{1+exp(-\theta^Tx)}$$
而神经网络则是将这样的计算单元组合起来，如下图所示：
![](https://i.loli.net/2019/02/28/5c773c569ae80.png)

$a_1 =g(X^T\theta^{(1)}),a_2 =g(X^T\theta^{(2)}),a_3 =g(X^T\theta^{(3)})$

$g(z) = \frac{1}{1+e^{-z}}$
$h_\theta(x)=g(\vec{a}^T\theta^{(4)});\vec{a} =\begin{bmatrix}
    a_1
    \\a_2
    \\a_3
\end{bmatrix} $

$h_\theta(x)$是一个从$\theta_1$到$\theta_4$的函数。

一种学习模型参数的方法是利用成本函数（cost function）：

$$J(\theta)=\frac{1}{2}\sum_{i=1}^m(y^{(i)}-h_\theta(x^{(i)}))^2$$

这是我们熟悉的二次成本函数（quadratic cost function）。可以使用梯度下降方法最小化成本函数来求得参数。这意味着使得你的预测和样本标签尽可能的接近。
在神经网络中的梯度下降算法有一个专门的名称叫做反向传播算法。

在上面那个神经网络的样例图中，与输入直接相连的称为隐藏层（hidden layer），与输出直接相连的称为输出层（output layer）。神经网络算法的一大特点就在于不知道隐藏层计算的东西的意义，另一个特点在于神经网络有比较多的局部最优值，可以通过多次随机设定初始值然后运行梯度下降算法获得最优值。

接着，展示了两个神经网络实现的应用的视频。一个是Hammerton数字识别应用，对手写的数字进行识别，该应用的作者是Yann LeCun，他以字符识别与卷积神经网络而著名。另外一个应用则是NETtalk神经网络，使用神经网络来阅读文本，作者是Terry J.Sejnowski。