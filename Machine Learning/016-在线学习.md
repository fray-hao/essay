迄今为止，我们所学的所有学习算法都被称为批学习算法（batch learning algorithms）。给定一个训练集合，之后再训练集合上运行学习算法，之后再用其他的测试集合进行测试。

除此之外，还有另一类学习算法，称为在线学习（online learning）。在这类算法中，即使是在学习的过程中，仍然需要进行预测。

比如是一个分类问题，先给你一个输入$x^{(1)}$，之后问你，你能预测出$x^{(1)}$的标记吗？这个时候你没有见过任何一个数据，之后你给出了一个预测。我们将预测表示为$\hat{y}^{(1)}$。在作出预测后，再告诉你$y^{(1)}$的真实值。之后再给你下一个样本，进行预测后，再给你真实值，整个过程就是这样的。
![](https://raw.githubusercontent.com/fray-hao/images/master/20190326150226.png)

这一算法属于边学习边预测的算法。在在线学习算法中，我们更关心这一过程中的总误差（Total Number of Errors）：
$$
\sum_{i=1}^m 1\begin{Bmatrix}
    \hat{y}^{(i)}\neq y^{(i)}
\end{Bmatrix}
$$

事实证明，许多已经学习过的学习算法，都可以应用于这种在线学习的形式。对于以前的训练集合可以运行批学习算法，而新的样本可以采用在线学习算法；或者用在线学习的形式进行学习。

举一个具体例子:感知器算法. 首先，初始化$\theta=0$,当第i个样本到来时，更新参数：
$$
\theta :=\theta + \alpha(y^{(i)}-h_\theta(x^{(i)}))x^{(i)}
$$
> 对于感知器算法，可以证明，若正负样本线性可分，那么在线学习算法是可以收敛的。