## 1. 逻辑回归函数
sigmoid函数，也叫做逻辑函数，是一个二分类函数，它将输入的任意实数x，输出为0到1之间的实数。该函数的公式：
```math
\begin{aligned}
& g(z) = \frac{1}{1+e^{-z}}
\end{aligned}
```
sigmoid函数形成的曲线如下：
![image.png](https://upload-images.jianshu.io/upload_images/13764292-781af10ec42fb862.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

那么逻辑回归模型的预测函数为：
```math
\begin{aligned}
&h_\Theta(x)=g(\theta^Tx)=\frac{1}{1+e^{-\theta^Tx}}
\end{aligned}
```
接下来，我们从概率角度来解释逻辑回归模型：

首先，逻辑回归模型预测的就是y=1时的概率：
```math
\begin{aligned}
&P(y=1\big|x;\theta) = h_\theta(x)
\end{aligned}
```
那么y=0的概率：
```math
\begin{aligned}
&P(y=0|x;\theta)=1-h_{\theta}(x)
\end{aligned}
```
将上面两个公式结合起来：
```math
\begin{aligned}
&P(y|x;\theta)=h_\theta(x)^y(1-h_{\theta}(x))^{1-y}
\end{aligned} 
```
参数θ的似然函数L则为：
```math
\begin{aligned}
L(\theta)&=P(\vec{y}|x;\theta)=\prod_iP(y^{(i)}|x^{(i)};\theta)
\\&=\prod_ih_\theta(x^{(i)})^{y^{(i)}}(1-h_{\theta}(x^{(i)}))^{1-y^{(i)}}
\end{aligned}
```

令：
```math
\begin{aligned}
\mathcal{l}(\theta) &= \log L(\theta)
\\&=\log\prod_{i=1}^mh_\theta(x^{(i)})^{y^{(i)}}(1-h_{\theta}(x^{(i)}))^{1-y^{(i)}}
\\&=\sum_{i=1}^my^{(i)}\log h_\theta(x^{(i)})+(1-y^{(i)})\log (1-h_\theta(x^{(i)}))
\end{aligned}
```
通过梯度上升来求$\mathcal{l}(\theta)$的最大值：
```math
\begin{aligned}
&\Theta :=\Theta +\alpha\triangledown_\theta\mathcal{l}(\theta)
\end{aligned}
```
求导：
```math
\begin{aligned}
&\frac{\partial}{\partial\Theta_j}\mathcal{l}(\theta)=\sum_{i=1}^m\frac{\partial}{\partial\Theta_j}\bigg(y^{(i)}\log h_\theta(x^{(i)})+(1-y^{(i)})\log \big(1-h_\theta(x^{(i)})\big)\bigg)
\end{aligned}
```

分别求偏导：

(1)
```math
\begin{aligned}
&\frac{\partial}{\partial\Theta_j}(y^{(i)}\log
h_\theta(x^{(i)})=y^{(i)}\cdot\frac{1}{h_\theta(x^{(i)}}\cdot\frac{\partial}{\partial\Theta_j}(h_\theta(x^{(i)}))
\\&---------------------------
\\&\frac{\partial}{\partial\Theta_j}(h_\theta(x^{(i)}))=\frac{\partial}{\partial\Theta_j}(\frac{1}{1+e^{-\theta^Tx^{(i)}}})=\frac{(1)'(1+e^{-\theta^Tx^{(i)}})-1\cdot(1+e^{-\theta^Tx^{(i)}})'}{(1+e^{-\theta^Tx^{(i)}})^2}
\\&=\frac{0-(0+e^{(-\theta^Tx^{(i)})})(-\theta^Tx^{(i)})'}{(1+e^{-\theta^Tx^{(i)}})^2}
\\&=\frac{-e^{(-\theta^Tx^{(i)})}\cdot(-1)(\theta^Tx^{(i)})'}{(1+e^{-\theta^Tx^{(i)}})^2}
\\&=\frac{e^{(-\theta^Tx^{(i)})}}{(1+e^{-\theta^Tx^{(i)}})^2}\cdot(\theta^Tx^{(i)})'
\\&=\frac{1}{1+e^{-\theta^Tx^{(i)}}}\cdot\frac{e^{-\theta^Tx^{(i)}}}{1+e^{-\theta^Tx^{(i)}}}\cdot(\theta^Tx^{(i)})'
\\&=h_\theta(x^{(i)})\cdot\frac{1+e^{-\theta^Tx^{(i)}}-1}{1+e^{-\theta^Tx^{(i)}}}\cdot(\theta^Tx^{(i)})'
\\&=h_\theta(x^{(i)})\cdot(1-h_\theta(x^{(i)}))\cdot(\theta^Tx^{(i)})'
\\&---------------------------
\\& \theta^Tx^{(i)} = \theta_0+\theta_1x_1^{(i)}+...+\theta_jx_j^{(i)}
\\&\frac{\partial}{\partial\Theta_j}(\theta^Tx^{(i)})=x_j^{(i)}
\\&---------------------------
\\&\frac{\partial}{\partial\Theta_j}(y^{(i)}\log
h_\theta(x^{(i)}) =y^{(i)}\cdot\frac{1}{h_\theta(x^{(i)})}\cdot  h_\theta(x^{(i)})\cdot(1-h_\theta(x^{(i)}))\cdot x_j^{(i)}
\\&=y^{(i)} \cdot(1-h_\theta(x^{(i)}))\cdot x_j^{(i)}
\end{aligned}
```
>https://blog.csdn.net/JUNJUN_ZHAO/article/details/78564557

(2) 同理：

```math
\begin{aligned}
&\frac{\partial}{\partial\Theta_j}(1-y^{(i)})\log \big(1-h_\theta(x^{(i)})\big)=(1-y^{(i)})(-h_\theta(x^{(i)}))\cdot x_j^{(i)}
\end{aligned}
```

则：

$
\begin{aligned}
&\frac{\partial}{\partial\Theta_j}\mathcal{l}(\theta)=\sum_{i=1}^m\bigg( y^{(i)} \cdot(1-h_\theta(x^{(i)}))\cdot x_j^{(i)}+(1-y^{(i)})(-h_\theta(x^{(i)}))\cdot x_j^{(i)}\bigg)
\\&=\sum_{i=1}^m\bigg( \bigg( y^{(i)} \cdot(1-h_\theta(x^{(i)}))+(1-y^{(i)})(-h_\theta(x^{(i)})\bigg)\cdot x_j^{(i)}\bigg)
\\&=\sum_{i=1}^m\bigg( \bigg( y^{(i)}-y^{(i)}h_\theta(x^{(i)})-h_\theta(x^{(i)})+y^{(i)}h_\theta(x^{(i)})\bigg)\cdot x_j^{(i)}\bigg)
\\&=\sum_{i=1}^m \bigg( y^{(i)}-h_\theta(x^{(i)})\bigg)\cdot x_j^{(i)}
\end{aligned}
$

最后，梯度下降公式为：

```math
\begin{aligned}
&\theta_j := \theta_j+\alpha\sum_{i=1}^m \bigg( y^{(i)}-h_\theta(x^{(i)})\bigg)\cdot x_j^{(i)}
\end{aligned}
```

## 2. 感知器算法（perceptron algorithm）

sigmoid函数可以将实数映射到[0,1]区间，那么，如何才能生成0或1呢？

感知器算法定义成如下的形式：

```math
\begin{aligned}
&g(z) = \begin{cases}
1&\text{if}\ z\ge0
\\0&\text{if otherwise}
\end{cases}
\end{aligned}
```

它的图形想一个阶梯函数（step function），如下：

![image.png](https://upload-images.jianshu.io/upload_images/13764292-6d0811e55a636232.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)