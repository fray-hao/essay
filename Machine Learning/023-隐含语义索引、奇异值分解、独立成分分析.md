我们在上一篇笔记中讲到了PCA(主成分分析)。PCA是一种直接的降维方法，通过求解特征值与特征向量，并选取特征值较大的一些特征向量来达到降维的效果。

>PCA有一个缺陷，构建协方差矩阵$\varSigma$有可能非常大，例如一个10000维的数据，协方差矩阵需要10000*10000的数据。

本文继续PCA的话题，包括PCA的一个应用——LSI(Latent Semantic Indexing, 隐含语义索引)和PCA的一个实现——SVD(Singular Value Decomposition,奇异值分解)。在SVD和LSI结束之后，关于PCA的内容就告一段落。视频的后半段开始讲无监督学习的一种——ICA(Independent Component Analysis, 独立成分分析)。
## LSI（隐含语义索引）
PCA经常使用的应用是关于文本数据，例如文本相似度计算。

假设我们使用多元伯努利事件模型（NB-MBEM）来表示文本：
![](https://raw.githubusercontent.com/fray-hao/images/master/20190507082553.png)
整个文本被表示成一个由0，1组成的向量。这样的向量在PCA的时候一般不做标准化。规范化文本数据的方差为1，会大幅增加生僻词的权重。例如，上式中的aardvark几乎没有出现在任何文本中，将方差标准化为1时，会大幅增加aardvark单词的权重。
当给定两个文件$x^{(i)},x^{(j)}$，来比较它们的相似性。一种常见的方法是高纬度空间中，比例代表这两个文本的高纬向量之间的角度，如果角度较小，就说它们相似。更正式的说，相似度计算的是它们之间的角度$\theta$的余弦：
$$
sim(x^{(i)},x^{(j)}) =\cos\theta = \frac{{x^{(i)}}^Tx^{(j)}}{||x^{(j)}||*||x^{(j)}||}
$$
使用余弦算法计算相似度时，如果包含意思相同的单词时，如果这两个单词不一样，该算法会认为它们之间没有关联，因为余弦的分子计算的是内积：
$$
{x^{(i)}}^Tx^{(j)} = \sum_k x_k^{(i)}x_k^{(j)}
$$

例如有一篇文章，只包含一个单词learn，另一偏文章只包含一个单词study，在高纬度上计算相似度为0。但是通过PCA后，可以将意义相同的词映射到低纬空间的同一个维度上去。

隐含语义索引指的就是使用主成分分析进行降维，将意义相同的词映射到低纬空间的同一个维度上去。这样一可以降低计算复杂度，二可以减少噪声。两个在高纬度上完全不相似的文本通过降维后就可能变得相似了。例如，通过隐含语义索引，将learn和study映射到同一个维度上，它们之间就有关联了
![](https://raw.githubusercontent.com/fray-hao/images/master/20190507091927.png)

## 奇异值分解

奇异值分解是PCA的一种实现。以前，我们介绍了PCAd基本实现手段，即首先计算协方差矩阵：
$$
\varSigma = \frac{1}{m}\sum_{i=1}^mx^{(i)}{x^{(i)}}^T
$$
然后，对其特征值与特征向量进行求解。这样做的不好处在于，协方差矩阵的维度是样本维度*样本维度。比如，对于100*100的图片来说，如果以像素值作为特征，那么每张图片的特征维度是10000，那么协方差矩阵的维度就是10000*10000。在这样的协方差矩阵上求解特征值，耗费的计算量呈平方级增长。
使用SVD可以求出PCA的解但无需耗费大量计算。下面介绍SVD。

在线性代数中，如果A是m*n的矩阵：
$$
A \in \mathbb{R}^{m\times n}
$$
那么就可以将它分解为三个矩阵的成绩：
$$
A = UDV^T
$$

> 矩阵分解：LU、QR、Jordan、SVD

其中 $U\in\mathbb{R}^{m\times n}$,U的列向量为是$AA^T$的特征向量
$D\in\mathbb{R}^{n\times n}$且为对角矩阵：
$$
D=\begin{bmatrix}
    \sigma_1 &0&\cdots&0
    \\0&\sigma_2&0&\vdots
    \\\vdots&0&\ddots&0
    \\0&\cdots&0&\sigma_n
\end{bmatrix}
$$
D的对角线上的每一个$\sigma_{i}$都是矩阵A的奇异（特征）值。且已按照大小排序好。

$V\in\mathbb{R}^{n\times n}$，V的列向量是$A^TA$的特征向量

![](https://raw.githubusercontent.com/fray-hao/images/master/20190508090741.png) 

奇异值分解可以非常有效地计算特征向量和PCA。
如：
$$
X = \begin{bmatrix}
    \cdots& x^{(1)}&\cdots
    \\\cdots& x^{(2)}&\cdots
    \\&\vdots&
    \\\cdots& x^{(m)}&\cdots
\end{bmatrix}
$$
则：
$$
\varSigma = \frac{1}{m}\sum_{i=1}^mx^{(i)}{x^{(i)}}^T = X^TX
$$

要得到协方差矩阵$\varSigma$的top k个特征向量。我们使用SVD分解：
$$
X= UDV^T
$$
则V的top k列既是$X^TX$的特征向量，也就是协方差矩阵$\varSigma$的前K个特征向量。
例如，字典中有50000个单词，那么我们设计的矩阵：
$$
x\in \mathbb{R}^{m*50000}
$$

如果有100个例子，那就是100\*50000。很容易用SVD实现。但是如果用协方差矩阵那就是50000\*50000,很难被表示（harder to represent）。所以SVD是一种有效的方式来实现PCA

https://blog.csdn.net/stdcoutzyx/article/details/38037659