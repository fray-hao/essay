https://blog.csdn.net/stdcoutzyx/article/details/37559995

之所以要讲它，主要基于两个原因。
1. 它是一个非常有用的model。虽然它的应用并不如混合高斯模型或者混合贝叶斯模型那样广泛，但是还是很有用的。
2. 这个模型的推导过程用到的一些数学步骤是非常有用的。具体来说，对于因子分析，EM算法中的隐含随机变量是连续取值的。而因子分析模型的推导过程和之前看到的一些推导过程不太一样。

为了引出这个模型，我们需要将其和混合高斯模型做比较。

在介绍高斯模型的时候，我们用过这样的一个数据集合

![](https://raw.githubusercontent.com/fray-hao/images/master/20190415090909.png)

这个数据集的维度为2，样本量大约100.我们希望对这个无标记的训练样本用包含两个分布的混合高斯模型进行建模。

当样本m非常大的时候，使用混合高斯模型是可以的。一般情况下，m应该至少大于样本的维度n。这样就存在一个问题，如果你的数据维度n等于m或者远远大于m。这时该怎么做。对于高纬的数据如何建模？这种情况经常会遇到。

假设我们的数据$\{x^{(1)},x^{(2)},...,x^{(m)},\}$具有这样的性质，那么应该怎么对P(x)进行建模。

我们可以不考虑混合高斯模型，直接用简单的高斯模型进行建模。

例如，x服从高斯分布：
$$
x\sim \mathcal{N}(\mu,\varSigma),\quad\varSigma\in \mathbb{R}^{n\times n}
$$
如果通过极大似然估计求这些参数的话。所有样本的平均值：
$$
\mu =\frac{1}{m}\sum_{i=1}^m x^{(i)}
$$
协方差：
$$
\varSigma =\frac{1}{m}\sum_{i=1}^m(x^{(i)}-\mu)(x^{(i)}-\mu)^T
$$
实际上，如果数据的维度远大于训练样本的数目（$n>> m$）,通过极大似然估计得到的矩阵是奇异矩阵（意味着不满秩，而且特征值为0）。这意味着矩阵$\varSigma$是不可逆的。

例如，n=m=2：

![](https://raw.githubusercontent.com/fray-hao/images/master/20190415120127.png)

拟合高斯模型如下：
![](https://raw.githubusercontent.com/fray-hao/images/master/20190415120702.png)
通过极大似然估计得到的高斯分布对应的轮廓是一个无限细而且无限长的形状。

也可以通过另外一种方式发现这个问题。将$\varSigma$代入高斯密度函数公式中：
![](https://raw.githubusercontent.com/fray-hao/images/master/20190415121445.png)

> 前面分母为0，后面矩阵不可解.这不是一个好的算法模型

那么给定维度大于样本数目的数据后，该如何对P(x)进行建模呢？


追本溯源，这个问题可以认为是数据信息缺乏的问题，即从训练数据中得不到模型所需要的全部信息。解决办法就是减小模型所需要的信息。本文提到的手段有两个：
- 不改变现有模型，但是加强模型的假设。下面提到的协方差矩阵限制就是这类方法。
- 降低模型的复杂度，提出一个需要更少信息（更少参数即是需要更少参数）的模型。因子分析模型就是此类。

可以将协方差矩阵$\varSigma$限制为对角矩阵:
$$
\varSigma = \begin{pmatrix}
    \sigma_1^2 &...&0
    \\\vdots  &\sigma_2^2&\vdots
    \\0&...&\sigma_n^2
\end{pmatrix}
$$
矩阵的极大似然结果：
$$
\sigma_j^2 = \frac{1}{m}\varSigma_i(x^{(i)}_j-\mu_j)^2
$$
用图来表示，这样的矩阵意味着高斯分布的轮廓的轴和坐标轴是平行的：
![](https://raw.githubusercontent.com/fray-hao/images/master/20190419083535.png)

这样的做法是有效的，对于给定的两个训练样本是可以得到一个非奇异协方差矩阵的，但是这样做会抛弃所有数据中的相关性质。所以这并不是一个好的模型。

实际上，你可以添加更为严格的约束。例如，令：
$$
\varSigma =\sigma^2J = \begin{pmatrix}
    \sigma^2 &...&0
    \\\vdots  &\sigma^2&\vdots
    \\0&...&\sigma^2
\end{pmatrix}
$$
令所有对角元素都是相同的，这意味着你限制高斯密度函数的轮廓必须是圆形的。
![](https://raw.githubusercontent.com/fray-hao/images/master/20190419084503.png)

无论将矩阵约束为一般化对角矩阵还是常值对角矩阵化矩阵，它们都是很强的假设。如果有足够的数据的话，你也许希望你的模型能够体现出一些变量之间的关联性。因子分析模型要做的就是这个事情。

---

因子分析模型（factor analysis model） 是这样对数据进行建模的。

我们需要假设存在一个隐含的随机变量，我们将这个变量设为z，它应该服从均值为0，协方差矩阵为单位矩阵的高斯分布：
$$
z\sim \mathcal{N}(0,I),\quad\quad z\in \mathbb{R}^d\ (d<n)
$$
> d小于样本x的维度n

再假设训练样本x由隐含变量z生成，即：
$$
x = \mu +\lambda z + \varepsilon
$$
其中，$\varepsilon\sim\mathcal{N}(0,\Psi)$

上述公式等价于当z已知的时候，x的概率分布：
$$
x|z\sim \mathcal{N}(\mu+\lambda z,\Psi)
$$

这既是因子分析模型的定义，该模型有三个参数：
$$
\begin{aligned}
    &\mu \in \mathbb{R}^n
    \\& \lambda\in \mathbb{R}^{n\times d}
    \\& \Psi\in \mathbb{R}^{n\times n},\quad\Psi\text{是对角矩阵}
\end{aligned}
$$

让我们看一个具体的例子。假设z是一维向量，x是二维向量：
$$
z\in \mathbb{R}^1,x\in\mathbb{R}^2
$$

再假设：
$$
\begin{aligned}
    &\lambda = \begin{bmatrix}
    2\\1
\end{bmatrix}
\\& \Psi = \begin{bmatrix}
    1&0
    \\0&2
\end{bmatrix}
\\& \mu = \begin{bmatrix}
    0\\0
\end{bmatrix}
\end{aligned}
$$

首先，生成mge随机变量z，如果z服从高斯分布的话，对其进行一系列随机取样后，可能得到这样的结果：
![](https://raw.githubusercontent.com/fray-hao/images/master/20190419103820.png)

然后，乘以$\lambda$,转化为二维：
![](https://raw.githubusercontent.com/fray-hao/images/master/20190419104123.png)

第三步，使用$\mu$进行平移。这个例子中$\mu$值为0，也就是不需要平移。

之后，加入随机抖动。因为$\varepsilon$服从高斯分布，这意味着需要为每个x进行采样，也就是以每个点为中心创建一个与坐标轴平行的对应着$\varepsilon$的高斯分布，从这些高斯分布中可以获得一些样本：
![](https://raw.githubusercontent.com/fray-hao/images/master/20190419105224.png)
从这些高斯分布中获得的样本构成了对x的取样

一种非正式的对于这些数据的直观理解：我们认为这些数据实际上是从某些低纬空间中生成的，所以这里的x是由一维的直线空间生成的，但是在生成之后，要加上一点随机噪声，所以我们得到了二维的点。

----

为了描述模型的拟合方法,我们需要将高斯分布写成一种不同的形式.

将x表示成分块向量：
$$
x = \begin{bmatrix}
    x_1
    \\x_2
\end{bmatrix}\qquad x_1\in\mathbb{R}^r,x_2\in\mathbb{R}^s;x\in\mathbb{R}^{r+s}
$$
> 用上述方式将变量分成两部分，前一部分有r个元素，后一部分有s个元素

假设 $x\sim \mathcal{N}(\mu,\varSigma)$,且：
$$
\mu =\begin{bmatrix}
    \mu_1\\\mu_2
\end{bmatrix},\varSigma=\begin{bmatrix}
    \varSigma_{11}&\varSigma_{12}
    \\\varSigma_{21}&\varSigma_{22}
\end{bmatrix}
$$
这里$\mu_1\in\mathbb{R}^r,\mu_2\in\mathbb{R}^s,\varSigma_{11}\in\mathbb{R}^{r\times r},\varSigma_{12}\in\mathbb{R}^{r\times s},\varSigma_{21}\in\mathbb{R}^{s\times r},\varSigma_{22}\in\mathbb{R}^{s\times s}$

这个高斯分布是$x_1,...,x_{r+s}$的联合概率分布。我们想知道这个高斯分布的边缘分布于条件分布。例如，对于高斯分布，我们知道P(x),那么如何计算边缘分布$P(x_1)$.

它应该等于：
$$
P(x_1) = \int_{x_2} P(x_1,x_2)dx_2
$$
经过计算，它应该服从：
$$
x_1\sim\mathcal{N}(\mu_1,\varSigma_{11})
$$
而条件概率：
$$
P(x_1|x_2) = \frac{P(x_1,x_2)}{P(x_2)}=\frac{P(x)}{P(x_2)}
$$
因为，$x\sim\mathcal{N}(\mu,\varSigma),x_2\sim\mathcal{N}(\mu_2,\varSigma_{22})$，两个正态分布相除得到一个新的正态分布。推导过程请参考Chuong B.Do写的《Gaussian processes》中的此项推导。这里直接写结果：
$$
x_1|x_2 \sim \mathcal{N}(\mu_{1|2},\varSigma_{1|2})
$$
其中，
$$
\begin{aligned}
    &\mu_{1|2} =\mu_1+\varSigma_{12}\varSigma_{22}^{-1}(x_2-\mu_2)
    \\& \varSigma_{1|2} = \varSigma_{11}-\varSigma_{12}\varSigma_{22}^{-1}\varSigma_{21}
\end{aligned}
$$
## 推导因子分析模型
首先，重新列出模型的定义公式：
$$
\begin{aligned}
    &z\sim\mathcal{N}(0,I)
    \\& \varepsilon \sim \mathcal{N}(0,\Psi)
    \\& x = \mu+\lambda z+\varepsilon
\end{aligned}
$$
接下来，将高斯分布的矩阵表示法应用到因子分析模型中，对模型进行分析。该方法认为z和x符合多元高斯分布，即：
$$
\begin{bmatrix}
    z
    \\x
\end{bmatrix} \sim \mathcal{N}(\mu_{zx},\varSigma)
$$

接下来求解$\mu_{zx},\varSigma$ 

已知$E[z]=0,E[\varepsilon]=0$,则：
$$
E[x]= E[\mu+\lambda z+\varepsilon] =\mu
$$
所以：
$$
\mu_{zx} = \begin{bmatrix}
    z\\x
\end{bmatrix} = \begin{bmatrix}
    \overrightharpoon{0}\\\mu
\end{bmatrix}
$$

$\varSigma$是块矩阵：
$$
\varSigma = \begin{bmatrix}
    E[(z-E[z])(z-E[z])^T]&E[(z-E[z])(x-E[x])^T]
    \\E[(x-E[x])(z-E[z])^T]&E[(x-E[x])(x-E[x])^T]
\end{bmatrix}
$$
协方差矩阵包含4个矩阵块。
左上角的矩阵$\varSigma_{zz}=Cov(z) = I$
左下角的矩阵$$
\begin{aligned}
    \varSigma_{xz}&=E[(x-E[x])(z-E[z])^T]\\&=E[(\mu+\lambda z+\varepsilon-\mu)\cdot z^T]\\&=E[\lambda zz^T]- E[\varepsilon z]  \quad \text{因为}\varepsilon,z \text{都是独立的，所以} E(\varepsilon,z)期望值为0
    \\&=\lambda E[zz^T] = \lambda
\end{aligned}
$$ 

右上角的矩阵：$E_{zx} = {E_{xz}}^T= \lambda^T$
右下角的矩阵：
$$
\begin{aligned}
\varSigma_{xx} &= E[(\lambda z+ \varepsilon)(\lambda z+ \varepsilon)^T]
\\& = E[\lambda zz^T\lambda^T+\varepsilon z^T\lambda^T+\lambda z\varepsilon^T+\varepsilon\varepsilon^T]
\\& = \lambda E[zz^T]\lambda^T + E[\varepsilon\varepsilon^T]
\\& = \lambda\lambda^T+\Psi
\end{aligned}
$$

将所有结果放到一起：
$$
\begin{bmatrix}
    z\\x
\end{bmatrix}\sim\mathcal{N}(
    \begin{bmatrix}
        \overrightharpoon{0}
        \\\mu
    \end{bmatrix},
    \begin{bmatrix}
        I& \lambda^T
        \\\lambda& \lambda\lambda^T+\Psi
    \end{bmatrix}
)
$$

> 所以，我们的模型的参数是$\mu,\lambda,\Psi$.为了求出这些参数。对于一个给定的训练集合，我们可以进行极大似然估计。

从上面的模型中，我们可以得到x的边缘分布为：
$$
x\sim\mathcal{N}(\mu,\lambda\lambda^T+\Psi)
$$
因而，对于一个训练集$\{x^{(i)};i=1,2,...,m\}$,我们可以写出参数的似然函数：
$$
\begin{aligned}
    \mathcal{L}(\mu,\lambda,\Psi) &= \sum_i\log P(x^{(i)})
    \\&  = \log\Pi_{i=1}^m\frac{1}{(2\pi)^{n/2}|\lambda\lambda^T+\Psi|^{1/2}}\times\exp(-\frac{1}{2}(x^{(i)-\mu})(\lambda\lambda^T+\Psi)^{-1}(x^{(i)}-\mu)^T)
\end{aligned}
$$

由上式，若是直接最大化似然函数的方法求解参数的话，会很难。我们会使用EM算法来求解因子分析模型的参数。


在 E-step:
$$
Q_i(z^{(i)})=P(z^{(i)};\mu,\lambda,\Psi)
$$
M-step:
$$
\mu,\lambda,\Psi:=\arg\max_{\mu,\lambda,\Psi}\sum_i\int_{z^{(i)}}Q_i(z^{(i)})\log\frac{P(x^{(i)},z^{(i)};\mu,\lambda,\Psi)}{Q_i(z^{(i)})}dz^{(i)}
$$

---

回顾：
因子分析是一个无监督学习问题。所以我们有一个无监督的训练集合:$\{x^{(1)},...,x^{(m)}\},\quad x^{(i)}\in \mathbb{R}^n$,我们想要对密度函数P(x)进行建模。

我们的模型假设存在一个隐含的随机变量z。z服从高斯分布：
$$
z\sim \mathcal{N}(0,I)\quad z\in\mathbb{R}^d,d<n
$$
我们想象：
$$
x=\mu+\lambda z+\varepsilon\quad \varepsilon\sim\mathcal{N}(0,\Psi)
$$
这个模型的参数：
$$
\begin{aligned}
    &\mu\in\mathbb{R}^n
\\& \lambda\in\mathbb{R}^{n\times d}
\\& \Psi\in\mathbb{R}^{n\times n},\text{diagnoal} 
\end{aligned}
$$
应该怎么样得到模型的的参数？

z和x的联合分布：
$$
\begin{bmatrix}
    z
    \\x
\end{bmatrix} \sim \mathcal{N}(\mu_{zx},\varSigma)
$$
将参数写出来：
$$
\begin{bmatrix}
    z\\x
\end{bmatrix}\sim\mathcal{N}(
    \begin{bmatrix}
        \overrightharpoon{0}
        \\\mu
    \end{bmatrix},
    \begin{bmatrix}
        I& \lambda^T
        \\\lambda& \lambda\lambda^T+\Psi
    \end{bmatrix}
)
$$
从上面的模型中，我们可以得到x的边际分布：
$$
x\sim\mathcal{N}(\mu,\lambda\lambda^T+\Psi)
$$

那么，给定一个无标记样本的训练集合，根据x的边际分布可以写出训练集合的对数似然性。

$$
\begin{aligned}
    \Pi_{i=1}^m P(x^{(i)},\lambda,\mu,\Psi)=\Pi_{i=1}^m\frac{1}{(2\pi)^{n/2}|\lambda\lambda^T+\Psi|^{1/2}}\times\exp(-\frac{1}{2}(x^{(i)-\mu})(\lambda\lambda^T+\Psi)^{-1}(x^{(i)}-\mu)^T)
\end{aligned}
$$

> 上述公式中的公式是高斯分布$\lambda\lambda^T+\Psi$对应的概率密度函数
> 
> 为什么不是求P(x,z):
> ![](https://raw.githubusercontent.com/fray-hao/images/master/20190425080901.png)

我们得到了给定训练集合时参数的似然性，你可以尝试最大化这个似然性公式，并得到参数的极大似然估计。但是如果你尝试这样做了，你尝试对似然性取对数，对参数求导，令导数为0，你会发现你无法获得解析形式的解。所以，我们需要用EM算法估计出因子分析模型的参数

为了简便令$\Theta=\{\mu,\lambda,\Psi\}$.

在E-step：
$$
Q_i(z^{(i)})=P(z^{(i)}|x^{(i)},\Theta)
$$

M-step:

$$
\Theta := \arg\max_\Theta\Sigma_i\int_{z^{(i)}}Q_i(z^{(i)})\log\frac{P(x^{(i)},z^{(i)};\Theta)}{Q_i(z^{(i)})}dz^{(i)}
$$
> 这个式子和之前高斯混合模型中版本有区别。唯一区别是这里对$z^{(i)}$求积分，因为$z^{(i)}$是一个高斯随机变量，它是连续取值的。所以这里对$z^{(i)}$求积分而不是求和。

为了求E-step和M-step，我们需要做几件事。
首先，$z^{(i)}$是一个连续随机变量，所以你需要一种方式来表示连续取值的概率密度，也就是说需要概率密度函数来表示$Q_i(z^{(i)})$。

$$
z^{(i)}|x^{(i)}\sim\mathcal{N}(\mu_{z^{(i)}|x^{(i)}},\varSigma_{z^{(i)}|x^{(i)}})
$$

根据高斯矩阵一节中，我们算出的条件概率的期望和方差：
$$
\begin{aligned}
    &\mu_{z^{(i)}|x^{(i)}}=\overrightharpoon{0}-\lambda^T(\lambda\lambda^T+\Psi)^{-1}(x^{(i)}-\mu)
    \\& \varSigma_{z^{(i)}|x^{(i)}} = I-\lambda^T(\lambda\lambda^T+\Psi)^{-1}\lambda
\end{aligned}
$$
代入上面两个公式就可以得到$Q_i(z^{(i)})$的概率密度函数了。即：
![](https://raw.githubusercontent.com/fray-hao/images/master/20190425092000.png)
在M-step，需要最大化如下公式来求取参数$\mu,\lambda,\Psi$
![](https://raw.githubusercontent.com/fray-hao/images/master/20190425092913.png)

上面公式中，第一步转化是先将$p(z,x)=p(x|z)*p(z)$,然后将log打开。第二步则是将积分转化为求z服从Q分布的时候，函数$\log p(x^{(i)}|z^{(i)};\mu,\lambda,\Psi)+\log p(z^{(i)})-\log Q_i(z^{(i)})$的期望，本文下面会省略E的下标。

参数推导过程略~~