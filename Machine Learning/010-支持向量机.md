支持向量机(support vector machines)是一种学习算法，可以生成一个非线性分类器。它是一个非常高效，且无需定制的学习算法。

课程的思路是：先构造线性分类器，然后用支持向量机进行改秒的改动和扩展，让它可以很好的生成非线性分界线。

我们先讲一些线性分类器的知识，为了引出这个问题，我需要介绍两种对于分类的直观理解。

第一种直观理解需要考虑logistic回归。我们用logistic函数表示y=1的概率。它在x=0时与回归线相交：
![](https://i.loli.net/2019/02/28/5c776517a4a73.png)
所以，当你进行逻辑回归的时候，你可以认为它是一个算法：
1. 首先计算 $\theta^Tx$
2. 如果$\theta^Tx>0$ ，预测为1；如果$\theta^Tx<0$，预测为0

那么如果$\theta^Tx$远远大于0。（$\theta^Tx$>>0），我们可以相当确定的预测y=1。同理，如果$\theta^Tx<<0$,那我们会非常确定y=0 。此时，我们会认为这样的模型是良好的。当我们通过训练集合，根据logistic回归拟合出分类器时，这样的分类器是良好的。实际上，如果我们根据训练集合找到了参数，那么我们的学习算法不仅需要保证分类结果正确，更要进一步地保证对于分类结果的确定性。这就是第一种直观理解。

第二种直观理解。首先假设训练集合是线性可分的，也就是一定有直线可以将训练集合分开。第二种直观理解是对几何间隔的理解。这条分界线距离正负样本要尽可能的大：
![](https://i.loli.net/2019/02/28/5c7773da81f91.png)

## 1. 向量机符号
为了描述支持向量机。我们要定义一些符号。这些符号与前面的线性分类器中用的符号有些区别：

$$
\begin{aligned}
    &y\in \{-1,+1\} \quad\quad\# not \quad y \in\{0,1\} 
    \\&  \text{h函数的输出值在\{-1,+1\}}内
    \\& g(z) =\begin{cases}
        1 \quad\quad if z\geq 0
        \\-1 \qquad if z< 0
    \end{cases}
    \\& h_\theta(x)  = g(W^TX+b)  \quad\quad\# not \ \ h_\theta(x)  = g(\theta^TX) 。\footnotesize新表示中b就是以前的\theta_0
\end{aligned}
$$

## 2. 函数间隔和几何间隔

一个超平面（hyper plane）wb和某个特定的训练样本（$x_i,y_i$）相关的函数间隔（functional margin）被定义为：
$$\hat{\mathbb{\gamma}}^{(i)}=y^{(i)}(w^Tx^{(i)}+b)$$

这个式子的意思是：如果$y^{(i)}=1$，目的是令$w^Tx^{(i)}+b>>0$；如果$y^{(i)}=-1$，为了获得较大的函数间隔，需要令$w^Tx^{(i)}+b<< 0$
> ">>"表示远远的意思

这个定义捕捉到了我们之前对函数间隔的直观理解的特点。

上述公式还有一个性质，即对于正确分类的数据点，函数间隔不小于0,即$y^{(i)}(w^Tx^{(i)}+b)>0$

再来一个定义，将超平面和整个训练集合的函数间隔定义为：
$$\hat{\gamma} = \min_i\hat{\gamma}^{(i)}$$

> 公式的含义：如果有多个训练样本，整个训练训练集合的函数间隔定义为所有样本中最坏的情况（函数间隔最小）

单纯的以最大化函数间隔为目标是没有多大意义的，应为通过调整参数，可以使函数间隔获得任意大的值。例如，w、b增加2倍，$w^Tx+b$值就会增加，从而使函数间隔变大。通过添加一个正规化条件（normalization condition）可以解决这个问题。例如，我们可以加一个正规化条件，使得w的长度为1：
$$||w||=1$$
接下来，讲几何间隔（geometric margin）。训练样本的几何间隔，分类器的确定的边界会由$w^Tx+b$给出
![](https://i.loli.net/2019/03/04/5c7c90427f352.png)
假设我们正确地对样本进行了分类。我会将几何间隔定义为：一个训练样本（比如$x^{(i)},y^{(i)}$）对应的点，和由超平面确定的分割线之间的几何距离

![](https://i.loli.net/2019/03/04/5c7c9138c6daf.png)
接下来，快速进行一些代数推导。
我们可以将法向量，也就是和超平面成90度角的向量，表示为$\frac{w}{||w||}$,几何距离为$\gamma^{(i)}$。则分割平面上投影的点为：
$$x^{(i)}-\gamma^{(i)}\cdot\frac{w}{||w||}$$

![](https://i.loli.net/2019/03/04/5c7c94f0209f9.png)
因为，$\frac{w}{||w||}$是单位向量，是一个长度为1且与超平面垂直的向量，它乘以$\gamma^{(i)}$就是$x^{(i)}$到分割平面在坐标轴上的法线距离。$x^{(i)}减去这个法线距离就是这个投影点在坐标上的位置。


http://blog.sina.com.cn/s/blog_8a951ceb0102wbbv.html