支持向量机(support vector machines)是一种学习算法，可以生成一个非线性分类器。它是一个非常高效，且无需定制的学习算法。

课程的思路是：先构造线性分类器，然后用支持向量机进行改秒的改动和扩展，让它可以很好的生成非线性分界线。

我们先讲一些线性分类器的知识，为了引出这个问题，我需要介绍两种对于分类的直观理解。

第一种直观理解需要考虑logistic回归。我们用logistic函数表示y=1的概率。它在x=0时与回归线相交：
![](https://i.loli.net/2019/02/28/5c776517a4a73.png)
所以，当你进行逻辑回归的时候，你可以认为它是一个算法：
1. 首先计算 $\theta^Tx$
2. 如果$\theta^Tx>0$ ，预测为1；如果$\theta^Tx<0$，预测为0

那么如果$\theta^Tx$远远大于0。（$\theta^Tx$>>0），我们可以相当确定的预测y=1。同理，如果$\theta^Tx<<0$,那我们会非常确定y=0 。此时，我们会认为这样的模型是良好的。当我们通过训练集合，根据logistic回归拟合出分类器时，这样的分类器是良好的。实际上，如果我们根据训练集合找到了参数，那么我们的学习算法不仅需要保证分类结果正确，更要进一步地保证对于分类结果的确定性。这就是第一种直观理解。

第二种直观理解。首先假设训练集合是线性可分的，也就是一定有直线可以将训练集合分开。第二种直观理解是对几何间隔的理解。这条分界线距离正负样本要尽可能的大：
![](https://i.loli.net/2019/02/28/5c7773da81f91.png)

## 1. 向量机符号
为了描述支持向量机。我们要定义一些符号。这些符号与前面的线性分类器中用的符号有些区别：

$$
\begin{aligned}
    &y\in \{-1,+1\} \quad\quad\# not \quad y \in\{0,1\} 
    \\&  \text{h函数的输出值在\{-1,+1\}}内
    \\& g(z) =\begin{cases}
        1 \quad\quad if z\geq 0
        \\-1 \qquad if z< 0
    \end{cases}
    \\& h_\theta(x)  = g(W^TX+b)  \quad\quad\# not \ \ h_\theta(x)  = g(\theta^TX) 。\footnotesize新表示中b就是以前的\theta_0
\end{aligned}
$$

## 2. 函数间隔和几何间隔

一个超平面（hyper plane）wb和某个特定的训练样本（$x_i,y_i$）相关的函数间隔（functional margin）被定义为：
$$\hat{\mathbb{\gamma}}^{(i)}=y^{(i)}(w^Tx^{(i)}+b)$$

这个式子的意思是：如果$y^{(i)}=1$，目的是令$w^Tx^{(i)}+b>>0$；如果$y^{(i)}=-1$，为了获得较大的函数间隔，需要令$w^Tx^{(i)}+b<< 0$
> ">>"表示远远的意思

这个定义捕捉到了我们之前对函数间隔的直观理解的特点。

上述公式还有一个性质，即对于正确分类的数据点，函数间隔不小于0,即$y^{(i)}(w^Tx^{(i)}+b)>0$

再来一个定义：

http://blog.sina.com.cn/s/blog_8a951ceb0102wbbv.html