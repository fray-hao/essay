- Debugging RL(reinforcement learning) algorithms
- LQR
  - Differential Dynamic Programming (DDP)微分动态规划
- Kalman filters(滤波)
- Linear quadratic gaussian （LQG）线性二次高斯分布  

## 调试RL算法
![](https://raw.githubusercontent.com/fray-hao/images/master/20190612093250.png)
假设你要为自动飞行直升机设计一个控制器。一个非常典型的方法是利用机器学习算法或几个学习算法的组合来解决控制器的设计问题。
控制器的作用是控制直升机在原地盘旋。所以，要做的第一件事情是建立一个模拟器（Build a simulator of helicopter）
![](https://raw.githubusercontent.com/fray-hao/images/master/20190612093408.png)
这只是意味着模型状态转移概率（this just means model of the state transition probability）。可以有多种方式来处理。也许可以尝试阅读直升机教科书，并且在已知直升机空气动力学基础上建立模拟器。但实际上这是很难处理的；另一个你可以做的事情是收集数据，这些数据可能适合线性模型，也可能适合非线性模型，下一个阶段是关于当前状态与当前操作的函数。因此，有不同的方式来估计状态转移概率。 

你可能做的第二件事是选择一个奖励函数，也许是如下图所示的二次函数：
![](https://raw.githubusercontent.com/fray-hao/images/master/20190613081834.png)
 
最后，在模拟器上允许RL算法，最大化期望并得到最佳策略：
![](https://raw.githubusercontent.com/fray-hao/images/master/20190613091453.png)

比如说你这样做了，使用控制器控制直升机做的比真的驾驶员控制的要糟，下一步该如何？
实际上有几件事可以做：
- 尝试改进模拟器。也许数据是非线性的，但是你认为它是线性的，这时需要收集更多数据，得到更好的状态转移概率
- 修改奖励函数
- 修改RL算法（价值函数）。

假设：
1. 直升机模拟器是准确的。拥有一个精确的动态模型
2. RL算法，正确的控制了仿真直升机。因此，它有一个最大化的预期收益
3. 预期收益最大化对应于自主飞行。
   
如果上面三个假设都成立，这意味着控制器可以很好的控制真实的直升机。这是这个问题诊断推理的根源。这些都是一些有用的各种改进控制问题的诊断学。

- 如果控制器采用策略在模拟器上运行的很好，但是在真实直升机上不好，这意味着模拟器不准确。
- 让控制器与人分别控制直升机（模拟器或真实直升机）并且计算奖励的总和。如果机器获取的最佳奖励总和小于人获得的最佳奖励总和，那么这表明强化学习算法是有问题的，奖励总和没有最大化。
- 如果机器获取的最佳奖励总和大于人获得的最佳奖励总和，但仍然比人类飞得更糟糕。这表明问题出现在奖励函数。最大化的奖励总和和自主飞行不是符合的更好。换句话说，问题出在优化目标，而不是算法上。

## DDP(differential dynamic programming)

回顾（recap）一下，线性二次调节控制（LQR）是什么。上一节，我们定义了一个最大边界问题（horizon problem）：
$$
\max E\Big[R(s_{\tiny0},a_{\tiny0})+...+R(s_{\tiny T},a_{\tiny T}) \Big]
$$
我们只要找到边界奖励的总和（horizon sum of rewards），这里不需要贴现了（discounting）。然后我们提出了动态规划算法：
1). 首先，计算最后时刻的T价值函数：
$$
{V_T}^*(s) = \max_{a_{\tiny T}}R(s_{\tiny T},a_{\tiny T})
$$
2). 逆向递归计算$V_t^*,V_{t-1}^*,...,V_0^*$:
$$
V_t^*(s)  = \max_a R(s,a)+\sum_{s'}P_{sa}^{(t)}(s')V_{t-1}^*(s')
$$
3). 计算最佳策略
$$
\pi_t^*(s) = \arg \max_a R(s,a)+\sum_{s'}P_{sa}^{(t)}(s')V_{t-1}^*(s')
$$
看一个具体的例子：

https://www.jianshu.com/p/f22de42327b4