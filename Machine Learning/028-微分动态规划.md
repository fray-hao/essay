- Debugging RL(reinforcement learning) algorithms
- LQR
  - Differential Dynamic Programming (DDP)微分动态规划
- Kalman filters(滤波)
- Linear quadratic gaussian （LQG）线性二次高斯分布  

## 调试RL算法
![](https://raw.githubusercontent.com/fray-hao/images/master/20190612093250.png)
假设你要为自动飞行直升机设计一个控制器。一个非常典型的方法是利用机器学习算法或几个学习算法的组合来解决控制器的设计问题。
控制器的作用是控制直升机在原地盘旋。所以，要做的第一件事情是建立一个模拟器（Build a simulator of helicopter）
![](https://raw.githubusercontent.com/fray-hao/images/master/20190612093408.png)
这只是意味着模型状态转移概率（this just means model of the state transition probability）。可以有多种方式来处理。也许可以尝试阅读直升机教科书，并且在已知直升机空气动力学基础上建立模拟器。但实际上这是很难处理的；另一个你可以做的事情是收集数据，这些数据可能适合线性模型，也可能适合非线性模型，下一个阶段是关于当前状态与当前操作的函数。因此，有不同的方式来估计状态转移概率。 

你可能做的第二件事是选择一个奖励函数，也许是如下图所示的二次函数：
![](https://raw.githubusercontent.com/fray-hao/images/master/20190613081834.png)
 
最后，在模拟器上允许RL算法，最大化期望并得到最佳策略：
![](https://raw.githubusercontent.com/fray-hao/images/master/20190613091453.png)

比如说你这样做了，使用控制器控制直升机做的比真的驾驶员控制的要糟，下一步该如何？
实际上有几件事可以做：
- 尝试改进模拟器。也许数据是非线性的，但是你认为它是线性的，这时需要收集更多数据，得到更好的状态转移概率
- 修改奖励函数
- 修改RL算法（价值函数）。

假设：
1. 直升机模拟器是准确的。拥有一个精确的动态模型
2. RL算法，正确的控制了仿真直升机。因此，它有一个最大化的预期收益
3. 预期收益最大化对应于自主飞行。
   
如果上面三个假设都成立，这意味着控制器可以很好的控制真实的直升机。这是这个问题诊断推理的根源。这些都是一些有用的各种改进控制问题的诊断学。

- 如果控制器采用策略在模拟器上运行的很好，但是在真实直升机上不好，这意味着模拟器不准确。
- 让控制器与人分别控制直升机（模拟器或真实直升机）并且计算奖励的总和。如果机器获取的最佳奖励总和小于人获得的最佳奖励总和，那么这表明强化学习算法是有问题的，奖励总和没有最大化。
- 如果机器获取的最佳奖励总和大于人获得的最佳奖励总和，但仍然比人类飞得更糟糕。这表明问题出现在奖励函数。最大化的奖励总和和自主飞行不是符合的更好。换句话说，问题出在优化目标，而不是算法上。

## DDP(differential dynamic programming)

回顾（recap）一下，线性二次调节控制（LQR）是什么。上一节，我们定义了一个最大边界问题（horizon problem）：
$$
\max E\Big[R(s_{\tiny0},a_{\tiny0})+...+R(s_{\tiny T},a_{\tiny T}) \Big]
$$
我们只要找到边界奖励的总和（horizon sum of rewards），这里不需要贴现了（discounting）。然后我们提出了动态规划算法：
1). 首先，计算最后时刻的T价值函数：
$$
{V_T}^*(s) = \max_{a_{\tiny T}}R(s_{\tiny T},a_{\tiny T})
$$
2). 逆向递归计算$V_t^*,V_{t-1}^*,...,V_0^*$:
$$
V_t^*(s)  = \max_a R(s,a)+\sum_{s'}P_{sa}^{(t)}(s')V_{t-1}^*(s')
$$
3). 计算最佳策略
$$
\pi_t^*(s) = \arg \max_a R(s,a)+\sum_{s'}P_{sa}^{(t)}(s')V_{t-1}^*(s')
$$
看上节课的例子：
$$
\begin{aligned}
    &S_t\in \mathbb{R}^n,a_t\in\mathbb{R}^d
    \\&S_{t+1}= A_ts_t+B_ta_t + w_t,\qquad w \sim \mathcal{N}(0,\varSigma_w)
\end{aligned}
$$
在这LQR问题中，当前的状态是以前的状态和行动的线性函数，然后加上高斯噪声。获得这个线性模型的方式如下：
首先，当前状态是与以前的状态和行为相关的函数：
$$S_{t+1}= f(s_t,a_t)$$
![](https://raw.githubusercontent.com/fray-hao/images/master/20190731171950.png)
然后，你可以选择其中一个点，线性化这个模型：
![](https://raw.githubusercontent.com/fray-hao/images/master/20190731172241.png)
![](https://raw.githubusercontent.com/fray-hao/images/master/20190731172606.png)
把它缩小到一个线性方程：
$$
\begin{aligned}
    &S_{t+1}= A_ts_t+B_ta_t 
\end{aligned}
$$
矩阵$A_t,B_t$将取决于你选择的线性此函数时你所选择的位置。这个线性估计在$s_t,a_t$附近近的很好

对于LQR，它的二次奖励函数为：
![](https://raw.githubusercontent.com/fray-hao/images/master/20190801154323.png)
> 矩阵$U_t,V_t$是半正定的。所以回报总是负的

然后用动态规划（DP）算法。
每一个时间步的价值函数将是一个二次状态函数：
![](https://raw.githubusercontent.com/fray-hao/images/master/20190801154647.png)

运行DP，获取参数值：
![](https://raw.githubusercontent.com/fray-hao/images/master/20190801161737.png)
> LQR的一个重要的属性是函数不依赖于噪声项。
将参数值插入计算矩阵$L_T$:
![](https://raw.githubusercontent.com/fray-hao/images/master/20190801155215.png)


## DDP(微分动态规划)
DDP（differential dynamic programming）是LQR应用的特殊方式。

在大多数例子中，尝试去控制一个系统（直升机、汽车、化工厂）时，使用的是连续的状态。
比如，你有一些模拟器，下一个状态是以前的行动的数据的函数。你的模拟器是一个非线性的，但是确定性的系统：
![](https://raw.githubusercontent.com/fray-hao/images/master/20190801162810.png)
比如说，有些特定的轨迹需要直升机遵循。那么如何运用LQR来完成自动驾驶。DDP的步骤如下：
1. 调用一些标称轨迹（come up with nominal trajectory）
   $\bar{s_0},\bar{a_0},\bar{s_1},\bar{a_1},....,\bar{s_t},\bar{a_t}$
2. 然后，围绕这个标称轨迹，来线性化f
    ![](https://raw.githubusercontent.com/fray-hao/images/master/20190806151933.png)
    > 这里明确的使用LQR的能力，或者说这些更精细的边界问题，来处理非稳定动态。直觉是：即使是一个非常糟糕的控制器，拿出你原有的标称轨迹，仍然期望你的状态和行动在时间T可能合理地相似于粗燥控制器所做的：
    ![](https://raw.githubusercontent.com/fray-hao/images/master/20190806152354.png)
3. 运用LQR来获取最佳策略（use LQR to get $\pi_t$）
4. 使用一个模拟器（模型），来提出一个新的正常轨迹（use simulator to get new nominal trajectory）。
   ![](https://raw.githubusercontent.com/fray-hao/images/master/20190806153114.png) 
https://www.jianshu.com/p/f22de42327b4