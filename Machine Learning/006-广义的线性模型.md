到现在为止，我们学习了两个算法来对$P(y|x;\theta)$进行建模。
1. $y\in \mathbb{R}$,假设y满足高斯分布，之后我们得到了基于最小二乘法的线性回归。
2. $y\in \{0,1\}$,在这种情况下，最自然的是伯努利分布，这这种情况下我们得到了logistic回归

> 伯努利分布可以对只取两个值的随机变量进行建模。

我现在要做的是以上面两类算法为例，说明它们是一类更广泛的算法的特例，这类算法被称为广义线性模型。

## 1. 指数分布族(Exponential Family)
指数家族：

```math
\begin{aligned}
&P(y;\eta)=b(y)exp(\eta^TT(y)-a(\eta))
\end{aligned}
```
> $\eta $： 自然参数\
> $T(y)$：充分统计量。通常$T(y)=y$

对于一组给定的a、b、T，这个公式定义了一个概率分布的集合，它以$\eta$为参数。当改变$\eta$时，会得到一组不同的概率分布

接下来，要证明的是高斯分布和伯努利分布都是指数分布族（指数密度函数）的特例。也就说通过选择特定形式的a、b、T,使这个公式变成伯努利或高斯分布的形式。

### 1.1 伯努利分布
先从伯努利分布说起：

$
P(y=1;\phi)=\phi
$
> 参数$\phi$指定了y=1的概率

伯努利分布：

$
\begin{aligned}
&P(y;\phi)=\phi^y(1-\phi)^{1-y}
\\&=exp(\log\phi^y(1-\phi)^{1-y})
\\&=epx(y\log\phi+(1-y)\log(1-\phi))
\\&=exp(\log\frac{\phi}{1-\phi}y+\log(1-\phi))
\end{aligned}
$

所以：

$
\begin{aligned}
\underbrace{1}_{b(y)}\cdot exp(\underbrace{\log\frac{\phi}{1-\phi}}_\eta\underbrace{y}_{T(y)}+\underbrace{\log(1-\phi)}_{-a(\eta)})
\end{aligned}
$
> $\eta$ 在伯努利分布中是一个标量

$\eta$参数为：

$
\begin{aligned}
&\eta=\log\frac{\phi}{1-\phi}
\\&\Rightarrow e^\eta = \frac{\phi}{1-\phi}
\\&\Rightarrow  e^\eta = \phi(1+e^\eta)
\\&\Rightarrow \phi = \frac{e^\eta}{1+e^\eta}
\\&\Rightarrow \phi = \frac{1}{\frac{1+e^\eta}{e^\eta}}
\\&\Rightarrow \phi= \frac{1}{1+e^{-\eta}}
\end{aligned}
$

则：
$
\begin{aligned}
& a(\eta) = -\log(1-\phi)
\\&= -log(1-\frac{1}{1+e^{-\eta}})
\\& = -log(\frac{1}{1+e^\eta})
\\&=\log(1+e^\eta)
\\
\\& T(y)=y
\\
\\&b(y)=1
\end{aligned}
$

我们得到了一组a、T、b的函数，之后指数密度函数就变成了伯努利分布的公式。在这种公式下，$\eta$也就是逻辑回归函数的函数值的变动会改变伯努利公式的分布。
### 1.2 高斯分布
再来看高斯分布。

$
\begin{aligned}
N(\mu,\sigma^2)
\end{aligned}
$
> $\mu$： 期望值，决定了位置\
$\sigma^2$: 方差，决定了分布的幅度

> 前一节，我们利用极大似然估计推导最小二乘的参数时，我们发现$\sigma^2$并不重要，不管$\sigma^2$是什么，我们都会得到相同的参数

这里，我们仅将$\sigma^2$设为1。$\sigma^2$仅仅是变量y的比例因子（scaling factor）。这种情况下高斯密度函数为：

$
\begin{aligned}
&P(y;\mu)=\frac{1}{\sqrt{2\pi}}exp(-\frac{1}{2}(y-\mu)^2)
\\& = \underbrace{\frac{1}{\sqrt{2\pi}}exp(-\frac{1}{2}y^2)}_{b(y)}exp(\underbrace{\mu}_\eta \underbrace{y}_{T(y)}-\underbrace{\frac{1}{2}\mu^2}_{a(\eta)})
\\& 
\end{aligned}
$

则：

$
\begin{aligned}
& \eta = \mu
\\&a(\eta) = \frac{1}{2}\,u^2 = \frac{1}{2}\eta^2
\\& T(y) =y
\\& b(y) = \frac{1}{\sqrt{2\pi}}exp(-\frac{y^2}{2})
\end{aligned}
$

所以，这样的一组a，b，t就将高斯密度函数表示成立指数分布族的形式。

### 1.3 其它分布
还有许多其他分布属于指数分布族，如：

- 多项式分布（multinomial），用来对多元分类问题进行建模；
- 泊松分布（Poisson），用来对计数过程进行建模，如网站的访客数量、商店的顾客数量等；
- 伽马分布（gamma）和指数分布（exponential），用来对时间间隔进行建模，如等车时间等；
- β分布（beta）和Dirichlet分布（Dirichlet），用于概率分布；
- Wishart分布（Wishart），用于协方差矩阵分布。

## 2.广义线性模型（GLM）

这里学习怎样用指数分布族推导出广义的线性模型（generalized linear models）。

首先，定义三个假设（assume），也可以将它们看做是设计决策（design choices）

（1） 给定输入x和参数$\theta$,输出y。我们假设这个要预测的响应变量y属于指数分布族(以$\eta$为参数)：

$
\qquad y|x;\theta
$~ ExponentialFamily($\eta$)
> 这意味着：我们可以选取一些函数a、b、T，使得y在给定x以$\theta$为参数下的条件概率分布属于指数分布族，并以$\eta$为参数。

（2） 给定x，最终的目标是要求出输出E[T(y)|x]，因为通常T(y)=y，故h(x)也可以被求出来。

$
\qquad h(x) = E[y|x]
$

(3)自然参数η与输入特征x呈线性相关，即

&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;实数时：$
\eta = \theta^Tx
$

&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;向量时：$
\eta_i = \theta_i^Tx
$

让我们看一个伯努利分布的例子：

给定X与$\theta$，我们进行预测：

$
\begin{aligned}
h(x) &= E[y|x;\theta]\qquad\qquad\footnotesize\text{\#根据假设1、2}
\\&= P(y=1|x;\theta) \qquad\qquad\footnotesize\text{\#伯努利中，y只能取1或0，所以y的期望值等于y=1时的概率}
\\&= \phi \qquad\qquad\footnotesize\text{\#y=1的概率等于}\phi
\\&= \frac{1}{1+e^{-\eta}} \qquad\qquad\footnotesize\text{\#根据前面指数家族中的结论}
\\&= \frac{1}{1+e^{-\theta^Tx}}\qquad\qquad\footnotesize\text{\#根据假设3}
\end{aligned}
$

这里还有个有趣的知识点，参数η的方程g(η)给定了分布的均值，它被叫做正则响应函数（canonical response function），而它的倒数叫做正则关联函数（canonical link function），高斯家族的正则响应函数就是判别函数（identify function），伯努利的正则响应函数就是逻辑函数（logistic function）。

$
\begin{aligned}
&g(\eta) =E(T(y);\eta)
\end{aligned}
$

我们再来讲一个多项式分布（multinormal）。多项式分布是在k可能取值上的分布：

$
\begin{aligned}
&y\in \{1,\quad,k\}
\end{aligned}
$

首先我们给出多项式分布的一些公式： 
```math
\begin{aligned}
& P(y=i) = \phi_i
\end{aligned}
```
其中y∈{1,2,3,⋯,k}，又因为$ϕ_i$累加为1：

$
\phi_k = 1 - \sum_{i=1}^{k-1}\phi_i
$


所以，我们可以只保留k−1个参数： 

$\phi_1,\phi_2,...,\phi_{k-1}$


为了有个直观的理解，我们先来看k=2时的情况。 此时p(y=1)=ϕ1，p(y=2)=ϕ2，并且ϕ1+ϕ2=1，于是有ϕ2=1−ϕ1 。

```math
\begin{aligned}
&p(y|\phi_1) = \phi_1^{2-y}\phi_2^{y-1}=\phi_1^{2-y}(1-\phi_1)^{y-1}
\\& \Rightarrow p(y|\phi) = \phi^{2-y}(1-\phi)^{y-1}
\end{aligned}
```
> 当y=1时，只取$ϕ_1$；当y=2时，只取$ϕ_2$，即$1-ϕ_1$，又因为只有一个参数$ϕ_1$了，干脆就直接令$ϕ_1=\phi$。这与两点分布所要表达的意义是一模一样的，只不过两点分布只做二分类，采取了取0和1的方式.

那么在更一般的多分类问题中。
$$
\begin{aligned}
&T(1) = \begin{bmatrix}
1\\
0\\
\vdots\\
0
\end{bmatrix}
,T(2) = \begin{bmatrix}
0\\
1\\
\vdots\\
0
\end{bmatrix}\in \mathbb{R}^{k-1}
\\&T(k-1) = \begin{bmatrix}
0\\
0\\
\vdots\\
1
\end{bmatrix},
T(k) = \begin{bmatrix}
0\\
0\\
\vdots\\
0
\end{bmatrix}
\end{aligned}
$$
再引入两个指数函数：
$$
\mathbb{1}\{True\} =1,\mathbb{1}\{False\} = 0
$$
> 例如：1{2=3} =0， 1{3=3}=1

把上面的定义合起来：
$$
T(y)_i =\mathbb{1}\{y=i\}
$$
>$
T(y)_i$表示向量T(y)中的第i个元素

接下来将多项式分布写成指数分布族的形式：
```math
\begin{aligned}
p(y) =&\phi_1^{1(y=1)}\phi_2^{1(y=2)}...\phi_k^{1(y=k)}
\\&=\phi_1^{T(y)_1}\phi_2^{T(y)_2}...\phi_{k-1}^{T(y)_k-1}\phi_k^{1-\sum_{i=1}^{k-1}T(y)_i}
\\&=\exp(T(y)_1\log\phi_1+T(y)_2\log\phi_2+\cdots+((1-\sum_{i=1}^{k-1}T(y)_i)\log\phi_k)\\
&=\exp(T(y)_1\log\frac{\phi_1}{\phi_k}+T(y)_2\log\frac{\phi_2}{\phi_k}+\cdots+T(y)_{k-1}\log\frac{\phi_{k-1}}{\phi_k}+\log\phi_k)\\
&=b(y)\exp(\eta^TT(y)-a(\eta)) 
\end{aligned}
```

我们有：

```math
\begin{aligned}
\eta&=\begin{bmatrix}\log\frac{\phi_1}{\phi_k}\\\log\frac{\phi_2}{\phi_k}\\\vdots\\\log\frac{\phi_{k-1}}{\phi_k}\end{bmatrix}\\
a(\eta)&=-\log\phi_k\\
b(y)&=1 
\end{aligned}
```

我们可以发现，向量η的第i个元素为： 
```math
\eta_i = \log\frac{\phi_i}{\phi_k}\Rightarrow e^{\eta_i} =\frac{\phi_i}{\phi_k}
```
令其累加，得到：
```math
\sum_{i=1}^{k}e^{\eta_i} = \frac{\sum_{i=1}^k\phi_i}{\phi_k}\Rightarrow \phi_k = \frac{1}{\sum_{i=1}^ke^{\eta_i}}
```

我们可以将$\phi$反过来写成关于$\eta$的函数：

```math
\begin{aligned}
\phi_i &= \frac{e^{\eta_i}}{1+\sum_{i=1}^{k-1}e^{\eta_i}}
\\& = \frac{e^{\theta_i^TX}}{1+\sum_{i=1}^{k-1}e^{\theta_i^TX}}
\end{aligned}
```

所以,我们的学习算法：

```math
\begin{aligned}
h_\theta (x) &= E[T(y)|x;\theta]
\\&= \begin{bmatrix}
\mathbb{1}\{y=1\} &
\\\vdots&\Huge|\normalsize x;\theta
\\\mathbb{1}\{y=1\}
\end{bmatrix} 
\\&=\begin{bmatrix}
\phi_1
\\ \phi_2
\\ \vdots
\\\phi_k-1
\end{bmatrix}
\\& = \begin{bmatrix}
e^{\theta_1^TX}/(1+\sum_{i=1}^{k-1}e^{\theta_i^TX})
\\\vdots
\\e^{\theta_{k-1}^TX}/(1+\sum_{i=1}^{k-1}e^{\theta_i^TX})
\end{bmatrix}
\end{aligned}
```

这个算法被称为softmax回归。它可以处理k类，而不是仅仅两类。