数据集为波特兰房价调查：

面积| 价格
---|---
2104 | 400
1416 | 232
1534|  315
852 |  178
1940 | 240

可视化点图：

![image.png](https://upload-images.jianshu.io/upload_images/13764292-c082debed11e136d.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

我们要做的是预测房屋尺寸与价格之间的关系。

首先，引入一些符号（notation）：
>m：样本数量（training examples）\
x：输入变量/特征（input variables/features）\
y：输出变量/目标变量（output variables/target variables）\
`$(x^{(i)},y^{(i)})$`： 训练样本列表中的行

监督学习的一般步骤为：我们先找到一个训练集合（training set）。让后将我们的由训练样本组成的训练集合提供给学习算法，之后我们的学习算法会生成一个输出函数（这个函数称为假设（hypothesis））。这个假设的输出函数，可以根据输入的新的房屋尺寸，生成对于房价的估计，所以假设函数h将输入x映射到输出y。如下图所示：
![image.png](https://upload-images.jianshu.io/upload_images/13764292-838491ab6fc0206c.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

## 1. 线性回归模型
为了设计学习算法，第一步要做的就是决定应该怎样表示假设。在这第一个案例中，我们用线性来表示这个假设：

$
\begin{aligned}
&h(x)=\theta_0+\theta_1x
\end{aligned}
$

> x为输入特征，这里就是房屋大小。\
$\theta$为学习算法的参数，机器学习的任务就是利用训练集来得到合适的参数值

当有n个输入特征时，一般表示:$
\begin{aligned}
&h(x)=\theta_0+\theta_1x_1+\theta_2x_2+...+\theta_nx_n
\end{aligned}
$\
令$x_0=1$,可以表示为以下形式：\
$
\begin{aligned}
&h(x) = \sum_{i=0}^n = \theta_ix_i=\theta^Tx
\end{aligned}
$
## 2. 损失函数
为了使预测值和实际价格尽可能接近，我们使用最小二乘法定义损失函数，使得损失函数最小化的时候能拟合假设函数的模型：\
$
\begin{aligned}
&J(\theta)=\frac{1}{2}\sum_{i=0}^n(h(x^{(i)})-y^{(i)})^2
\end{aligned}
$

## 3. 梯度下降
接下来，我们讲几个可以使得$J(\theta)$最小的算法

（1）梯度下降（gradient descent）：一种搜索算法。基本思想：先给参数($\theta$)一些值，然后通过不断改变$\theta$的值，使得$J(\theta)$不断减少，直到$J(\theta)$取得了最小值。如下图所示：
![image.png](https://upload-images.jianshu.io/upload_images/13764292-83191f3995090e97.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)
梯度下降公式：

$$
\begin{aligned}
&\theta_i := \theta_i - \alpha\frac{\partial}{\partial\theta_i}J(\theta)
\end{aligned}
$$
> := 表示把右边的表达式的值赋值给左边变量\
$a$表示学习速度，控制梯度下降的步子。$a$值太小时，收敛的速度慢；$a$太大时，有可能会越过最优值，

当只有一样本时：

$
\begin{aligned}
\frac{\partial}{\partial\theta_i}J(\theta)&=\frac{\partial}{\partial\theta_i}(\frac{1}{2}(h(x)-y)^2)
\\&=\frac{1}{2}\cdot2(h(x)-y)\frac{\partial}{\partial\theta_i}(h(x)-y) \qquad\# chain\ rule:\ x^2
\\&=(h(x)-y)\frac{\partial}{\partial\theta_i}(\theta_0x_0+...+\theta_ix_i+...+\theta_nx_n-y) \qquad\# partial
\\&=(h(x)-y)\cdot x_i
\end{aligned}
$

所以

$
\begin{aligned}
&\theta_i := \theta_i - \alpha(h(x)-y)\cdot x_i)
\end{aligned}
$

当有n个样本时：

$
\begin{aligned}
\frac{\partial}{\partial\theta_j}J(\theta)&=\frac{\partial}{\partial\theta_i}(\frac{1}{2}\sum_{i=0}^n(h(x^{(i)})-y^{(i)})^2)
\\&=\frac{1}{2}\sum_{i=0}^n\frac{\partial}{\partial\theta_j}(h(x^{(i)})-y^{(i)})^2
\\&=\frac{1}{2}\sum_{i=0}^n2\cdot(h(x^{(i)})-y^{(i)})\frac{\partial}{\partial\theta_j}((h(x^{(i)})-y^{(i)})
\\&=\sum_{i=0}^n(h(x^{(i)})-y^{(i)})\frac{\partial}{\partial\theta_j}(\theta_0x_0^{(i)}+...+\theta_jx_j^{(i)}+...+\theta_nx_n^{(i)}-y^{(i)})
\\&=\sum_{i=0}^n(h(x^{(i)})-y^{(i)})x_j^{(i)}
\end{aligned}
$

所以：

$
\begin{aligned}
&\theta_j := \theta_j - \alpha\sum_{i=0}^n(h(x^{(i)})-y^{(i)})x_j^{(i)}
\end{aligned}
$

> 损失函数是一个平方函数，它的形状并不复杂，是一个碗状，它只有一个全局最小值，没有局部优化值。 
![image.png](https://upload-images.jianshu.io/upload_images/13764292-35d8164789f9a61a.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

上面的算法实际上叫做批梯度下降算法（batch gradient descent）。它是指在每一次迭代时需要遍历整个训练集来进行梯度的更新。当训练样本数以百万计时，批量梯度下降运算效率就低了。

## 4. 随机梯度下降（Stochastic gradient descent）
随机梯度下降，也称为增量下降(Incremental descent)。它和批量梯度下降法原理类似，区别在与求梯度时没有用所有的m个样本的数据，而是仅仅选取一个样本i来求梯度。公式如下：

$
\begin{aligned}
&Repeat\qquad\{
\\&\quad For\ \ j=1\ \ to\quad m\quad \{
\\&\qquad \theta_j:=\theta_j-\alpha(h(x^{(i)})-y(i))\cdot x_j^{(i)}
\\&\quad \}
\\&\}
\end{aligned}
$

对于大规模的数据，随机梯度下降算法通常会快得多，但是随机梯度下降算法不会精确地收敛到全局的最小值。例如，我们的函数图形的等高线如下图，参数总体向着全局最小值附近徘徊，一般情况下已经足够了：
![image.png](https://upload-images.jianshu.io/upload_images/13764292-b32ab1974354d288.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

## 5. 正规方程（Normal equation）
首先定义符号：

(1)

$
\begin{aligned}
& \bigtriangledown_\Theta J = \begin{bmatrix}
\frac{\partial J}{\partial\Theta_0}
\\
\\\vdots
\\
\\\frac{\partial J}{\partial\Theta_n}
\end{bmatrix}\in \mathbb{R}^{n+1}
\end{aligned}
$
> $\bigtriangledown_\Theta J$是一个梯度向量

梯度下降的向量形式：

$
\begin{aligned}
&\Theta := \Theta-\alpha\cdot\bigtriangledown_\Theta J
\end{aligned}
$

(2)

假设f(A)是一个从矩阵映射到实数的函数，也就是说它以矩阵作为输入，即$A\in \mathbb{R}^{m*n}$

$
\begin{aligned}
\bigtriangledown_Af(A) =\begin{bmatrix}
&\frac{\partial f}{\partial A_{11}}\quad&\cdots\quad&\frac{\partial f}{\partial A_{1n}}&
\\
\\&\vdots\quad &\ddots\quad&\vdots&
\\
\\&\frac{\partial f}{\partial A_{11}}\quad&\cdots\quad&\frac{\partial f}{\partial A_{1n}}&
\end{bmatrix}
\end{aligned}
$

(3)

如果A是一个n*n的矩阵

$
\begin{aligned}
& A\in \mathbb{R}^{n*n}
\end{aligned}
$

那么A的迹则为：

$
\begin{aligned}
&trA=\sum_{i=i}^{n}A_{ii}
\end{aligned}
$

(4)

已知：
$
\begin{aligned}
&Fact:
\\&\qquad    trAB = trBA
\\&\qquad trABC = trCAB=trBCA
\\& If\quad f(A) = trAB,\quad \bigtriangledown_AtrAB=B^T
\\&\qquad trA = trA^T
\\&If\quad a\in\mathbb{R}\quad tra=a
\\&\bigtriangledown_AtrABA^TC=CAB+C^TAB^T
\end{aligned}
$

接下来，我们开始推导：

$
\boxed{
\begin{aligned}
\\&X=\begin{bmatrix}
\text{\text{\textemdash}\textemdash} (x^{(1)})^T\text{\textemdash}\text{\textemdash}
\\\vdots
\\\text{\text{\textemdash}\textemdash} (x^{(m)})^T\text{\textemdash}\text{\textemdash}
\end{bmatrix}
\\&X\Theta = \begin{bmatrix}
 (x^{(1)})^T\Theta
\\\vdots
\\(x^{(m)})^T\Theta
\end{bmatrix}=\begin{bmatrix}
h(x^{(1)})
\\\vdots
\\h(x^{(m)})
\end{bmatrix}
\\&\vec{y}=\begin{bmatrix}
y^{(1)}
\\\vdots
\\y^{(m)}
\end{bmatrix}
\\& X\Theta-y = \begin{bmatrix}
h(x^{(1)})-y^{(1)}
\\\vdots
\\h(x^{(m)})-y^{(m)}
\end{bmatrix}
\\
\\
\\&\qquad reacall\quad Z^TZ = \sum_iZ^2_i
\\&J(\Theta)=\frac{1}{2}\sum_{i=0}^n(h(x^{(i)})-y^{(i)})^2=\frac{1}{2}(X\Theta-y)^T(X\Theta-y)
\end{aligned}}
$

为了使损失函数值最小，也就是求其导数为0：

$
\begin{aligned}
&\bigtriangledown_\Theta J(\Theta) \overset{set}{=}\vec{0}
\end{aligned}
$

也就是求：

$
\boxed{
\begin{aligned}
&\bigtriangledown_\Theta \frac{1}{2}(X\Theta-y)^T(X\Theta-y)
\\&=\frac{1}{2}\bigtriangledown_\Theta(\Theta^TX^TX\Theta-\Theta^TX^Ty-y^TX\Theta+y^Ty)
\\&=\frac{1}{2}\bigtriangledown_\Theta tr\underbrace{(\Theta^TX^TX\Theta-\Theta^TX^Ty-y^TX\Theta+y^Ty)}_{\text{因为这是一个实数}}
\\&=\frac{1}{2}(\bigtriangledown_\Theta tr\ \Theta\Theta^TX^TX-\bigtriangledown_\Theta tr\ y^TX\Theta-\bigtriangledown_\Theta\ y^TX\Theta)
\end{aligned}}
$

因为：

$
\boxed{
\begin{aligned}
&\bigtriangledown_\Theta \underbrace{\Theta}_A\underbrace{\ }_B\underbrace{\Theta^T}_{A^T}\underbrace{X^TX}_C=\underbrace{X^TX}_C\underbrace{\Theta}_A\underbrace{\ }_B+\underbrace{X^TX}_{C^T}\underbrace{\Theta}_{A}\underbrace{\ }_{B^T}
\\&\bigtriangledown_\Theta\ \underbrace{y^TX}_B\underbrace{\Theta}_A = \underbrace{X^Ty}_{B^T}
\end{aligned}}
$

把这些结果代入：

$
\boxed{
\begin{aligned}
&\bigtriangledown_\Theta J(\Theta) = \frac{1}{2}[X^TX\Theta+X^TX\Theta-X^Ty-X^Ty]=X^TX\Theta-X^Ty
\\&X^TX\Theta-X^Ty\overset{set}{=}0
\\&X^TX\Theta=X^Ty
\\&\Theta=(X^TX)^{-1}X^Ty
\end{aligned}}
$

梯度下降与正规方程的比较：

梯度下降 | 正规方程
---|---
需要选择学习率 | 不需要
需要多次迭代| 一次运算得出
当特征数量n大时也能较好适用。算法复杂度为`$O(kn^2)$`|需要计算`$(X^TX)^{-1},$`如果特征数量n较大则运算代价大，因为矩阵逆的计算时间复杂度为`$O(n^3)$`  ，通常来说当n小于10000 时还是可以接受的
适用于各种类型的模型|只适用于线性模型，不适合逻辑回归模型等其他模型

