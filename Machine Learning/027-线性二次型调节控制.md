## 回顾
MDP是一个五元组：
![](https://raw.githubusercontent.com/fray-hao/images/master/20190605082650.png)
因此，我们有值迭代。经过一段时间迭代后，将导致V至V*的转换。我们找到了最优的值函数
![](https://raw.githubusercontent.com/fray-hao/images/master/20190605082822.png)
最后计算最优的策略。也就是找到a的最大值：
![](https://raw.githubusercontent.com/fray-hao/images/master/20190605083228.png)


## 变种
接下来，描述几个常见的变异MDPs。

### 1. State-action Rewards

将奖励函数改为从状态行为对（state action pair）映射到实数的函数：
$$
R: S_\lambda A\mapsto \mathbb{R}
$$
决策过程是一个状态行动的顺序：
![](https://raw.githubusercontent.com/fray-hao/images/master/20190605084553.png)

在这种设定下，整个过程的总奖励为：
![](https://raw.githubusercontent.com/fray-hao/images/master/20190605084801.png)

然后，像往常一样，我们的目标是找到一个策略。该策略寻找the function mapping from the state's action。当执行这一策略时，可以最大化总收益的预期值。所以，上面公式中的定义实际变成了MDP的状态行动回报。

使用状态行为回报（state action reward）使你能更直接的模式化问题。在不同的动作中，我们有不同的成本（costs）。比如，让机器人动起来比让它提留在原地的代价（costs）更昂贵，因为停留的回报可能有更低的成本（costs），这是因为你不适用电池电源。再比如户外驾驶，从非常粗糙的石子路或草地上驶过，要比柏油路上驶过的代价更昂贵、更困难，所以，你可以指定一个动作，要求驾驶在草地或石子路上比在柏油路上更昂贵。

在这种形式下，最优值函数：
![](https://raw.githubusercontent.com/fray-hao/images/master/20190605090246.png)

现在，看一下值迭代：
![](https://raw.githubusercontent.com/fray-hao/images/master/20190605090647.png)
最优策略：
![](https://raw.githubusercontent.com/fray-hao/images/master/20190605090809.png)
### 2. 有限边界的MDP(finite horizon MDP)
有限边界MDP由以下5元祖构成：
$$
(S,A,\{P_{sa}\},T,R)
$$
- T : horizon time。 T是一个称为边界时间的参数。它的真正含义是：we‘ll be taking action in the MDP only for a total f capital T times。

在这种设定下，整个过程的奖励定义为：
$$
R(s_{\tiny0},a_{\tiny0})+R(s_{\tiny1},a_{\tiny1})+...+R(s_{\tiny T},a_{\tiny T})
$$

事实证明，最优的策略可能是非平稳的（non-stationary）。stationary， means that it doesn't depend on time.非平稳的意思是它可能取决于时间。因此，非平稳的大致意思是我采取的最佳行动将是随着不同的time steps而不同。
例如，假设我们有一个机器人，在左侧有1个加1的奖励，而在很远的右边有一个加10的奖励：
![](https://raw.githubusercontent.com/fray-hao/images/master/20190606081726.png)

因此，取决于你剩余多少时间，它可以取更好地取获得+1或+10的奖励。如果是处于游戏的初期，你仍然有大量的时间，它可能是更好地走向+10的奖励，以获得更大的奖励，如果你只剩下几个时间步长，那么就没有足够的时间去获得+10的奖励，你最好获得+1的奖励，它近的多。这个例子说明的是，当你再某一状态时，可以采取的最好的行动取决于时间。

因为在序列中，策略是非稳定的，过渡的概率也成为非平稳过渡的概率（non-stationary transistion probabilities）。在t+1时的状态is joined from 以前状态行为的状态转移概率：
$$
S_{t+1}\sim P^{(t)}_{s_{t,a}}
$$
例如，建立飞机的模型。如果飞机要飞很长时间。随着时间的推移，燃料燃烧，飞机变得更轻，所以飞机的动态策略随着时间的推移而变化，混合状态（mixed state）不仅取决于当前状态、行为，也取决于烧了多少燃料，也就是它处于什么时间
另一例子：你知道未来24小时内的大风和降水情况。从这里飞到纽约，在不同的时间，飞不同的路线的成本不同。
再比如：因为时间段不同，交通驾驶的成本是不同的。

这里状态转移概率是与时间有关的函数。

同样，奖励函数R也随着时间变化，这样，五元组表示为：
$$
(S,A,\{P^{t}_{sa}\},T,R^{(t)})
$$
整个过程的奖励为：
$$
R^{(0)}(s_{\tiny0},a_{\tiny0})+R^{(1)}(s_{\tiny1},a_{\tiny1})+...+R^{(t)}(s_{\tiny T},a_{\tiny T})
$$
那么，如何找到最佳策略呢？

首先，价值函数为：
$$
V_t(s) = \mathbb{E}\Big[R^{(t)}(s_t,a_t)+R^{(t+1)}(s_{t+1},a_{t+1})+...+R^{(T)}(s_T,a_T)|\pi,s_t=s\Big]
$$

在有限边界的MDP问题中，我们也采取基于迭代的算法来估算最优值函数：
$$
V_t^*(s)=\max_a R^{(t)}(s,a)+\sum_{s'}P^{(t)}_{s,a}(s')V^*_{t+}(s')
$$

在最后时刻，最优值函数为：
$$
V_T^*(s) =\max_a R^{(T)}(s,a)
$$

因此，在有限边界的MDP下，这实际上给了一个非常漂亮的动态规划算法。可以从计算$V^*_T$开始，之后反向求$V^*_{T-1},V^*_{T-2},...V^*_0$, 为所有步长计算出$V^*$。然后，最后一步是：
$$
\pi_t^*(s) = \arg\max_a R^{(t)}(s,a)+\sum_{s'} P_{sa}(s')V_{t+1}^*(s')
$$
在有限边界的情况下，最终的行动可能取决于what time it is。

## 线性二次型调节控制（LQR）
状态行为回报和有限边界的结合构成了一个特殊的有限边界MDP的模型，它被称为线性二次型调节控制(Linear Quadratic Regulation (LQR))，这个模型广泛应用于机器人学(robotics)中。它是有限边界的特例，所以五元组为：
$$
(S,A,\{P_{sa}\},T,R)
$$

首先我们明确下这个模型做了哪些假设。这些系统对于许多系统是合理的。 有了这些假设，可以得到很优雅的算法。

假设模型的状态和行动是连续的，并且：
$$
S=\mathbb{R}^n,A=\mathbb{R}^d
$$
状态转移概率是一个混合状态分布（mixed state distribution）。未来的状态是当前状态与行为的线性组合，且有噪音：
$$
P_{sa} : s_{t+1} = A_ts_t+B_ta_t+w_t, \text{where} \ w_t\sim N(0,\sigma_t)
$$
其中$A_t\in \mathbb{R}^{n\times n},B_T\in \mathbb{R}^{n\times d}$是两个矩阵，并且是可以随着时间改变的。因此是非平稳的动态。需要说明的是，我们这里假设这两个矩阵是固定的，也就是预先知道的。
噪音项$w_t$一般不重要，我们会看到只要噪音的高斯分布的均值为0，噪音不会影响到最优价值的计算。

我们继续假设奖励函数是为：
$$
R^{(t)}(s_t,a_t)= -\Big(s_t^TU_ts_t+a_t^TV_ta_t\Big)
$$
其中，$U_t\in \mathbb{R}^{n\times n},V_t\in \mathbb{R}^{d\times d}$是两个半正定矩阵，也就是说奖励函数的值永远是负数。
>注意：$s_t$上面的T是转置的意思，不是表示时间边界T。

![](https://raw.githubusercontent.com/fray-hao/images/master/20190610103204.png)

假设，你有一个直升机，你想要的状态$s_t\approx 0$,这时你可以选择$U_t=I，V_t=I$等于单位矩阵.这时奖励函数：
$$
R(s_t,a_t)= -s_t^Ts^t-a_t^Ta^t= -||s_t||^2-||a_t||^2
$$

奖励为状态与行为的二次方成本。在实践中，这往往是一个有效的系统。

接下来的几个步骤，是属于非稳态动力学（non-stationary dynamic）的。相当于普遍情况下的时间变化的动态（time varied dynamics）以及不同时间下的奖励函数（time varied reward function）

>这个所谓的微分动态规划的扩展

这个模型中关键的是动力学是线性的：$S_{t+1}=As_t+Ba_t$，还有假设奖励函数是二次的。那么，如何得到这样的模型呢？
以前面我们学习过的倒立摆为例，你可以做的事情是运行你的倒立摆，从状态0开始，采取一些行动，得新状态1，再采取行动，得到新状态,这样不断重复直至状态T。
然后，你可以重复多次：
![](https://raw.githubusercontent.com/fray-hao/images/master/20190610110154.png)
然后用线性回归（最小二次化误差项）的方法求得A和B，即：
$$
\arg\min_{A,B} \sum_{i=1}^m\sum_{t=0}^{T-1}||\Big(As_t^{(i)}+Ba_t^{(i)}\Big)||^2
$$

如果模型是非线性的：
$$
S_{t+1} = f(s_ta_t)
$$
例如，倒立摆：
$$w
S_{t+1} = f\begin{pmatrix}
    x_t
    \\ \dot{x_t}
    \\ \theta_t,a
    \\ \dot{\theta_t}
\end{pmatrix}
$$
之后进行线性化：
![](https://raw.githubusercontent.com/fray-hao/images/master/20190610112200.png)

上图中，横轴表示$s_t,a_t$,纵轴表示下一状态。在$\bar{s_t},\bar{{a_t}}$这一点，通过求函数f的导数，来估计下一个状态。
> 状态行动是成对的

当$s_{t+1}=f(s_t)$时,也就是没有行动a时，线性化后：
$$
s_{t+1} \approx f'(\bar{s_t})(s_t-\bar{s_t})+f(\bar{s_t})\qquad \bar{s_t} =constant
$$
然后，可以将这个转换为关于A，B的矩阵。
> 这里需要说明的是通用状态$\bar{s_t}$的选择，如果在左边很远的位置，这将不是一个准确逼近的位置。例如，倒立摆中，期望的状态往往是接近0的状态。如果s等于零，相当于X在倒立摆的铁路轨道的中心。你通常选择在任何状态下使杆子垂直，以接近0的速度在轨道中心。所以，通常选择接近0的s作为选择点来线性化倒立摆的状态。这是以你希望近似的很好的一个区域。

一般的线性近似公式是：
$$
s_{t+1} \approx f(\bar{s_t},\bar{a_t})+(\nabla_sf(\bar{s_t},\bar{a_t}))^T\cdot(s_t-\bar{s_t})+(\nabla_af(\bar{s_t},\bar{a_t}))^T(a_t-\bar{a_t})
$$
然后，可以将它转换为这样的矩阵：
$$
s_{t+1} = As_t+Ba_t
$$
我们知道了如何为线性或非线性模型进行建模。

现在，我将提出一个LQR的问题，即在前面的假设情况下，我们如何找出策略，最大化我们有限的奖励边界的总和。

首先，最大化有限的奖励总和的预期值。
$$
\max : R^{(1)}(s_0,a_0)+R^{(2)}(s_1,a_1)+...+R^{(T)}(s_T,a_T)
$$

https://blog.csdn.net/weixin_34388207/article/details/87028317