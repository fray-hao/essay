## 回顾
MDP是一个五元组：
![](https://raw.githubusercontent.com/fray-hao/images/master/20190605082650.png)
因此，我们有值迭代。经过一段时间迭代后，将导致V至V*的转换。我们找到了最优的值函数
![](https://raw.githubusercontent.com/fray-hao/images/master/20190605082822.png)
最后计算最优的策略。也就是找到a的最大值：
![](https://raw.githubusercontent.com/fray-hao/images/master/20190605083228.png)


## 变种
接下来，描述几个常见的变异MDPs。

### 1. State-action Rewards

将奖励函数改为从状态行为对（state action pair）映射到实数的函数：
$$
R: S_\lambda A\mapsto \mathbb{R}
$$
决策过程是一个状态行动的顺序：
![](https://raw.githubusercontent.com/fray-hao/images/master/20190605084553.png)

在这种设定下，整个过程的总奖励为：
![](https://raw.githubusercontent.com/fray-hao/images/master/20190605084801.png)

然后，像往常一样，我们的目标是找到一个策略。该策略寻找the function mapping from the state's action。当执行这一策略时，可以最大化总收益的预期值。所以，上面公式中的定义实际变成了MDP的状态行动回报。

使用状态行为回报（state action reward）使你能更直接的模式化问题。在不同的动作中，我们有不同的成本（costs）。比如，让机器人动起来比让它提留在原地的代价（costs）更昂贵，因为停留的回报可能有更低的成本（costs），这是因为你不适用电池电源。再比如户外驾驶，从非常粗糙的石子路或草地上驶过，要比柏油路上驶过的代价更昂贵、更困难，所以，你可以指定一个动作，要求驾驶在草地或石子路上比在柏油路上更昂贵。

在这种形式下，最优值函数：
![](https://raw.githubusercontent.com/fray-hao/images/master/20190605090246.png)

现在，看一下值迭代：
![](https://raw.githubusercontent.com/fray-hao/images/master/20190605090647.png)
最优策略：
![](https://raw.githubusercontent.com/fray-hao/images/master/20190605090809.png)
### 2. 有限边界的MDP(finite horizon MDP)

https://blog.csdn.net/weixin_34388207/article/details/87028317