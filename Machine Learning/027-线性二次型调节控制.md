## 回顾
MDP是一个五元组：
![](https://raw.githubusercontent.com/fray-hao/images/master/20190605082650.png)
因此，我们有值迭代。经过一段时间迭代后，将导致V至V*的转换。我们找到了最优的值函数
![](https://raw.githubusercontent.com/fray-hao/images/master/20190605082822.png)
最后计算最优的策略。也就是找到a的最大值：
![](https://raw.githubusercontent.com/fray-hao/images/master/20190605083228.png)


## 变种
接下来，描述几个常见的变异MDPs。

### 1. State-action Rewards

将奖励函数改为从状态行为对（state action pair）映射到实数的函数：
$$
R: S_\lambda A\mapsto \mathbb{R}
$$
决策过程是一个状态行动的顺序：
![](https://raw.githubusercontent.com/fray-hao/images/master/20190605084553.png)

在这种设定下，整个过程的总奖励为：
![](https://raw.githubusercontent.com/fray-hao/images/master/20190605084801.png)

然后，像往常一样，我们的目标是找到一个策略。该策略寻找the function mapping from the state's action。当执行这一策略时，可以最大化总收益的预期值。所以，上面公式中的定义实际变成了MDP的状态行动回报。

使用状态行为回报（state action reward）使你能更直接的模式化问题。在不同的动作中，我们有不同的成本（costs）。比如，让机器人动起来比让它提留在原地的代价（costs）更昂贵，因为停留的回报可能有更低的成本（costs），这是因为你不适用电池电源。再比如户外驾驶，从非常粗糙的石子路或草地上驶过，要比柏油路上驶过的代价更昂贵、更困难，所以，你可以指定一个动作，要求驾驶在草地或石子路上比在柏油路上更昂贵。

在这种形式下，最优值函数：
![](https://raw.githubusercontent.com/fray-hao/images/master/20190605090246.png)

现在，看一下值迭代：
![](https://raw.githubusercontent.com/fray-hao/images/master/20190605090647.png)
最优策略：
![](https://raw.githubusercontent.com/fray-hao/images/master/20190605090809.png)
### 2. 有限边界的MDP(finite horizon MDP)
有限边界MDP由以下5元祖构成：
$$
(S,A,\{P_{sa}\},T,R)
$$
- T : horizon time。 T是一个称为边界时间的参数。它的真正含义是：we‘ll be taking action in the MDP only for a total f capital T times。

在这种设定下，整个过程的奖励定义为：
$$
R(s_{\tiny0},a_{\tiny0})+R(s_{\tiny1},a_{\tiny1})+...+R(s_{\tiny T},a_{\tiny T})
$$

事实证明，最优的策略可能是非平稳的（non-stationary）。stationary， means that it doesn't depend on time.非平稳的意思是它可能取决于时间。因此，非平稳的大致意思是我采取的最佳行动将是随着不同的time steps而不同。
例如，假设我们有一个机器人，在左侧有1个加1的奖励，而在很远的右边有一个加10的奖励：
![](https://raw.githubusercontent.com/fray-hao/images/master/20190606081726.png)

因此，取决于你剩余多少时间，它可以取更好地取获得+1或+10的奖励。如果是处于游戏的初期，你仍然有大量的时间，它可能是更好地走向+10的奖励，以获得更大的奖励，如果你只剩下几个时间步长，那么就没有足够的时间去获得+10的奖励，你最好获得+1的奖励，它近的多。这个例子说明的是，当你再某一状态时，可以采取的最好的行动取决于时间。

因为在序列中，策略是非稳定的，过渡的概率也成为非平稳过渡的概率（non-stationary transistion probabilities）。在t+1时的状态is joined from 以前状态行为的状态转移概率：
$$
S_{t+1}\sim P^{(t)}_{s_{t,a}}
$$
例如，建立飞机的模型。如果飞机要飞很长时间。随着时间的推移，燃料燃烧，飞机变得更轻，所以飞机的动态策略随着时间的推移而变化，混合状态（mixed state）不仅取决于当前状态、行为，也取决于烧了多少燃料，也就是它处于什么时间
另一例子：你知道未来24小时内的大风和降水情况。从这里飞到纽约，在不同的时间，飞不同的路线的成本不同。
再比如：因为时间段不同，交通驾驶的成本是不同的。

这里状态转移概率是与时间有关的函数。

同样，奖励函数R也随着时间变化，这样，五元组表示为：
$$
(S,A,\{P^{t}_{sa}\},T,R^{(t)})
$$
整个过程的奖励为：
$$
R^{(0)}(s_{\tiny0},a_{\tiny0})+R^{(1)}(s_{\tiny1},a_{\tiny1})+...+R^{(t)}(s_{\tiny T},a_{\tiny T})
$$
那么，如何找到最佳策略呢？

首先，价值函数为：
$$
V_t(s) = \mathbb{E}\Big[R^{(t)}(s_t,a_t)+R^{(t+1)}(s_{t+1},a_{t+1})+...+R^{(T)}(s_T,a_T)|\pi,s_t=s\Big]
$$

在有限边界的MDP问题中，我们也采取基于迭代的算法来估算最优值函数：
$$
V_t^*(s)=\max_a R^{(t)}(s,a)+\sum_{s'}P^{(t)}_{s,a}(s')V^*_{t+}(s')
$$

在最后时刻，最优值函数为：
$$
V_T^*(s) =\max_a R^{(T)}(s,a)
$$

因此，在有限边界的MDP下，这实际上给了一个非常漂亮的动态规划算法。可以从计算$V^*_T$开始，之后反向求$V^*_{T-1},V^*_{T-2},...V^*_0$, 为所有步长计算出$V^*$。然后，最后一步是：
$$
\pi_t^*(s) = \arg\max_a R^{(t)}(s,a)+\sum_{s'} P_{sa}(s')V_{t+1}^*(s')
$$
在有限边界的情况下，最终的行动可能取决于what time it is。

## 线性二次型调节控制（LQR）
状态行为回报和有限边界的结合构成了一个特殊的有限边界MDP的模型，它被称为线性二次型调节控制(Linear Quadratic Regulation (LQR))，这个模型广泛应用于机器人学(robotics)中。它是有限边界的特例，所以五元组为：
$$
(S,A,\{P_{sa}\},T,R)
$$

首先我们明确下这个模型做了哪些假设。这些系统对于许多系统是合理的。 有了这些假设，可以得到很优雅的算法。

假设模型的状态和行动是连续的，并且：
$$
S=\mathbb{R}^n,A=\mathbb{R}^d
$$
状态转移概率是一个混合状态分布（mixed state distribution）。未来的状态是当前状态与行为的线性组合，且有噪音：
$$
P_{sa} : s_{t+1} = A_ts_t+B_ta_t+w_t, \text{where} \ w_t\sim N(0,\sigma_t)
$$
其中$A_t\in \mathbb{R}^{n\times n},B_T\in \mathbb{R}^{n\times d}$是两个矩阵，并且是可以随着时间改变的。因此是非平稳的动态。需要说明的是，我们这里假设这两个矩阵是固定的，也就是预先知道的。
噪音项$w_t$一般不重要，我们会看到只要噪音的高斯分布的均值为0，噪音不会影响到最优价值的计算。

我们继续假设奖励函数是为：
$$
R^{(t)}(s_t,a_t)= -\Big(s_t^TU_ts_t+a_t^TV_ta_t\Big)
$$
其中，$U_t\in \mathbb{R}^{n\times n},V_t\in \mathbb{R}^{d\times d}$是两个半正定矩阵，也就是说奖励函数的值永远是负数。
>注意：$s_t$上面的T是转置的意思，不是表示时间边界T。

![](https://raw.githubusercontent.com/fray-hao/images/master/20190610103204.png)

假设，你有一个直升机，你想要的状态$s_t\approx 0$,这时你可以选择$U_t=I，V_t=I$等于单位矩阵.这时奖励函数：
$$
R(s_t,a_t)= -s_t^Ts^t-a_t^Ta^t= -||s_t||^2-||a_t||^2
$$

奖励为状态与行为的二次方成本。在实践中，这往往是一个有效的系统。

接下来的几个步骤，是属于非稳态动力学（non-stationary dynamic）的。相当于普遍情况下的时间变化的动态（time varied dynamics）以及不同时间下的奖励函数（time varied reward function）

>这个所谓的微分动态规划的扩展

这个模型中关键的是动力学是线性的：$S_{t+1}=As_t+Ba_t$，还有假设奖励函数是二次的。那么，如何得到这样的模型呢？
以前面我们学习过的倒立摆为例，你可以做的事情是运行你的倒立摆，从状态0开始，采取一些行动，得新状态1，再采取行动，得到新状态,这样不断重复直至状态T。
然后，你可以重复多次：
![](https://raw.githubusercontent.com/fray-hao/images/master/20190610110154.png)
然后用线性回归（最小二次化误差项）的方法求得A和B，即：
$$
\arg\min_{A,B} \sum_{i=1}^m\sum_{t=0}^{T-1}||\Big(As_t^{(i)}+Ba_t^{(i)}\Big)||^2
$$

如果模型是非线性的：
$$
S_{t+1} = f(s_ta_t)
$$
例如，倒立摆：
$$w
S_{t+1} = f\begin{pmatrix}
    x_t
    \\ \dot{x_t}
    \\ \theta_t,a
    \\ \dot{\theta_t}
\end{pmatrix}
$$
之后进行线性化：
![](https://raw.githubusercontent.com/fray-hao/images/master/20190610112200.png)

上图中，横轴表示$s_t,a_t$,纵轴表示下一状态。在$\bar{s_t},\bar{{a_t}}$这一点，通过求函数f的导数，来估计下一个状态。
> 状态行动是成对的

当$s_{t+1}=f(s_t)$时,也就是没有行动a时，线性化后：
$$
s_{t+1} \approx f'(\bar{s_t})(s_t-\bar{s_t})+f(\bar{s_t})\qquad \bar{s_t} =constant
$$
然后，可以将这个转换为关于A，B的矩阵。
> 这里需要说明的是通用状态$\bar{s_t}$的选择，如果在左边很远的位置，这将不是一个准确逼近的位置。例如，倒立摆中，期望的状态往往是接近0的状态。如果s等于零，相当于X在倒立摆的铁路轨道的中心。你通常选择在任何状态下使杆子垂直，以接近0的速度在轨道中心。所以，通常选择接近0的s作为选择点来线性化倒立摆的状态。这是以你希望近似的很好的一个区域。

一般的线性近似公式是：
$$
s_{t+1} \approx f(\bar{s_t},\bar{a_t})+(\nabla_sf(\bar{s_t},\bar{a_t}))^T\cdot(s_t-\bar{s_t})+(\nabla_af(\bar{s_t},\bar{a_t}))^T(a_t-\bar{a_t})
$$
然后，可以将它转换为这样的矩阵：
$$
s_{t+1} = As_t+Ba_t
$$
我们知道了如何为线性或非线性模型进行建模。

现在，我将提出一个LQR的问题，即在前面的假设情况下，我们如何找出策略，最大化我们有限的奖励边界的总和。

首先，最大化有限的奖励总和的预期值。
$$
\max : R^{(1)}(s_0,a_0)+R^{(2)}(s_1,a_1)+...+R^{(T)}(s_T,a_T)
$$

我们想要计算$V^*_t$，我们用上一小节介绍的动态规划算法求解，具体来说就是先求出最后时刻T,然后求T-1,T-2,....。具体来说：
1. 初始化
   对于最后时刻的T，我们有：
   $$
   \begin{aligned}
       V_T^*(s_T) &= \max_{a_{T}}R^{(T)}(s_T,a_T)
       \\& = \max_{a_T}-s_T^TU_Ts_T-a^T_TV_Ta_T
       \\& = -s_T^TU_Ts_T\qquad (\text{because}\  a^T_TV_Ta_T\geq0, \text{因为我们假设} V_T\text{是半正定的，在最后时刻最好的行动是}a_T=0)
   \end{aligned}
   $$
   最佳的策略：
   $$
   \pi^*_T(s_T) = \arg\max_{a_{T}}R^{(T)}(s_T,a_T)=0
   $$
2. 动态规划
   我们的目标是在给定$V_{t+1}^*$时求出$V_{t}^*$。
   在以前的有限的DP中，
   $$
   \begin{aligned}
       V_T^*(s_t) &= \max_{a_t}R^{(t)}(s_t,a_t)+\sum_{s_{t+1}}P_{s_ta_t}(s_{t+1})V_{t+1}^*(s_{t+1})
       \\&=\max_{a_t}R^{(t)}(s_t,a_t)+E_{s_{t+1}\sim P_{s_ta_t}}\Big[ V^*_{t+1}(s_{t+1})\Big]
   \end{aligned}
   $$
   在LQR中，事实证明它具有以下有用的属性。每一个价值函数都可以表达为二次函数。具体来说，我们假设$V_{t+1}^*$是这样的一个二次函数：
   $$
   \begin{aligned}
       &V_{t+1}^*(s_{t+1}) = {s_{t+1}}^T\Phi_{t+1}s_{t+1}+\psi_{t+1}
       \\& \Phi_{t+1}\in \mathbb{R}^{n\times n}\quad \psi_{t+1}\in \mathbb{R}
   \end{aligned}
   $$
   然后，将$V_{t+1}^*$插入刚才定义的动态规划步骤的方程中，得到：
   $$
   V_t^*(s_t) = s_t^T\Phi_ts_t+\psi_t,\text{for some }\ \Phi_t,\psi_t
   $$
   当t=T时：
   $$
   V_T^*(s_T) = -s_T^TU_Ts_T
   $$
   也：
   $$
   V_T^*(s_T) = s_T^T\Phi_Ts_T+\psi_T
   $$
   所以：
   $$
   \Phi_T = -U_T, \psi_T=0,
   $$
    这样就可以开始递归了。
    ![](https://raw.githubusercontent.com/fray-hao/images/master/20190611085512.png)
    ![](https://raw.githubusercontent.com/fray-hao/images/master/20190611085710.png)
    上式中第二行用到了最优价值函数的定义，第三行是将假设中的定义进行了代入。注意表达式最后是关于at的二次参数，因此可以求出最优行动$a^*_t$为：
    ![](https://raw.githubusercontent.com/fray-hao/images/master/20190611090001.png)
    换句话说，在给定状态下的最佳行动将是与状态$s_t$线性相关的。求得最佳策略：
    ![](https://raw.githubusercontent.com/fray-hao/images/master/20190611090447.png)
    最佳行动实际上是状态的线性函数。
    基于$a^*_t$我们可以求出Φt和Ψt的值，它们之间的关系也被称为离散Ricatti方程(Discrete Ricatti equations):
    ![](https://raw.githubusercontent.com/fray-hao/images/master/20190611114904.png)

最后总结一下，有限边界的LQR问题求解的算法步骤如下：
1. 初始化参数$\Phi_T=-U_T,\psi_T=0$
2. 递归： 对每一个t = T - 1, ..., 1, 0，利用$\Phi_{t+1},\psi_{t+1}$和离散Ricatti方程求出$\Phi_t,\psi_t$。如果存在某个策略使得状态趋于0，那么迭代一定是收敛的。    

https://blog.csdn.net/weixin_34388207/article/details/87028317