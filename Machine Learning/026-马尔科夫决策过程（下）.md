## 回顾
先回顾（recap）一下上节课的内容。MDP是一个五元组$（S,A,\{P_{sa}\},\gamma,R）$。
然后举了一个机器人导航的例子：
![](https://raw.githubusercontent.com/fray-hao/images/master/20190529091026.png)
- S: 11个位置状态
- A： 四个行动方向
- $P_{sa}$: 向正确的目标方向移动的概率0.8，两侧移动的概率各为0.1
- R: 状态(4,3)的奖励为+1，状态(4,2)的奖励为-1，其它状态为-0.02
- $\gamma$: 0.99。 折扣因子，表明未来的选择回报小于现在的选择回报

我们的目标是找到策略$\pi$。它是一个函数映射：$S\mapsto A$。告诉我们在每一个状态采取什么行动。并且我们的目标是找到一个策略来最大化我们的总收益的预期值。

我们将策略的预期收益的价值函数定义为：
$$
V^\pi(s)= E\bigg[ R(s_{\tiny0})+\gamma R(s_{\tiny1})+\gamma^2 R(s_{\tiny2})+...\ \bigg|\ s_{\tiny0}=s,\pi \bigg]
$$
> 在策略$\pi$下，从状态s开始的总收益。

要找最好的策略，我们先找到最大化收益(期望值)的价值函数，定义为：
$$
V^*(s) = \max_\pi V^\pi(s)
$$
然后，根据$V^*(s)$就可以计算出最佳的策略：
$$
\pi^*=\arg\max_a\sum_{s'}P_{sa}(s')V^*(s')
$$

根据贝尔曼方程：
$$
V^*(s) = R(s)+\max_a\gamma\sum_{s'}P_{sa}(s')V^*(s')
$$
可以进行价值迭代算法、策略算法来求策略。

> 策略算法是给定一个固定的策略$\pi$,通过求解线性方程组来求预期收益$V^{\pi}$

## 离散化
到目前为止，我们一直都在讨论离散状态/有限状态下的MDP问题，现在我们来看下在连续状态（continous state）下如何求解MDP问题。

举例来说，如果你想要控制一辆车，一辆车可以被定位成位置和方向：$x,y,\theta$,以及马尔科夫化速度（Markov the velocity）：$\dot{x},\dot{y},\dot{\theta}$。所有这些都依赖与是否要对于运动和位置建模或是否要建立动力模型。

热带雨林中直升机飞行由位置，及旋转控制，马尔科夫化成线性速度与角速度，它的状态为：$x,y,z,\phi,\theta,\psi,\dot{x},\dot{y},\dot{z},\dot{\phi},\dot{\theta},\dot{\psi}$。

反向钟摆问题：铁路上运行的小车，如何保持杆子的平衡：
![](https://raw.githubusercontent.com/fray-hao/images/master/20190530092051.png)
这个问题的状态由车的位置以、杆子的旋转以及线速度和角速度构成：$x,\theta,\dot{x},\dot{\theta}$

这些例子都是连续状态的空间，怎么样用价值迭代或策略迭代来解决MDP，实现汽车控制、直升机控制以及倒摆问题呢？

也许求解无限状态下的MDP问题最简单的方法就是先将无限状态离散化成有限状态，然后再用之前介绍的价值迭代或者策略迭代算法了。

假设我们有两个状态s1和s2，我们可以用下图所示的网格来离散化这个状态空间：
![](https://raw.githubusercontent.com/fray-hao/images/master/20190530135000.png)
图中的每一个网格都代表独立的离散状态$\bar{s}$。有了离散的状态集$\bar{S}$就可以用价值迭代或策略迭代来求$V^*(\bar{s}),\pi^*(\bar{s})$。

机器人处于一定的状态，然后你会找出处于什么离散dygrid单元（discretized dygrid）中。
> dygrid 就是$\bar{s}$

![](https://raw.githubusercontent.com/fray-hao/images/master/20190530135817.png)
离散化的方法可以在很多场景都有很好的应用，但是它也有两个明显的缺点。第一个缺点是离散化只是对连续状态的近似，有时会有很大的误差。

为了更好地理解这一点，考虑如下的监督学习问题：
![](https://raw.githubusercontent.com/fray-hao/images/master/20190530142141.png)
如果我们用线性回归作拟合，那么拟合效果是很好的。但是如果我们用离散化的方法去作拟合，那么拟合效果就如下图所示：
![](https://raw.githubusercontent.com/fray-hao/images/master/20190530142243.png)
离散化的方法无法精确表示光滑曲线，如果需要降低误差，那么需要将离散化的粒度变得更小。

离散化的第二个缺点被称为维度的诅咒(curse of dimensionality)。假设我们把n维状态空间离散化成k份，那么所有离散状态的总数是$k^n$个。当n值变大时，所有离散状态的总数呈指数性增长。比如当n=10，k=100时，所有离散状态的总数是$100^{10} = 10^{20}$个，这个数字对于现在的计算机来说也是很难处理过来的。

例如，强化学习的一种相当知名的应用：工厂自动化（factory automations）。如果你的工厂的状态生产线上有20台机器，这些机器做流水线上的一部分工作，然后把那部分转交到不同的机器。依据不同机器可以做的事情的不同，你要用强化学习算法来规划不同机器做事情的顺序。因此，如果你有n台机器，并且没台机器可以有k个状态，如果用离散化来处理，状态的总数将会为$k^n$。

再比如，如果你有一个棋盘游戏，如果棋盘上有n块，如果没块可以位于k个位置，那么这也造成了维度祸根，最终在棋盘游戏中，离散状态的数量将会以指数形式增长。

维度祸根说明，离散化不能很好地处理高纬状态空间。作为一个经验法则，离散化通常对1维或2维状态的问题有较好的效果。如果处理得当，离散化也能很好处理4维状态。在极端情况下，离散化最多能处理到6维状态。一旦维数超过6，那么离散化将很难发挥出作用。

> 在大多数问题中，状态空间要比行为空间维度大的多，例如，驾驶汽车它的状态是6维的，而行为是2维的。所以，一般会将行为进行离散化，而不是状态离散化。
## 价值函数近似
现在我们介绍另一种求解无限状态下MDP问题的方法，这次我们来直接估计V*。这个方法叫做价值函数近似(value function approximation)，在强化学习问题中有着成功的应用。


假设有一个MDP模型（model）也称为模拟器(simulator)。这个模型是表示状态转移概率的。 具体来说，我们假设模拟器就是一个黑箱，它的输入是任意状态s和行动a，输出是根据状态转换概率$P_{sa}$得到的下一个状态s':
![](https://raw.githubusercontent.com/fray-hao/images/master/20190603104156.png)

我们可以有多种方法获得这个模型。一种方法是通过物理模拟，比如我们可以通过物理定律和已知参数进行推导，或者使用现成的物理模拟软件进行建模。
例如，你对倒立摆感兴趣，你的行动是a（向右或向左的力），状态是$x,\dot{x},\theta,\dot{\theta}$
![](https://raw.githubusercontent.com/fray-hao/images/master/20190603111530.png)
![](https://raw.githubusercontent.com/fray-hao/images/master/20190603112212.png)

另一种获得模型的方法从MDP的训练数据中进行学习。想象我们有一个物理的倒立摆系统，我们要做的是：初始化倒立摆设置为某种初始状态，然后执行一些策略。在状态为0时，采取行动$a_0$,系统将过渡到新的状态$s_1$，然后采取行动$a_1$,...,T个序列构成了轨迹。重复这个轨迹m次：
![](https://raw.githubusercontent.com/fray-hao/images/master/20190604080231.png)
然后就可以估计$s_{t+1}$；了。我们可以将$s_{t+1}$看成是一个以$s_t$和$a_t$}为参数的函数。例如，在倒立摆问题中，$s_{t+1},s_t,a_t\in \mathbb{R}^{4}$，都是4维向量，可以运行4次线性回归来预测每一个状态变量。例如，我们想预测下一个状态$s_{t+1}$，根据之前的状态线性函数和行为，可以建立如下的线性模型：
$$
s_{t+1}=A{s_t}+B{a_t}\qquad A\in\mathbb{R}^{4\times4},B\in\mathbb{R}^4
$$

通过线性回归的算法可以求得模型中的参数，也就是A和B两个矩阵。通过最大似然估计法，求最优值：
$$
\arg\min_{A,B}\sum_{i=1}^m\sum_{t=0}^{T-1}||s_{t+1}^{(i)}-(A{s_t}^{(i)}+B{a_t}^{(i)})||^2
$$
求出A和B两个参数后，一种方法是建立一个确定(deterministic)模型，也就是通过等式给定参数$s_t$和$a_t$来唯一预测$s_{t+1}$:
$$
s_{t+1}=A{s_t}+B{a_t}
$$
另一种方法是建立一个随机(stochastic)模型，也就是说$s_{t+1}$是关于输入的一个随机函数，这个模型可以表示为：
$$
s_{t+1}=A{s_t}+B{a_t}+\epsilon_t
$$
其中$ε_t$是噪音项，通常来说$ε_t \sim N(0, Σ)$。

上面我们假设$s_{t+1}$是关于当前状态和行动的线型函数，但在实际情况中，非线性函数也是有可能的。如果建立非线性函数的模型下一节再讲。

接下来我们讲：给定一个MDP的模拟器（给定一个模型），如何用算法来估计$V^*$。

理念是这样的：在线性回归算法中，给定输入特征X，我们选择其中一些特征来估算参数，我们将参数看做关于X的函数。这里的估计最优值函数做的是同样的事情。我们从状态s中选择一些特征$\phi(s)$。如果你想近似值函数为状态的一个线性函数，选择$\phi(s)=s$是一个合理的选择。但是，也可以选择为其它。例如，倒立摆问题中，我们选择：
$$
\phi(s) = \begin{bmatrix}
    1
    \\x
    \\\dot{x}^2
    \\\dot{x}
    \\\theta\cdot x
    \\\dot{\theta}^2
\end{bmatrix}
$$
然后，近似值函数为(pproximate the value function ）:
$$
V(s) = \theta^T\phi(s)
$$
> 就想我们在线性回归中所做的，目标是找到特征$\phi(s)$的一个线性组合，作为值函数$V(s)$的近似。

接下来，回顾一下离散状态下的值迭代：
$$
\begin{aligned}
    V(s):&=R(s)+\gamma\max_a\sum_{s'}P_{sa}(s')V(s')
    \\&=R(s)+\gamma\max_aE_{s'\sim P_{sa}}[V(s')]
\end{aligned}
$$

在离散的状态空间中，值迭代通过求和得出，而在连续的状态空间中，用积分的方式来替代求和：
$$
\begin{aligned}
    V(s):&=R(s)+\gamma\max_a\int_{s'} P_{sa}(s')V(s')
    \\&=R(s)+\gamma\max_aE_{s'\sim P_{sa}}[V(s')]
\end{aligned}
$$
现在我们介绍拟合价值迭代(fitted value iteration)算法，拟合价值迭代的中心思想是通过某个监督学习算法（这里我们用线性回归）来近似求出价值函数。即前面所述的：$V(s) = \theta^T\phi(s)$。

拟合价值迭代的步骤如下：
1. Random sample $\{s^{(1)},s^{(2)},...,s^{(m)}\}\in S$。 从状态中随机选择m个样本形成状态集。
2. Initialize $\theta := 0$ 。 初始化参数向量为0
3. 重复：
   ![](https://raw.githubusercontent.com/fray-hao/images/master/20190604143105.png)
    算法的每次循环中，首先每次取样出k个状态，计算出q(a),这个值是当前状态$s^{(i)}$选择行动a的期望，即：q(a) is estimate of $R(s^{(i)})+\gamma E_{s'\sim P_{s^{(i)}a}}[V(s')]$。然后令$y^{(i)}$为最大化a时的q(a)，即：$y^{(i)}$ is an estimate of  $R(s^{(i)})+\gamma \max_a E_{s'\sim P_{s^{(i)}a}}[V(s')]$,这个值就是对V(s)的近似（参考近似值函数）。最后通过应用监督学习算法（线性回归）使得V(s)与$y^{(i)}$尽可能的接近。

和有限状态的价值迭代算法不同，拟合价值迭代并不能保证算法总是收敛的。然而在实际应用中，算法通常是收敛的。注意，如果我们使用上一小节介绍的确定性的模型，那么价值迭代算法可以通过令k=1的方式进行简化。

最后，拟合价值迭代算法输出的是V，这是对V*的近似,那么如何去找到一个策略呢？当系统处于某个状态s时，我们需要选择一个行动，这个行动a将会是：
$$
\arg\max_a E_{s'\sim P_{sa}}[V^*(s')]
$$    

计算这个值的过程和拟合价值迭代算法的内层循环很相似。

如果我们采用的是一个确定性的模型，也就是说下一个状态就是关于之前的某状态下采用的行动(stated action)的函数：
$$
s_{t+1} = f(s_t,a_t)
$$
这种形式下，行动a的算法可以简化为：
$$
\arg\max_a V^*(f(s,a))
$$

如果你有一个随机模拟器，则：
$$
s_{t+1} = f(s_t,a_t)+\epsilon_t
$$
在这种情况下：
$$
E_{s'\sim P_{sa}}[V^*(s')]\approx V^*(E[s'])= V^*(f(s,a))
$$ 
所以采用的行动a：
$$
\arg\max_a V^*(f(s,a))
$$
> 它基本上忽视了模拟器的噪音。它往往处理的很好。因为，许多仿真器，结果是一些线性或一些非线性的函数加上零均值高斯噪音。

而$V^*(f(s,a))$即：
$$
\theta^T\phi(s')\quad\text{where s'=f(s,a)}
$$
https://www.jianshu.com/p/a03c307449ba

https://blog.csdn.net/weixin_34050005/article/details/87028324