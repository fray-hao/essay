## 回顾
先回顾（recap）一下上节课的内容。MDP是一个五元组$（S,A,\{P_{sa}\},\gamma,R）$。
然后举了一个机器人导航的例子：
![](https://raw.githubusercontent.com/fray-hao/images/master/20190529091026.png)
- S: 11个位置状态
- A： 四个行动方向
- $P_{sa}$: 向正确的目标方向移动的概率0.8，两侧移动的概率各为0.1
- R: 状态(4,3)的奖励为+1，状态(4,2)的奖励为-1，其它状态为-0.02
- $\gamma$: 0.99。 折扣因子，表明未来的选择回报小于现在的选择回报

我们的目标是找到策略$\pi$。它是一个函数映射：$S\mapsto A$。告诉我们在每一个状态采取什么行动。并且我们的目标是找到一个策略来最大化我们的总收益的预期值。

我们将策略的预期收益的价值函数定义为：
$$
V^\pi(s)= E\bigg[ R(s_{\tiny0})+\gamma R(s_{\tiny1})+\gamma^2 R(s_{\tiny2})+...\ \bigg|\ s_{\tiny0}=s,\pi \bigg]
$$
> 在策略$\pi$下，从状态s开始的总收益。

要找最好的策略，我们先找到最大化收益(期望值)的价值函数，定义为：
$$
V^*(s) = \max_\pi V^\pi(s)
$$
然后，根据$V^*(s)$就可以计算出最佳的策略：
$$
\pi^*=\arg\max_a\sum_{s'}P_{sa}(s')V^*(s')
$$

根据贝尔曼方程：
$$
V^*(s) = R(s)+\max_a\gamma\sum_{s'}P_{sa}(s')V^*(s')
$$
可以进行价值迭代算法、策略算法来求策略。

> 策略算法是给定一个固定的策略$\pi$,通过求解线性方程组来求预期收益$V^{\pi}$


https://www.jianshu.com/p/a03c307449ba