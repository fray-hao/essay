## 回顾
先回顾（recap）一下上节课的内容。MDP是一个五元组$（S,A,\{P_{sa}\},\gamma,R）$。
然后举了一个机器人导航的例子：
![](https://raw.githubusercontent.com/fray-hao/images/master/20190529091026.png)
- S: 11个位置状态
- A： 四个行动方向
- $P_{sa}$: 向正确的目标方向移动的概率0.8，两侧移动的概率各为0.1
- R: 状态(4,3)的奖励为+1，状态(4,2)的奖励为-1，其它状态为-0.02
- $\gamma$: 0.99。 折扣因子，表明未来的选择回报小于现在的选择回报

我们的目标是找到策略$\pi$。它是一个函数映射：$S\mapsto A$。告诉我们在每一个状态采取什么行动。并且我们的目标是找到一个策略来最大化我们的总收益的预期值。

我们将策略的预期收益的价值函数定义为：
$$
V^\pi(s)= E\bigg[ R(s_{\tiny0})+\gamma R(s_{\tiny1})+\gamma^2 R(s_{\tiny2})+...\ \bigg|\ s_{\tiny0}=s,\pi \bigg]
$$
> 在策略$\pi$下，从状态s开始的总收益。

要找最好的策略，我们先找到最大化收益(期望值)的价值函数，定义为：
$$
V^*(s) = \max_\pi V^\pi(s)
$$
然后，根据$V^*(s)$就可以计算出最佳的策略：
$$
\pi^*=\arg\max_a\sum_{s'}P_{sa}(s')V^*(s')
$$

根据贝尔曼方程：
$$
V^*(s) = R(s)+\max_a\gamma\sum_{s'}P_{sa}(s')V^*(s')
$$
可以进行价值迭代算法、策略算法来求策略。

> 策略算法是给定一个固定的策略$\pi$,通过求解线性方程组来求预期收益$V^{\pi}$

## 离散化
到目前为止，我们一直都在讨论离散状态/有限状态下的MDP问题，现在我们来看下在连续状态（continous state）下如何求解MDP问题。

举例来说，如果你想要控制一辆车，一辆车可以被定位成位置和方向：$x,y,\theta$,以及马尔科夫化速度（Markov the velocity）：$\dot{x},\dot{y},\dot{\theta}$。所有这些都依赖与是否要对于运动和位置建模或是否要建立动力模型。

热带雨林中直升机飞行由位置，及旋转控制，马尔科夫化成线性速度与角速度，它的状态为：$x,y,z,\phi,\theta,\psi,\dot{x},\dot{y},\dot{z},\dot{\phi},\dot{\theta},\dot{\psi}$。

反向钟摆问题：铁路上运行的小车，如何保持杆子的平衡：
![](https://raw.githubusercontent.com/fray-hao/images/master/20190530092051.png)
这个问题的状态由车的位置以、杆子的旋转以及线速度和角速度构成：$x,\theta,\dot{x},\dot{\theta}$
https://www.jianshu.com/p/a03c307449ba