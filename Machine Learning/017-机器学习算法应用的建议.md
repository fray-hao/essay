机器学习算法中有很多算法，那么在实际问题中应该怎样使用这些算法？当算法遇到瓶颈时该选择什么样的方向对算法进行改进？这篇文章既是解决这样的问题。这些建议针对已有的算法用到具体问题上，对发明新的算法用处不大。

本文聚集在三个主要内容：
- 调试ML算法时如何进行诊断（diagnostics for debugging learning algorithms）
- 误差分析与销蚀分析（error analysis and ablative analysis）
- 如何在一个ML问题上进行研究（how to get started on a machine learning problem）

在对具体的内容分析之前，先介绍一个很有用的建议，即避免过早优化（premature optimization）。

在我们进行项目开发或课题研究时，往往会遇到一些问题，在没有弄清楚问题之前，即没有明确的证据说明问题确实出在这里，我们往往想当然的对自己认为出问题的地方进行改进或优化，运气好时能把问题解决，运气不好则浪费了时间。

比如，在项目开发时，过早的优化不是瓶颈的代码段。例如，使用汇编语言来实现。虽然该代码段被优化得很快，但对系统的性能提高微乎其微，可以说是浪费了时间。而对于研究来说，过早的去做一些不能解决问题的事情浪费的时间可能会更多。因而，我们必须有一些能够判断问题所在的方法。这就引出了我们下面的内容。

## 1. 学习算法的调试问题

比如你想建立一个垃圾邮件识别系统。目前的研究现状如下：
- 经过仔细地**筛选**，在50000个特征（词）中选择了具有100个特征的很小的集合，来建立垃圾邮件识别系统
- 使用贝叶斯逻辑回归模型**算法**(BLR)
  $$
    \max_\theta\sum_{i=1}^m\log p(y^{(i)}|x^{(i)},\theta)-\lambda||\theta||^2
  $$
- 使用梯度下降算法实现，目前的**测试**误差率为20%

那么下一步应该做什么？

为了减少误差，改进算法。可能的解决方法如下：
- 提供跟多的训练样本
- 是否使用更少的特征
- 是否使用更多的特征
- 选取的特征是不是不够好，是否能找到更好的特征
- 梯度下降还没有完全收敛，可以多迭代几次
- 尝试其他的算法，例如牛顿方法。测试是否收敛性更好
- 调整贝叶斯逻辑回归模型中的$\lambda$
-  尝试用SVM算法，测试是否比逻辑回归算法更好

实际上可能的方法数以百计，我们只列出这8项。其中有些方法肯定有效， 如果一一尝试的话非常耗时。

面对这8种方法，如何选择改进的方向呢？只能靠运气吗？当然不是，如果我们能找到几种标准，排除上面大部分的方向，只保留1、2个，那么就可以节省很多时间。
### 1.1 方差/偏差分析
第一个标准就是判定问题是出在高方差还是高偏差上。一般来说，高方差(high variance)针对的是过拟合问题，即训练误差很小但泛化误差很大。而高偏差（high bias）针对的是模型本身不适合的问题，如特征数目过少等问题，表现既是训练误差和泛化误差都很大。示意图如下：

那么如何判断是高方差还是高偏差呢？

![](https://raw.githubusercontent.com/fray-hao/images/master/20190327093905.png)
由图1的高方差学习曲线可以看到，在高方差下，随着样本数的增加测试误差会呈下降趋势，而训练误差随着样本数的增加会单调上升，因为随着测试点的增加，它越来越难拟合出完美的曲线。**高方差的诊断方法**就是：在高方差下可以比较训练误差和测试误差，如果它们之间相差很大，那么可以通过增加样本数目使得模型的过拟合程度减少，从而提高性能。

由图2的高偏差学习曲线可以看到，在高偏差下，样本增加到一定程度后，测试误差就不变了，即使增加再多的样本数，误差程度也不会进一步缩小了；而训练误差会随着样本数的增加而递增。所以，在高偏差下，当你发现训练误差超过预期误差后，即使增加再多的样本，也不能把训练误差拉倒预期误差之下。

回到前面的修正措施列表，我们可以将前4条进行分类
- 提供跟多的训练样本 ： **可以修正高方差**
- 使用更少的特征：**可以修正高方差**
- 是否使用更多的特征： **可以修正高偏差**
- 找到更好的特征： **可以修正高偏差**

更多的样本使拟合变得平衡，可以解决高方差的问题；更少的特征也可以降低过拟合的程度，解决高方差问题。更多的特征和更好的特征可以增加模型的复杂度，提高模型在数据上的拟合程度，从而解决高偏差问题。

如果能够区别是高方差还是高偏差就可以排除掉上面四项措施中的2种，从而节省大量时间。


###1.2 收敛与目标函数是否正确的问题

考虑下面的例子，仍是针对垃圾邮件判断问题。
- 使用贝叶斯逻辑回归模型可以达到正常邮件（non spam）上2%的错误率，但是判断垃圾邮件上也有2%的错误率（也就是说2%的可能正常的邮件也被过滤了）。
- 使用svm和线性核算法可以达到判断正常邮件上10%的错误率，判断垃圾邮件上有0.1%的错误率

这个例子说明的是，你建立垃圾邮件判别系统时，你特别希望使用逻辑回归实现，因为它计算效率很高，或者需要随时进行学习，或者因为逻辑回归运行更简单、更快速。但是，它的运行效果并不好，将2%的邮件误判为垃圾邮件是不可接受的。

所以，应该怎么办？伴随着这个问题的是算法的收敛性。

你可能怀疑BLR没有收敛，可以再进行多次迭代。实际上，如果你考虑逻辑回归的优化目标。比如，逻辑回归是J(θ)的话，如果将这个函数作为迭代次数的函数，迭代次数与目标函数的趋势图如下：
![](https://raw.githubusercontent.com/fray-hao/images/master/20190327122354.png)
但是这样的趋势图在通常情况下不能分辨出目标函数是否已收敛。因为在训练的后期目标函数的每步优化往往都只能提高一点点。所以相对于查看曲线的方法，我们需要更好的判断逻辑回归是否收敛的方法。

另外一个问题是BLR模型的优化目标函数是否找对？例如，在垃圾邮件识别的例子里，你可能会关心这样的一个加权准确率函数：
$$
a(\theta) = \frac{1}{m}\sum_{i=1}^m w^{(i)}1\begin{Bmatrix}
  h_\theta(x^{(i)})= y^{(i)}
\end{Bmatrix}
$$
$a(\theta)$是对所有预测正确样本的加权求和。非垃圾邮件的权值相对于垃圾邮件要高点。因为相对于垃圾文件，你更希望正确的预测非垃圾邮件。

第二种诊断方法可以帮助你判断，问题到底是出在：
- 算法的收敛
- 目标函数的选择上

假设svm比逻辑回归的效果更好，这就意味着svm的加权准确率要高于逻辑回归的加权准确率：
$$
a(\theta_{svm})>a(\theta_{BLR})
$$
> $\theta_{svm}$表示由svm算法生成的参数；$\theta_{BLR}$表示由贝叶斯逻辑回归生成的参数。

现在，我们可以通过判断在BLR的目标函数上，$\theta_{svm}$与$\theta_{BLR}$的表现，来判断到底发生了什么问题。
BLR尝试最优化的目标函数：
$$
J(\theta) = \sum_{i=1}^m\log p(y^{(i)}|x^{(i)},\theta)-\lambda ||\theta||^2
$$
可能发生两种情况：
$$
\begin{aligned}
  &a(\theta_{svm})>a(\theta_{BLR})
  \\&J(\theta_{svm})>J(\theta_{BLR})
\end{aligned}
$$
因为贝叶斯逻辑回归要将$J(\theta)$最大化，$J(\theta_{svm})>J(\theta_{BLR})$意味着目前由贝叶斯逻辑回归输出的参数无法使得$J(\theta)$最大化，因为svm生成的参数可以使$J(\theta)$取一个更大的值。所以，这告诉我们BLR算法的收敛程度没有svm高，造成了其在实际问题中表现差，这个时候就需要改进训练算法，使之收敛程度更高。
$$
\begin{aligned}
  &a(\theta_{svm})>a(\theta_{BLR})
  \\&J(\theta_{svm})<J(\theta_{BLR})
\end{aligned}
$$

$J(\theta_{svm})<J(\theta_{BLR})$意味着,在目标函数的最大化上，贝叶斯逻辑回归比svm做的更好。但是对于我们所关心的加权准确率。svm却获得了更好的结果，这意味着某些参数即使不能使目标函数J最大化，也能获得更好的加权准确率。最大化$J(\theta)$并不一定意味着最大化加权准确率。这告诉我们，也许$J(\theta)$是一个错误的优化目标函数，此时应该改进目标函数

回到我开头列出的后四项改进措施上：
- 梯度下降还没有完全收敛，可以多迭代几次。 **解决收敛程度问题，改进算法**
- 尝试其他的算法，例如牛顿方法。测试是否收敛性更好**解决收敛程度问题，改进算法**
- 调整贝叶斯逻辑回归模型中的$\lambda$ **改进目标函数**
-  尝试用SVM算法，测试是否比逻辑回归算法更好**改进目标函数** 

### 1.3 诊断实例

Ng以他的某些学生正在做的一个自动驾驶直升机飞行的项目作为例子来分析如何找到问题所在。首先，对于一个能自动驾驶直升机的程序来说，要经过如下步骤：
1. 建立一个精确的模拟器。
2. 选择一个损失函数J
3. 使用强化学习算法（Reinforcement Learning，RL）来对损失函数进行优化，输出参数$\theta_{RL}$。之后，我们会用这些参数来进行直升机的控制。

直升机控制的效果很差，我们进行相反的假设。假设下面的条件都成立：
- 直升机的模拟器很精确
- 假设强化学习算法能够在模拟条件下正确地控制飞机。也就是强化学习算法能够使损失函数$J(\theta)$最小化，从而能正确地控制直升机
- 再假设$J(\theta)$最小化对应着正确飞行。

如果这些条件都成立，那么参数$\theta_{RL}$就可以精确地控制直升机了。但是事实上$\theta_{RL}$并不能很好的控制直升机。这意味着上面的假设中至少有一项不成立。如何找出问题所在呢？

我们用的诊断方法是这样的。

https://blog.csdn.net/stdcoutzyx/article/details/18500441