例如垃圾邮件分类。我们假设：
$y\in\{0,1\}$

> 1 表示垃圾邮件

我们要做的第一个决策是：给你一封邮件，你怎么用一个特征向量x来表示这封邮件。

> 电子邮件仅仅是一段文本，就像一个词列表（list of words）

当收到一封邮件时，我们会遍历词典，并得到词典中词的列表
```
a
ardwork
ausworth
.
.
.
buy
.
.
.
zymurgy
```
一旦发现某个词，出现在邮件中，就将相应位置设为1。例如，我的邮件中出现了a,buy这两个词，那么特征向量X则为：
![](https://raw.githubusercontent.com/fray-hao/images/master/20190225105258.png)

> 这是一种创建特征向量来表示邮件的方式。

接下来，需要建模：
$
p(x|y)
$
> 给定y=1或y=0输出x。

我们的x是一个n维的值为0、1的向量：
$
 x \in\{0,1\}^n
$

如果我们的词典有50000个词，那么n就是50000。对于x来说，就有$2^{50000}$种可能的值。一种对其建模的方法就是就是使用多项式分布，但是由于x有$2^{50000}$个可能，那么就需要$2^{50000}-1$个参数。这参数就太多了。

## 1. 朴素贝叶斯原理推导
与GDA类似，朴素贝叶斯对一个测试样本分类时，通过比较p(y=0|x)和p(y=1|x)来进行决策。根据贝叶斯公式：

$
\begin{aligned} p(y=1|x)=\frac{p(x|y=1)p(y=1)}{p(x)} \end{aligned}
$

其中x是一个多维向量，x=(x1,x2,…,xn)，则:

p(x|y)=p(x1|y)p(x2|y,x1)p(x3|y,x1,x2)…p(xn|y,x1,x2,…,xn−1)

上面这个式子如此多的条件概率，可没法求呀。那么就限定一下条件，使得特殊情况下可以求解，于是就有了下面这个很Naive的假设：

$\footnotesize 给定类别y的条件下，x_i是条件独立（conditional independent）的$

例如，$p(x_1,x_2,\dots,x_{50000}|y)$。根据概率论的链式法则，它应该等于

$\begin{aligned}
&p(x_1,x_2,\dots,x_{50000}|y)
\\&=p(x_1|y)p(x_2|y,x_1)p(x_3|y,x_1,x_2)\dots p(x_{50000}|y,x_1,\dots,x_{49999})  \qquad \footnotesize\#链式法则
\\&=p(x_1|y)p(x_2|y)p(x_3|y)\dots p(x_{50000}|y)  \qquad \footnotesize\#贝叶斯假设
\\& =\Pi_{i=1}^{5000} p(x_i|y)
\end{aligned}$

贝叶斯假设一个词的出现不会影响其他词出现的概率。这种假设明显是错误的，例如，出现了"cs229"，那么有大概率出现"机器学习"。但是，事实证明，虽然这个假设在字面意义上是错的，但是朴树贝叶斯算法仍然是一个非常有效的算法，可以对文本文档进行分类。例如，判定它是否是垃圾邮件，或者自动将邮件分到不同的类别中。

朴素贝叶斯的参数：
$\begin{aligned}
 &\phi_{i|y=1} = p(x_i=1|y=1)
 \\ &\phi_{i|y=0} = p(x_i=1|y=0)    
 \\& \phi_{y} = p(y=1)
\end{aligned}$

为了拟合出模型的参数，需要用到联合似然（joint likelihood）：
$
\begin{aligned}
l(\phi_y,\phi_{i|y=1},\phi_{i|y=0}) &= \Pi_{i=1}^mp(x^{(i)},y^{(i)})
\\&= \Pi_{i=1}^m\Big( \Pi_{j=1}^{n_i}p(x_j^{(i)}|y;\phi_{i|y=0},\phi_{i|y=1})  \Big)p(y^{(i)};\phi_y)
\end{aligned}
$

与高斯判别分析中的推导类似，对似然函数取对数并最大化，通过求导可求解出:

$\begin{aligned}
\phi_{j|y=1}  = \frac{\sum_{i=1}^m\mathbb{1}\{ x_j^{(i)}=1,y^{(i)}=1 \}}{\sum_{i=1}^m\mathbb{1}\{y^{(i)}=1\}}
\end{aligned}$
>式中1为指示函数

> 分子的含义：遍历整个训练集合，对于标签y=1的邮件，计算其中词语j出现的邮件数目之和。换句话说，就是遍历所有的垃圾邮件，统计这些垃圾邮件中包含词语j的邮件数目。
> 分母的含义： 对i从1到m求和，最后得到垃圾邮件的总数。分母就是训练样本中垃圾邮件的数目
> 比例的含义：所有垃圾邮件中j出现的比例（概率）

$\begin{aligned}
\phi_{j|y=0}  = \frac{\sum_{i=1}^m\mathbb{1}\{ x_j^{(i)}=1,y^{(i)}=0 \}}{\sum_{i=1}^m\mathbb{1}\{y^{(i)}=0\}}
\end{aligned}$

$\begin{aligned}
\phi_{y} = \frac{\sum_{i=1}^m\mathbb{1}\{ y^{(i)}=1\}}{m} 
\end{aligned}$

给你一封新邮件时，使用贝叶斯公式，结合p(x|y)、p(y)就可以求出p(y|x)

## 2.拉普拉斯平滑（Laplace smoothing）
对于上面推导的公式，假设xi的理论取值范围是0~99，但我们的训练样本中，没有任何一个样本中出现过99这个值，那么按照上面贝叶斯公式计算，会出现0/0的情况：

$\begin{aligned}
  p=(y=1|x) &= \frac{p(x|y=1 )p(y=1)}{p(x|y=1 )p(y=1)+p(x|y=0)p(y=0)} 
 \\& =\frac{\Pi_{i=1}^{99}p(x_i|y=1)p(y=1)}{...} 
 \\&= \frac{\Pi_{i=1}^{99}p(x_{99}|y=1)p(y=1)}{...} 
 \\& = \frac{0}{0+0}
\end{aligned}$

> 如果仅仅因为你之前没有见过一些事件，你就认为这些事件不太可能发生。但是这并不表示这些事件不可能发生。

我们来看下拉普拉斯的概念。

我们想要预测概率(p(y=1)),通常情况下根据极大似然估计，我们得到的结果应该是1的数目