例如垃圾邮件分类。我们假设：
$y\in\{0,1\}$

> 1 表示垃圾邮件

我们要做的第一个决策是：给你一封邮件，你怎么用一个特征向量x来表示这封邮件。

> 电子邮件仅仅是一段文本，就像一个词列表（list of words）

当收到一封邮件时，我们会遍历词典，并得到词典中词的列表
```
a
ardwork
ausworth
.
.
.
buy
.
.
.
zymurgy
```
一旦发现某个词，出现在邮件中，就将相应位置设为1。例如，我的邮件中出现了a,buy这两个词，那么特征向量X则为：
![](https://raw.githubusercontent.com/fray-hao/images/master/20190225105258.png)

> 这是一种创建特征向量来表示邮件的方式。

接下来，需要建模：
$
p(x|y)
$
> 给定y=1或y=0输出x。

我们的x是一个n维的值为0、1的向量：
$
 x \in\{0,1\}^n
$

如果我们的词典有50000个词，那么n就是50000。对于x来说，就有$2^{50000}$种可能的值。一种对其建模的方法就是就是使用多项式分布，但是由于x有$2^{50000}$个可能，那么就需要$2^{50000}-1$个参数。这参数就太多了。

## 1. 朴素贝叶斯原理推导
与GDA类似，朴素贝叶斯对一个测试样本分类时，通过比较p(y=0|x)和p(y=1|x)来进行决策。根据贝叶斯公式：

$
\begin{aligned} p(y=1|x)=\frac{p(x|y=1)p(y=1)}{p(x)} \end{aligned}
$

其中x是一个多维向量，x=(x1,x2,…,xn)，则:

p(x|y)=p(x1|y)p(x2|y,x1)p(x3|y,x1,x2)…p(xn|y,x1,x2,…,xn−1)

上面这个式子如此多的条件概率，可没法求呀。那么就限定一下条件，使得特殊情况下可以求解，于是就有了下面这个很Naive的假设：

$\footnotesize 给定类别y的条件下，x_i是条件独立（conditional independent）的$

例如，$p(x_1,x_2,\dots,x_{50000}|y)$。根据概率论的链式法则，它应该等于

$\begin{aligned}
&p(x_1,x_2,\dots,x_{50000}|y)
\\&=p(x_1|y)p(x_2|y,x_1)p(x_3|y,x_1,x_2)\dots p(x_{50000}|y,x_1,\dots,x_{49999})  \qquad \footnotesize\#链式法则
\\&=p(x_1|y)p(x_2|y)p(x_3|y)\dots p(x_{50000}|y)  \qquad \footnotesize\#贝叶斯假设
\\& =\Pi_{i=1}^{5000} p(x_i|y)
\end{aligned}$

贝叶斯假设一个词的出现不会影响其他词出现的概率。这种假设明显是错误的，例如，出现了"cs229"，那么有大概率出现"机器学习"。但是，事实证明，虽然这个假设在字面意义上是错的，但是朴树贝叶斯算法仍然是一个非常有效的算法，可以对文本文档进行分类。例如，判定它是否是垃圾邮件，或者自动将邮件分到不同的类别中。

朴素贝叶斯的参数：
$\begin{aligned}
 &\phi_{i|y=1} = p(x_i=1|y=1)
 \\ &\phi_{i|y=0} = p(x_i=1|y=0)    
 \\& \phi_{y} = p(y=1)
\end{aligned}$

为了拟合出模型的参数，需要用到联合似然（joint likelihood）：
$
\begin{aligned}
l(\phi_y,\phi_{i|y=1},\phi_{i|y=0}) &= \Pi_{i=1}^mp(x^{(i)},y^{(i)})
\\&= \Pi_{i=1}^m\Big( \Pi_{j=1}^{n_i}p(x_j^{(i)}|y;\phi_{i|y=0},\phi_{i|y=1})  \Big)p(y^{(i)};\phi_y)
\end{aligned}
$

与高斯判别分析中的推导类似，对似然函数取对数并最大化，通过求导可求解出:

$\begin{aligned}
\phi_{j|y=1}  = \frac{\sum_{i=1}^m\mathbb{1}\{ x_j^{(i)}=1,y^{(i)}=1 \}}{\sum_{i=1}^m\mathbb{1}\{y^{(i)}=1\}}
\end{aligned}$
>式中1为指示函数

> 分子的含义：遍历整个训练集合，对于标签y=1的邮件，计算其中词语j出现的邮件数目之和。换句话说，就是遍历所有的垃圾邮件，统计这些垃圾邮件中包含词语j的邮件数目。
> 分母的含义： 对i从1到m求和，最后得到垃圾邮件的总数。分母就是训练样本中垃圾邮件的数目
> 比例的含义：所有垃圾邮件中j出现的比例（概率）

$\begin{aligned}
\phi_{j|y=0}  = \frac{\sum_{i=1}^m\mathbb{1}\{ x_j^{(i)}=1,y^{(i)}=0 \}}{\sum_{i=1}^m\mathbb{1}\{y^{(i)}=0\}}
\end{aligned}$

$\begin{aligned}
\phi_{y} = \frac{\sum_{i=1}^m\mathbb{1}\{ y^{(i)}=1\}}{m} 
\end{aligned}$

给你一封新邮件时，使用贝叶斯公式，结合p(x|y)、p(y)就可以求出p(y|x)

## 2.拉普拉斯平滑（Laplace smoothing）
对于上面推导的公式，假设xi的理论取值范围是0~99，但我们的训练样本中，没有任何一个样本中出现过99这个值，那么按照上面贝叶斯公式计算，会出现0/0的情况：

$\begin{aligned}
  p=(y=1|x) &= \frac{p(x|y=1 )p(y=1)}{p(x|y=1 )p(y=1)+p(x|y=0)p(y=0)} 
 \\& =\frac{\Pi_{i=1}^{99}p(x_i|y=1)p(y=1)}{...} 
 \\&= \frac{\Pi_{i=1}^{99}p(x_{99}|y=1)p(y=1)}{...} 
 \\& = \frac{0}{0+0}
\end{aligned}$

> 如果仅仅因为你之前没有见过一些事件，你就认为这些事件不太可能发生。但是这并不表示这些事件不可能发生。

我们来看下拉普拉斯的概念。
拉普拉斯平滑（Laplace Smoothing）又被称为加1平滑，是比较常用的平滑方法。平滑方法的存在是为了解决零概率问题。
 
所谓的零概率问题，就是在计算新实例的概率时，如果某个分量在训练集中从没出现过，会导致整个实例的概率计算结果为0。针对文本分类问题就是当一个词语在训练集中没有出现过，那么该词语的概率为0，使用连乘计算文本出现的概率时，整个文本出现的概率也为0。这显然是不合理的，因为不能因为一个事件没有观测到就判断该事件的概率为0。

例如，我们想要预测概率(p(y=1)),通常情况下根据极大似然估计，我们得到的结果应该是1的数目
$$\begin{aligned}
  P(y=1)=\frac{\#"1"s}{\#"0"s+\#"1"s}
\end{aligned}$$

而拉普拉斯平滑就是将所有数字加上1：
$$\begin{aligned}
  P(y=1)=\frac{\#"1"s+1}{\#"0"s+1+\#"1"s+1}
\end{aligned}$$

例如，一个球队进行比赛，前5场都输，那么第6场赢的概率为：

$$P(y=1) = \frac{0+1}{5+1+0+1}=\frac{1}{7}$$

更一般性的，如果y可以取k种可能的值，对于m次观察结果{$y^{(1)}、y^{(2)}、\dots、y^{(k)}$},极大似然估计公式为：

$$
\begin{aligned}
  \phi_j = \frac{\sum_{i=1}^m\mathcal{I}\{y^{(i)}=j\}}{m}
\end{aligned}
$$

使用Laplace平衡后，计算公式变为：

$$
\begin{aligned}
 \phi_j = \frac{\sum_{i=1}^m\mathcal{I}\{y^{(i)}=j\}+1}{m}+k 
\end{aligned}
$$

回到朴素贝叶斯算法，我们可以修正各分量的计算公式：

$$
\begin{aligned}
&\phi_{j|y=1}  = \frac{\sum_{i=1}^m\mathcal{I}\{ x_j^{(i)}=1,y^{(i)}=1 \}+1}{\sum_{i=1}^m\mathcal{I}\{y^{(i)}=1\}+2}
\\&\phi_{j|y=0}  = \frac{\sum_{i=1}^m\mathcal{I}\{ x_j^{(i)}=1,y^{(i)}=0 \}+1}{\sum_{i=1}^m\mathcal{I}\{y^{(i)}=0\}+2}
\end{aligned}
$$

## 3.朴素贝叶斯变种

在前两节的垃圾邮件分类中，我们创造了一个这样的特征向量X，每一项对应着词典中的一个词，之后，根据每个词是否出现在邮件中，我们得到了X每项的值：
![](https://raw.githubusercontent.com/fray-hao/images/master/20190225105258.png)

朴素贝叶斯是一个生成学习算法。这意味着这个算法是对P(x|y)进行建模。对于朴素贝叶斯来说，模型为：
$$
P(x|y)=\Pi_{i=1}^nP(x|y)
$$
与此同时，还需要对P(y)进行建模。之后，用贝叶斯公式将两项合起来。所以我们需要预测给定的邮件是否为垃圾邮件，所以你会得到：

$$
\arg\max_yP(y|x) = \arg\max_yP(x|y)P(y)
$$

这就是朴素贝叶斯方法
> 在整个算法中x的取值范围为$x\in \{0,1\}$,表示是否是垃圾邮件； 特征向量X的长度应该等于词典中词的数目

这里介绍两个朴素贝叶斯算法的两个变化（variation）版本。

第一个是一个普适性（generalization）变化。可以让X取更多值：
$$x\in{1,2,\dots ,k}$$
这个算法的模型与标准算法的模型一致：
$$P(x|y)=\Pi_{i=1}^nP(x_i|y)$$
看起来一样，区别在于，$x_i$可以取多个值，这里是一个多项式分布，而不是伯努利分布。

例如，房价预测问题。根据房屋的特征，预测房屋是否会在未来六个月内卖掉，这是一个分类问题。一旦使用朴素贝叶斯方法，之后会给定一个连续取值的特征，比如，房屋的面积。通常要做的工作是将这些连续取值的房屋面积分散成几个离散的区段：

| areas   |      <500      |  1000-1500 |1500-2000|>2000
|----------|:------:|:------:|:------:|:------:|
| $x_i=$ |  1 | 2 |3|4

之后根据房屋的面积在哪个区间为条件为特征取想要的值。

第二种朴素贝叶斯的变种是用来专门处理文本文档的，或者更一般的说是用来对序列进行分类的。
算法的思想：在朴素贝叶斯算法中，x取值为0，1，丢失了很多信息，它只会记录某个词是否在文本中出现了，并不能显示每一个词出现的次数。如某个词出现的次数越多，有可能表明它更像垃圾邮件。我们这里的算法叫朴素贝叶斯的事件模型，会考虑到每个词出现在邮件中的次数

> 前面的用于文本分类的模型被称为：多元伯努利的事件模型（Multivariate Bernoulli Event model）。而我们这里用的模型叫做多项式事件模型（Multinomial Event Model）.

我们换一个思路，这次我们不先从词典入手，而是从邮件入手。样本中的第i封邮件的特征向量则为：
$$X =\{x_1^{(i)},x_2^{(i)},\dots , x_{n^i}^{(i)}\}$$
> $n^i$表示的是第i封邮件中词的数量。

特征向量中的每一个元素的值会是到词典的一个索引。例如， 例如，若邮件以“A NIPS…”开头，若”A”是字典中的第1个单词，“NIPS”是字典中的第35000个单词，那么$x_1=1,x_2=35000$。显然，这里的$x_i$已经不再是二值（0，1）的了，而是指向词典的索引的，所以该模型称作多项式事件模型。 

在这种情况下，生成模型为：
$$p(y|x)=(\Pi_{i=1}^np(x_i|y))p(y)$$

生成过程是这样的：首先，y的值也就是种类标签，被选中，也就是表示邮件是否是垃圾邮件的类别标签被生成出来。然后，遍历邮件中所有的词的位置，按照某种概率分布生成一些词。基于它们是否选择向你发送邮件，如果他们向你发送垃圾邮件，那么他们会倾向于向内发送垃圾邮件。

它的参数：

$$
\begin{aligned}
\Phi_{k|y=1} = p(x_j=k|y=1)  
\end{aligned}
$$
> 