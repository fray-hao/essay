## 1. 复习上节课内容
支持向量机的定义：
$$
\begin{aligned}
  &h_{w,b} = g(w^Tx+b) 
  \\& g(z) \text{\textasciitilde}  \begin{cases}
      1& if\quad z\geq0
      \\-1 & otherwise 
  \end{cases}
\end{aligned}
$$
标签用-1，+1表示：
$$
y\in\{-1,1\}
$$

函数间隔：
$$
\hat{\gamma}^{(i)} = y^{(i)}(w^Tx^{(i)}+b)
$$
几何间隔：
$$
\gamma^{(i)} = y^{(i)}（\frac{w^T}{||w||}x^{(i)}+\frac{b}{||w||}）
$$
几何间隔的含义可以这样解释。如下图：
![](https://i.loli.net/2019/03/05/5c7dfe46e81e1.png)
如上图，几何间隔表示的是一个训练样本和超平面之间的距离，它实际上是一个有符号的距离。如果样本分类是正确的，它是正的；如果分类错误，则是负的。分割样本的超平面由$w^Tx+b=0$定义。

而整体样本的函数间隔：
$$\hat{\gamma} = \min_i\hat\gamma^{(i)}$$
整体样本的几何间隔：
$$\gamma = min_i\gamma^{(i)}$$
> 考虑的是最坏的情况

对于几何间隔，可以按任意比例缩放w和b，而不会改变结果。因为，$w^Tx+b=0$与$2w^Tx+2b=0$是同一个超平面。
比如，我们可以添加一个这样的约束：
$$||w||=1$$
这样，可以先求出w和b的解，之后通过重新缩放这些参数，使之满足上述条件，使w的模变成1。
还可以添加其他诸如$|w_1|=1,w_1^2+|w_2|=1$等约束。

## 2. 几何间隔最大化的优化
在w的模等于1的情况下，几何间隔等于函数间隔。我们要找到一个最大化的$\gamma$,并且所有训练样本的函数间隔（即几何间隔）大于或等于$\gamma$：
$$
\begin{aligned}
&\max_{\gamma,w,b}\gamma
\\&s.t.\quad y^{(i)}(w^Tx^{(i)}+b) \geq \gamma ，||w||=1
\end{aligned}
$$

求解了这个最优问题之后，就可以得到最优间隔分类器（optimal margin classifier）。但是，这并不是一个比较好的优化接近问题，因为约束“||w||=1”是一个非常糟糕的非凸性约束（nonconvex constraints）。要求的w位于一个球体的表面上。

如果我们要想得到一个凸优化问题，我们必须要保证诸如梯度下降算法这样的局部最优值搜素算法，不会找到局部最优值。
所以，我们需要改变优化问题：
$$
\begin{aligned}
&\max_{\hat{\gamma},w,b} \frac{\hat{\gamma}}{||w|
}   
\\&s.t. y^{(i)}(w^Tx+b)\geq\hat{\gamma}
\end{aligned}
$$
> 需要满足的条件是每一个样本的函数间隔都要大于等于整体的函数间隔，但是我们要最优化（大化）的却不是函数间隔而是几何间隔（因为$\gamma = \frac{\hat{\gamma}}{||w|
} $）。

在这个优化问题中，我们避免了非凸性约束，但是又产生了一个非凸性优化目标：$\frac{\hat{\gamma}}{||w|
} $。我们可以添加一个缩放约束条件：
$$\hat{\gamma}=1$$
也就是：
$$\min_{i}y^{(i)}(w^Tx^{(i)})+b=1$$

通过对w,b进行调整，将函数间隔变为1。将这个缩放约束加入上面的第二个约束问题：

$$
\begin{aligned}
&\max_{w,b} \frac{1}{||w|
}   
\\&s.t.\quad  y^{(i)}(w^Tx+b)\geq1
\end{aligned}
$$
也就是：

$$
\begin{aligned}
&\min_{w,b} ||w||^2
\\&s.t.\quad y^{(i)}(w^Tx+b)\geq1
\end{aligned}
$$
这个就是最优间隔分类器的最终的形式化表示。“$||w||^2$”是一个二次凸函数，可以对其优化 

##3. 原始问题与对偶优化问题
primal and duo optimization problems

### 3.1 原始问题
首先介绍下拉格朗日乘数法（lagrange multipliers）。假设你想使函数f(w)最大化或最小化，与此同时需要满足一些约束：
$$
\begin{aligned}
&\min_wf(w)
\\& s.t.\quad h_i(w) =0;i=1,...,n \qquad\footnotesize\# 对于每个i，约束函数的值都是0  
\end{aligned}
$$
想求解这个最优化问题，首先创建一个拉格朗日算子，它应该等于原始的目标函数加上这些限制函数的线性组合：
$$
\begin{aligned}
 L(w,\beta) =f(w) +\sum_i\beta_{i}h_i(w) 
\end{aligned}
$$
> $\beta_i$ 称为拉格朗日乘数

最优化的解决方法是先对原始参数w求偏导：
$$
\frac{\partial L}{\partial w} \overset{set}{=}0
$$
然后对拉格朗日乘数求偏导：

$$
\frac{\partial L}{\partial \beta} \overset{set}{=}0
$$

拉格朗日乘数法的扩展形式，可以用来解决更复杂的带约束的优化问题

 $$
\begin{aligned}
&\min_wf(w)
\\& s.t.\quad g_i(w)\leq 0\qquad i=1,...,k
\\&\qquad h_i(w) =0  \qquad i=1,...,n  
\end{aligned}
$$
在这种情况下，既有不等式约束，也有等式约束。接下来是拉格朗日算子，在这种情况下，它被称为一般化的拉格朗日算子:
$$
\begin{aligned}
 L(w,\beta) =f(w) +\sum_{i=1}^k\alpha_ig_i(w)+\sum_{i=1}^n\beta_{i}h_i(w) 
\end{aligned}
$$
接下来定义：

$$
\theta_p(w) = \max_{\alpha,\beta,a_i\geq0} L(w,\alpha,\beta)
$$
> p代表primal，原始问题

当约束条件不满足时，例如，$g_i(w)>0$,那么只要它对应的$\alpha_i$取任意大的值，就可以让整个算子$ L(w,\beta)$无穷大，则：
$$
\theta_p(w) =\infty
$$
同理，如果$h_i(w)\ne0$,那么$\theta_p(w)$也等于无穷
当约束条件都满足时：
$$
\theta_p(w) =f(w)
$$
> 所有的拉格朗日乘数项求和的值变为0

所以可得：
![](https://raw.githubusercontent.com/fray-hao/images/master/20190306134819.png)
那么，求min f(w)就是求下式的值，定义为p*：
![](https://raw.githubusercontent.com/fray-hao/images/master/20190306134928.png)
实际上等价于原始问题p。
###3.2 对偶问题
对偶问题的定义：
$$
\begin{aligned}
 \theta_D(\alpha,\beta) = \min_wL(w,\alpha,\beta) 
\end{aligned}
$$
对其取最大值，即给出对偶优化问题，定义为d*：
$$
d^* = \max_{\alpha,\beta,\alpha_i\ge0} \theta_D(\alpha,\beta) =\max_{\alpha,\beta,\alpha_i\ge0}\min_{w}L(w,\alpha,\beta)
$$
从公式上看，对偶问题就是把原始问题中的min，max换了顺序。

实际上，下列公式是一个事实：

$$d^*\leq p^*$$
也就是对偶问题的值小于等于原始最优化问题的值，也可以表示为：
$$
\max_{\alpha,\beta,\alpha_i\ge0}\min_{w}L(w,\alpha,\beta) \leq \min_{w}\max_{\alpha,\beta,\alpha_i\ge0}L(w,\alpha,\beta)  
$$


>\theta_D$是一个以拉格朗日乘数为变量的函数

在某些特定条件下，这两个最优化问题会取相同的值：

令f为凸（convex）函数 (它的Hessian矩阵是一个半正定矩阵)
之后假设$h_i$约束是仿射(affine)函数。这意味着：$h_i(w) = a^Tw+b$。它和线性函数有点类似，如果没有b这项，我们就说$h_i$是线性函数，当有截距时，就叫做仿射函数。
接着假设$g_i(w)$是严格可执行的。这意味着存在一个w，使得对于所有i，$g_i(w)$小于0，即：$$
\exists w,s.t.\quad \forall i , g_i(w)<0
$$
在这些条件下，存在$w^*,\alpha^*,\beta^*$,使得$w^*$是原始问题的解，$\alpha^*,\beta^*$是拉格朗日乘数，是对偶问题的解。且原始问题的值等于对偶问题的值等于广义拉格朗日算子的值：
$$
p^*=d^*=L(w^*,\alpha^*,\beta^*)
$$
此外，参数还需要满足以下条件：
$
\begin{aligned}
 &\frac{\partial}{\partial w}L(w^*,\alpha^*,\beta^*)=0
\end{aligned}
$
$
\begin{aligned}
 &\frac{\partial}{\partial \beta}L(w^*,\alpha^*,\beta^*)=0
\end{aligned}
$
$
\begin{aligned}
 a_i^*g_i(w^*) = 0
\end{aligned}
$
$
\begin{aligned}
 g_i(w^*) \leq 0
\end{aligned}
$
$
\begin{aligned}
 a_i^*\geq0
\end{aligned}
$
这些条件称为KKT(Karush-kuhn-Tucker)互补条件
补充一点,下面两者相乘需要等于0：
$a^**g_i(w^*) = 0$
## 4. 最优间隔优化问题
在前面讲的应用到最优间隔优化问题
https://www.cnblogs.com/madrabbit/p/6964838.html