## 1. 复习上节课内容
支持向量机的定义：
$$
\begin{aligned}
  &h_{w,b} = g(w^Tx+b) 
  \\& g(z) \text{\textasciitilde}  \begin{cases}
      1& if\quad z\geq0
      \\-1 & otherwise 
  \end{cases}
\end{aligned}
$$
标签用-1，+1表示：
$$
y\in\{-1,1\}
$$

函数间隔：
$$
\hat{\gamma}^{(i)} = y^{(i)}(w^Tx^{(i)}+b)
$$
几何间隔：
$$
\gamma^{(i)} = y^{(i)}（\frac{w^T}{||w||}x^{(i)}+\frac{b}{||w||}）
$$
几何间隔的含义可以这样解释。如下图：
![](https://i.loli.net/2019/03/05/5c7dfe46e81e1.png)
如上图，几何间隔表示的是一个训练样本和超平面之间的距离，它实际上是一个有符号的距离。如果样本分类是正确的，它是正的；如果分类错误，则是负的。分割样本的超平面由$w^Tx+b=0$定义。

而整体样本的函数间隔：
$$\hat{\gamma} = \min_i\hat\gamma^{(i)}$$
整体样本的几何间隔：
$$\gamma = min_i\gamma^{(i)}$$
> 考虑的是最坏的情况

对于几何间隔，可以按任意比例缩放w和b，而不会改变结果。因为，$w^Tx+b=0$与$2w^Tx+2b=0$是同一个超平面。
比如，我们可以添加一个这样的约束：
$$||w||=1$$
这样，可以先求出w和b的解，之后通过重新缩放这些参数，使之满足上述条件，使w的模变成1。
还可以添加其他诸如$|w_1|=1,w_1^2+|w_2|=1$等约束。

## 2. 几何间隔最大化的优化
在w的模等于1的情况下，几何间隔等于函数间隔。我们要找到一个最大化的$\gamma$,并且所有训练样本的函数间隔（即几何间隔）大于或等于$\gamma$：
$$
\begin{aligned}
&\max_{\gamma,w,b}\gamma
\\&s.t.\quad y^{(i)}(w^Tx^{(i)}+b) \geq \gamma ，||w||=1
\end{aligned}
$$

求解了这个最优问题之后，就可以得到最优间隔分类器（optimal margin classifier）。但是，这并不是一个比较好的优化接近问题，因为约束“||w||=1”是一个非常糟糕的非凸性约束（nonconvex constraints）。要求的w位于一个球体的表面上。

如果我们要想得到一个凸优化问题，我们必须要保证诸如梯度下降算法这样的局部最优值搜素算法，不会找到局部最优值。
所以，我们需要改变优化问题：
$$
\begin{aligned}
&\max_{\hat{\gamma},w,b} \frac{\hat{\gamma}}{||w|
}   
\\&s.t. y^{(i)}(w^Tx+b)\geq\hat{\gamma}
\end{aligned}
$$
> 需要满足的条件是每一个样本的函数间隔都要大于等于整体的函数间隔，但是我们要最优化（大化）的却不是函数间隔而是几何间隔（因为$\gamma = \frac{\hat{\gamma}}{||w|
} $）。

在这个优化问题中，我们避免了非凸性约束，但是又产生了一个非凸性优化目标：$\frac{\hat{\gamma}}{||w|
} $。我们可以添加一个缩放约束条件：
$$\hat{\gamma}=1$$
也就是：
$$\min_{i}y^{(i)} $$


