
解决过拟合的方法：

- 减少特征的数量：
    - 人工的选择保留哪些特征；
    - 模型选择算法（上一讲）

- 正则化
    - 保留所有的特征，但是降低参数的量/值；
    - 正则化的好处是当特征很多时，每一个特征都会对预测y贡献一份合适的力量；

## 1. 贝叶斯统计与正则化

以线性回归为例，我们会选择参数，使得似然性最大化：
$$
\begin{aligned}
    \max_\theta \Pi_iP(y^{(i)}|x^{(i)};\theta)
\end{aligned}\tag{1}
$$
上述公式是频率学派（frequency procedure）的一个例子。这个公式之后的哲学观点是这样的：在数据背后，有一组参数$\theta$,用来生成x和y，这些$\theta$是真实的（不是随机变量，只是不知道值是什么），控制了房价y，但是，我们不知道$\theta$的值是什么，我们要做的是用一些方法来估计参数$\theta$的值。极大似然估计是一种估计$\theta$值的一种可能的方法。

除了统计学派外，还有一个学派就是贝叶斯学派（Bayesian school）。贝叶斯学派的观点认为：我们不知道$\theta$的值，所以我们为$\theta$的值赋予一个先验分布(prior).用先验分布来表示$\theta$取值的不确定性。

贝叶斯学派认为θ是一个服从某一分布的变量,即$\theta\text{\textasciitilde}p(\theta)$，其值是未知的。在这一假设中，我们需要确认先验分布Prior Distribution P(θ)从而表达我们对参数θ出现的“信任程度”。

例如，$\theta$的先验概率可能服从高斯分布，均值为0，协方差矩阵为$\tau^2I$:
$$
\theta \text{\textasciitilde}N(0,\tau^2I)
$$
给定训练集：
$$
S= \begin{Bmatrix}
    x^{(i)},y^{(i)};i=1,...,m
\end{Bmatrix}
$$
$p(\theta)$表示了，在没有见到任何数据的时候，我认为它最可能的形式。
我们会计算，给定了训练集合之后的后验概率.
> 根据贝叶斯公式，$p(\theta)$ 与右侧的公式成正比



$$
p(\theta|S)∝(\Pi_{i=1}^m p(y^{(i)}|x^{(i)},\theta))p(\theta)\tag{2}
$$

> 注意公式1与公式2中条件概率的区别。公式2中$p(y^{(i)}|x^{(i)},\theta))$中x与$\theta$之间用逗号分隔，表示$\theta$是随机变量。而公式1中$P(y^{(i)}|x^{(i)};\theta)$中x与$\theta$以分号隔开，表示$\theta$是一个具体的未知量。

当我们要对一个新的x进行预测时，我们会计算在参数θ的条件下预测结果的条件概率，公式为：
$$
p(y|x,S) = \int_\theta p(y|x,\theta)p(\theta|s)d\theta
$$
为了得到一个可信的预测结果，我们可以使用y|x的期望:
$$
E\begin{bmatrix}
    y|x,S 
\end{bmatrix}= \int_y yp(y|x,S) dy
$$
实际证明，对于许多问题，上述步骤很难计算。因为θ是一个n+1维的向量，p(θ|S)的计算由于需要对高维的θ进行积分变得很复杂。所以通常情况下，不会计算完整的后验概率，我们通过最大后验概率估计（Maximum a Posterior，MAP）算法对θ进行估计：
$$
\begin{aligned}
    \hat{\theta}_{MAP}  &= \arg\max_{\theta} p(\theta|S)      \\&=\arg\max_\theta\Pi_{i=1}^m p(y^{(i)}|x^{(i)},\theta)p(\theta)
\end{aligned}
$$      
注意到这一算法相比最大似然估计算法而言，只是在最后加了一项前验概率p(θ)。直观理解是，我们常令θ服从$\theta\text{\textasciitilde}N(0,\tau^2I)$的高斯分布。这一分布使贝叶斯MAP算法相比最大似然估计算法更难发生过拟合的情况，因为这一算法将大多数参数θ设为0或0附近的值。
当需要预测的时候：
$$
h_{\hat{\theta}_{MAP}} = \hat{\theta}_{MAP}^Tx
$$

MAP和其对应的需要最小化的目标函数为：
$$
\min\sum_{i=1}^m||y^{(i)}-\theta^Tx^{(i)}||^2 + \lambda||\theta||^2
$$
$\lambda||\theta||^2$被称为正则化项，$\lambda =1/\tau^2 $,也可以使用交叉校验方法进行选取。正则化项会使曲线更平滑，从而减少过拟合的可能性，因为参数值过大时，会惩罚目标函数，从而倾向于选择比较小的参数值。

实际上，特征选择与正则化之间有一定的联系，可以这样考虑，当特征选择将一个特征删除的时候，等同于正则化时将该特征相对应的$\theta$分量设置为0.
在文本分类问题上，特征数目（50000）有时会远远大于样本数目，使用逻辑回归就容易产生过拟合，添加正则项是解决该问题的一个不错的方法。
https://blog.csdn.net/stdcoutzyx/article/details/18500441

https://www.jianshu.com/p/7ed336956ea0

https://blog.csdn.net/knight_wzz/article/details/52942970