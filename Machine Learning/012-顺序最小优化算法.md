前面讲到，参数w是输入样本之和：
$$
w=\sum_{i=1}^ma_iy^{(i)}x^{(i)}
$$
给定一个x，需要预测它的假设：
$$
h_{w,b}(x) = g(w^Tx+b) 
$$
而$w^Tx+b$可以表示为内积形式：
$
w^Tx+b =\sum_{i=1}^m\alpha_i<x^{(i)},x>+b
$
在SVM的特征向量空间中，有时候训练样本的维数非常高，甚至是无限维的向量。但是你可以使用  来高效地计算内积，而不必把x显式的表示出来。

这个结论仅对一些特定的特征空间成立
## 1. 核（kernels）

### 1.1 核函数
核具有这样的性质，算法对于x的依赖，仅仅局限于内积。在整个算法的过程中，都无法显示地直接使用向量x的值，而只需要使用到训练样本与输入特征向量的内积。

核的概念是这样的，比如说有一个输入x，诸如房屋面积之类的，你希望基于它预测出该房屋是否在未来六个月之内被卖掉。我们经常会将这个特征x映射到一组特征上，假如我们看出来样点x和y的分布符合3次曲线，那么我们最好使用x的三次多项式来逼近这些样点，特征x映射到三维上的过程叫做特征映射。我们用$\phi(x)$表示将原始特征转换成更高维特征的映射。
$$
\phi(x)=\begin{bmatrix}
    x\\
    x^2
    \\x^3
\end{bmatrix}
$$
我们希望将得到的特征映射后的特征应用于SVM分类，而不是最初的特征。这样，我们需要将前面W(a)公式中内积从$<x^{(i)},x^{(j)}>$映射到$<\phi(x^{(i)}),\Phi(x^{(j)})>$。这意味着我们将使用映射后的高维特征来运算svm，而不是原始的一维特征。
一些情况下,$\phi(x)$的维度会非常高。问题是，维度非常高时，无法有效的计算出内积。因为计算机将这些非常高维的向量表示出来之后求它们之间的内积代价很大。

然而，计算这些向量之间的内积代价很小，表示特征向量的内积的函数就是核函数：
$$
K(\phi(x^{(i)}),\phi(x^{(j)})) = <\phi(x^{(i)}),\phi(x^{(j)})>
$$
此时我们便可用核函数K代替SVM算法之前使用的$ <\phi(x^{(i)}),\phi(x^{(j)})>$。使用函数替换后，算法的时间复杂度可以大大减少，
让我们看看具体是怎么做的。比如有两个输入:
$x,z\in \mathbb{R}^n$
假设核函数：
$$
\begin{aligned}
   k(x,z) &= (x^Tz)^2 
   \\&=(\sum_{i=1}^nx_iz_i)(\sum_{j=1}^nx_jz_j) 
   \\&= \sum_{i=1}^n\sum_{j=1}^n(x_ix_j)(z_iz_j)
   \\& = (\phi(x))^T(\phi(z))
\end{aligned}
$$
与其对应的$\phi(x)$为（n=3）:
$$
\phi(x) =\begin{bmatrix}
   x_1x_1
   \\x_1x_2 
   \\x_1x_3
   \\x_2x_1
   \\x_2x_2
   \\x_2x_3
   \\x_3x_1
   \\x_3x_2
   \\x_3x_3
\end{bmatrix}
$$
如上式所示，若先计算$\phi(x)$，然后计算$\phi(x)^T\phi(z)$是非常低效的。因为例如最初的特征是n维的，我们将其映射到$n^2$维，然后再计算，这样需要$O(n^2)$的时间复杂度。而经过核函数可以达到同样的效果，却只用计算始特征x和z内积然后再将得数平方即可，这样的时间复杂度为O(n)。

核函数可以有很多形式，对于上面的核函数，其更为一般的形式为：
$$
K(x,z) =(x^Tz+c)^2
$$
这个核函数可以在O(n)的复杂度内计算出来。 与其对应的$\phi(x)$为（n=3）:
$$
\phi(x) =\begin{bmatrix}
   x_1x_1
   \\x_1x_2 
   \\x_1x_3
   \\x_2x_1
   \\x_2x_2
   \\x_2x_3
   \\x_3x_1
   \\x_3x_2
   \\x_3x_3
   \\\sqrt{2c}x_1
   \\\sqrt{2c}x_2
   \\\sqrt{2c}x_3
   \\c
\end{bmatrix}
$$
这个特征向量中，包含了单项式（即一次项）、二次项（或叫内积项）。参数c可以控制单项式与二次项之间的相对权重

再来看下，核的一般化的形式：
$$
K(x,z)=(x^Tz+c)^d
$$
它能够映射出维度为$C_{n+d}^d$的特征向量

接下来，我们通过例子学习如何构造核。

假设有一组属性x，将其转换成一个特征向量$\phi(x)$ ,给定一组属性z，将其转换成一个特征向量$\phi(z)$:
$$
\begin{aligned}
 &x\to \phi(x)
 \\&z\to \phi(z)  
\end{aligned}
$$
核就是计算两个特征向量之间的内积：$<\phi(x),\phi(z)>$。直观理解就是：如果x和z非常相似，那么$\phi(x)$和$\phi(z)$将会大概指向相同的方向，因此内积将会比较大、相反地，如果x和z的相似度非常低，那么$\phi(x)$和$\phi(z)$将可能指向不同的方向，因此内积将会比较小。
如果你面对一个新的学习问题，要对样本进行分类，你希望找到一个核，一种方式就是使k(x,z)取一个较大的值，当你希望学习算法认为x和z是相似的；反之则取一个较小的值。
因此，我们可以认为k(x,z)是$\phi(x)$和$\phi(z)$相似度的一种测量，或许将核定义为以下公式：
$$
k(x,z) =exp(\frac{-||x-z||^2}{2\sigma^2})
$$
这是一种合理的测量方法，当x和z很近时值接近于1，很远时值接近0. 在特定的例子中是可以作为SVM的核的. （这个核叫做高斯核 Gaussian kernel,映射到一个无限维的特征）
### 1.2 核函数的合法性
给定一个函数K，我们能否使用K来替代计算$\phi(x)^T\phi(z)$，即核函数K的合法性如何确定？比如给出了$K(x,z)=(x^Tz+c)^2$，我们是否能够认为K是一个有效的核函数？

合法性的验证：
给定m个训练样本$\{x^{(1)},x^{(2)},...,x^{(m)}\}$.每一个元素对应一个特征向量。我们可以定义一个矩阵K（$k\in \mathbb{R}^{m\times m}$）。
那么，我们可以将任意两个不同的元素和带入K中，计算得到
$$
k_{ij} = K(x^{(i)},x^{(j)})
$$
如果假设K是有效地核函数，那么根据核函数定义:
$$
k_{ij} = K(x^{(i)},x^{(j)})=\phi(x^{(i)})^T\phi(x^{(j)}) =\phi(x^{(j)})^T\phi(x^{(i)})=K(x^{(j)},x^{(i)}) = K_{ji}I
$$

可见，矩阵K应该是个对称阵。让我们得出一个更强的结论，首先使用符号$\phi_k(x)$来表示映射函数$\phi(x)$的第k维属性值。那么对于任意向量z，得
$$
\begin{aligned}
z^TKz &=  \sum_i\sum_j z_iK_{ij}z_j
\\&=  \sum_i\sum_j z_i\phi(x^{(i)})^T\phi(x^{(j)})z_j 
\\&=\sum_i\sum_j z_i\sum_k\phi_k(x^{(i)})\phi_k(x^{(j)})z_j 
\\&=\sum_i\sum_j\sum_kz_i\phi_k(x^{(i)})\phi_k(x^{(j)})z_j
\\&=\sum_k(\sum_iz_i\phi_k(x^{(i)}))^2
\\&\geq0
\end{aligned}
$$
从这个公式我们可以看出，如果K是个合法的核函数，那么，在训练集上得到的核函数矩阵K应该是半正定的（K>=0）。
这样我们得到一个核函数的必要条件：
>K是有效的核函数 ==> 核函数矩阵K是对称半正定的。

可幸的是，这个条件也是充分的，也就是逆命题也是成立的，这就给了我们一种测试核函数是否合法的方式。这是一个由Mercer提出的定理。所以核有时被称为Mercer核：
$$
\begin{aligned}
   \footnotesize如果函数K是\mathbb{R}^n\times\mathbb{R}^n\to\mathbb{R}上的映射（也就是从两个n维向量映射到实数域）。那么如果K是一个有效核函数（也称为Mercer核函数），那么当且仅当对于训练样例\{x^{(1)},x^{(2)},...,x^{(m)}\}，其相应的核函数矩阵是对称半正定的。
\end{aligned}
$$

核明显不合法的例子是：如果有一个输入x，$k(x,x)=-1$,那么这个函数一定不是一个合法的核。因为$\phi(x)^T\phi(x)\ne -1$。x和自己的内积一定大于等于0.

接下来，我们将核与svm联系起来。比如我们需要在svm中使用核。你需要选择的是选择核函数。假如选择：
$$
k(x,z) =exp(\frac{-||x-z||^2}{2\sigma^2})
$$
或者
$$
k(x,z) = (x^Tz+c)^d
$$
我们需要在svm中出现的所有$<x^{(i)}，x^{(j)}>$替换成$k(x^{(i)}，x^{(j)})$，
之后，运行相同的svm算法。

这样做有什么好处呢？
我们引出svm是为了解决非线性学习问题。核的作用是将你的原始输入数据映射到非常高维的特征空间，之后，在这个高维的空间中，运行swm，会找到最优间隔分类器。原始数据中线性不可分的数据，映射到高纬空间中变成了线性可分的。这就是svm输出非线性决策边界的整个过程。我们需要做的仅仅是求解凸优化问题。

最后要讲的是：核的应用范围比svm要更广。在svm中，我们提出了对偶优化问题，这使得我们能将整个算法写成这些项的内积形式。实际上，许多其他的算法中，例如，逻辑回归、线性回归或者感知器算法，可以将这些算法写成内积的形式，然后用核函数。这意味着隐式地将原有的特征向量映射到更高维，并且使得算法仍然能够工作。
## 2. 软间隔（L1 norm soft margin）
软间隔是非线性决策边界。它的概念是这样的：比如有一个数据集合，它是线性可分割的，但是如果有几个样本，使得整个数据集合不再是线性可分的。应该怎么办？实际上，有些时候即使整个数据集合是线性可分割的，但是如果出现了一个异常数据，决定边界会发生剧烈的变化。导致分类器有更小的边界。
![](https://raw.githubusercontent.com/fray-hao/images/master/20190311101219.png)

接下来要讲的L1  norm soft margin svm是一种轻微修改后的形式化版本，这样的处理会帮助我们处理两种情况：
- 数据是线性不可分割的
- 异常数据不那么敏感的

 svm的原始问题：
 $$
 \begin{aligned}
&   \min \frac{1}{2}||w||^2
 \\&s.t.\quad y^{(i)} (w^Tx+b)\ge1
 \end{aligned}
 $$
 我们对其进行修改：
 $$
 \begin{aligned}
&   \min_{w,b,\xi} \frac{1}{2}||w||^2 + \textcolor{blue}{C\sum_{i=1}^m\xi_i}
 \\&s.t.\quad y^{(i)} (w^Tx+b)\ge1 \textcolor{blue}{-\xi_i}
 \\&\textcolor{blue}{\xi_i\ge0\qquad i=1,...,m}
 \end{aligned}
 $$ 
 也就是，我们增加一些惩罚项$\xi_i$。我要求所有的训练样本都需要被分割并且其函数分割都要大于等于1-$\xi_i$。
 > 注意到，我们此时允许出现几何间隔小于1的点，但同时也希望这些点越少越好，所以将其加入目标函数，并乘以参数C（称为损耗因子）

 如果函数间隔$y^{(i)} (w^Tx+b)>0$,这意味着你的分类是正确的。如果小于0，则分类错误。通过令一些$\xi>1$ ,实际上允许其中的一些样本的函数间隔小于0，因此允许我的算法对一些样本进行错误分类，也就是允许边界小于1。

 这也是一个凸优化问题，和之前推导svm对偶问题类似。我们也可以推导出这个优化问题的对偶问题。
 首先构建拉格朗日算子：
$$
L(w,b,\xi,\alpha,r)=\frac{1}{2}||w||^2+C\sum_{i=1}^m\xi_i-\sum_{i=1}^ma_i(y^{(i)}(x^Tw+b)-1+\xi_i)-\sum_{i=1}^mr_i\xi_i
$$

>$\alpha,r$ 表示拉格朗日乘数。

通过将对w和对β的导数设为0，我们可以得到以下优化问题:
$$
\begin{aligned}
  &\max_\alpha  W(\alpha)=\sum_{i=1}^ma_i-\frac{1}{2} \sum_{i,j}^my^{(i)}y^{(j)}\alpha_i\alpha_j<x^{(i)},x^{(j)}>
  \\ &s.t.\quad \sum_{i=1}^m\alpha_iy^{(i)} =0
  \\&\qquad 0\le \alpha_i\leq C,i=1,...,m
\end{aligned}
$$
>优化问题的对偶问题经过推导和简化后，和原问题的唯一区别是原始问题的$a_i\ge0$,，而这里是介于0和C之间。

通过求解α我们可以解出SVM中的其他参量，得到完整的SVM模型。但此时，KKT对偶互补条件变为 $0\leq\alpha_i\leq C$

其α的不同取值对应的情况如下:
$$
\begin{aligned}
 & \alpha_i =0\Rightarrow y^{(i)}(w^Tx^{(i)}+b)\geq1  
 \\& \alpha_i =C\Rightarrow y^{(i)}(w^Tx^{(i)}+b)\leq1
 \\& 0< \alpha_i < C\Rightarrow y^{(i)}(w^Tx^{(i)}+b)=1
\end{aligned}
$$

### 坐标上升算法 Coordinate Ascent
当我们要解决优化问题$\max_a W(a_1,a_2,...,a_m)$时，我们之前可以使用梯度下降法和牛顿法求解。下面我们介绍一个新方法，考虑到W是α的函数，因此我们可以固定除$α_i$外的所有值，然后对$α_i$求导。不断执行这一过程，直到α收敛。函数的执行过程可如下所示
$$
\begin{aligned}
   &repeat\{
      \\&\qquad for\ i\ = \ 1,...,m\{
         \\&\qquad\qquad a_i:=\arg \max_{\hat{a_i}}W(a_1,...,a_{i-a},\hat{a_i},a_{i+1},...,a_m)
       \\&\qquad\}
\\&\}
\end{aligned}
$$
下图是坐标上升执行的一个过程：
![](https://raw.githubusercontent.com/fray-hao/images/master/20190311121251.png)
椭圆代表了二次函数的各个等高线，变量数为2，起始坐标是(2,-2)。图中的直线式迭代优化的路径，可以看到每一步都会向最优值前进一步，而且前进路线是平行于坐标轴的，因为每一步只优化一个变量。
坐标上升方法相比于牛顿方法等求极值的方法通常会用更多的步骤，但由于其每次只针对一个参数求极值，所以代价非常小。总而言之，它在通常情况下是一个相当高效的算法。

这个算法存在一些变种。坐标上升的基本形式不能直接在svm中应用。因为在对偶问题中有约束：
$$
\sum_{i=1}^my^{(i)}\alpha_i=0
$$
所以，如果固定除了一个之外的所有a,那么你不可能仅改变一个a的同时不违反这个约束。如果每次仅仅允许一个参数变化，那从本质上说那个参数并不是变量，因为根据约束条件可以由其他固定的参数推出这个参数。
所以在SMO中，我们每次改变两个参数，而不再是一个。
## 3. SMO算法
SMO算法即sequenctial minimal optimization（序列最小优化），最小指的是一次改变最小数目的$a_i$,这里我们只需要改变2个。

SMO算法的步骤：
1. 由启发式法则(经验法则)选择两个参数 $a_i,a_j$
2. 保持除了$a_i,a_j$之外的所有参数不变
3. 在满足所有约束条件的情况下，用$a_i,a_j$这两个参数最优化$W(a)$

需要一直运行这个算法，直到满足如下的收敛条件：
$$
\begin{aligned}
 & \alpha_i =0\Rightarrow y^{(i)}(w^Tx^{(i)}+b)\geq1  
 \\& \alpha_i =C\Rightarrow y^{(i)}(w^Tx^{(i)}+b)\leq1
 \\& 0< \alpha_i < C\Rightarrow y^{(i)}(w^Tx^{(i)}+b)=1
\end{aligned}
$$
接下来，要讨论的是如何完成SMO算法中关键的第三步。
在这里，我用$a_1,a_2$来表示$a_i,a_j$，目的是使表达更加简明。
我们有这样的对偶约束：
$$
\sum_{i=1}^my^{(i)}\alpha_i=0
$$
这意味着：
$$
a_1y^{(1)}+a_2y^{(2)}=-\sum_{i=3}^my^{(i)}\alpha_i=\zeta
$$
对偶问题还有另外一个约束：
$$
0\leq a_i\leq C
$$
这个约束被称为方形约束。如下图：
![](https://raw.githubusercontent.com/fray-hao/images/master/20190312093238.png)

$a_1,a_2$表示的值一定位于这个从0到C的方形里。
算法的图是这样的：
![](https://raw.githubusercontent.com/fray-hao/images/master/20190312131714.png)

这意味着：
$$
\alpha_1=\frac{\zeta-\alpha_2y^{(i)}}{y^{(1)}}
$$
则最优化:
$$
W(a_1,a_2,a_3,...)=W(\frac{\zeta-\alpha_2y^{(i)}}{y^{(1)}},a_2,a_3,...) 
$$
实际上，由于W是一个二次函数，所以如果将上面的W视为$a_2$的函数，如果保持其它参数固定的话，上面的式子可以简化为标准的一元二次函数：
$$
aa_2^2+ba_2+c
$$
这个函数很容易求最优值。如果最优值的解落在了方形区域中，就得到了想要的结果；如果最优解落在了方形区域外，则需要对解进行裁剪，以使它落在方形区域中：
![](https://raw.githubusercontent.com/fray-hao/images/master/20190312133935.png)
这样，你会得到这个二次优化问题的最优解，即满足方形约束，同时也满足解在直线上的约束。换句话说，解一定在方形区域中的线段上：
![](https://raw.githubusercontent.com/fray-hao/images/master/20190312134216.png)
按照这种方式解出$a_2$之后，之后可以根据$a_2$和$a_1$之间的函数关系求出$a_1$


## 4. SVM 应用
### 4.1 手写识别
选取多项式核和高斯核的效果都很好
![](https://raw.githubusercontent.com/fray-hao/images/master/20190312135255.png)
其性能和最好的神经网络相当
### 4.2 蛋白质序列分类

蛋白质序列有的很长、有的很短。怎么解决？可以写出所有的四个字母的组合。然后检查氨基酸序列，统计这些子序列都出现过多少次。例如，BAJT出现2次，对应的组合上就写2.
![](https://raw.githubusercontent.com/fray-hao/images/master/20190312135811.png)
这种表述方法与蛋白质的长度无关。实际上的向量的维数为$20^4$(20种氨基酸)。也就是说，有一个160000维的特征向量，即使用现代计算机的标准来看，也是相当大了。很明显，我们不想显示地将这些高纬向量表达出来。想象一下，有1000个样本，用double类型变量进行存储，这要占很大的空间。实际上有一个非常高效的动态规划算法（dynamic programming algorithm）可以高效的计算这样的两个特征向量的内积

https://www.cnblogs.com/jerrylead/archive/2011/03/18/1988419.html
https://blog.csdn.net/qrlhl/article/details/47978549