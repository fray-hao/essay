前面讲到，参数w是输入样本之和：
$$
w=\sum_{i=1}^ma_iy^{(i)}x^{(i)}
$$
给定一个x，需要预测它的假设：
$$
h_{w,b}(x) = g(w^Tx+b) 
$$
而$w^Tx+b$可以表示为内积形式：
$
w^Tx+b =\sum_{i=1}^m\alpha_i<x^{(i)},x>+b
$
在SVM的特征向量空间中，有时候训练样本的维数非常高，甚至是无限维的向量。但是你可以使用  来高效地计算内积，而不必把x显式的表示出来。

这个结论仅对一些特定的特征空间成立
## 1. 核（kernels）

核具有这样的性质，算法对于x的依赖，仅仅局限于内积。在整个算法的过程中，都无法显示地直接使用向量x的值，而只需要使用到训练样本与输入特征向量的内积。

核的概念是这样的，比如说有一个输入x，诸如房屋面积之类的，你希望基于它预测出该房屋是否在未来六个月之内被卖掉。我们经常会将这个特征x映射到一组特征上，假如我们看出来样点x和y的分布符合3次曲线，那么我们最好使用x的三次多项式来逼近这些样点，特征x映射到三维上的过程叫做特征映射。我们用$\phi(x)$表示将原始特征转换成更高维特征的映射。
$$
\phi(x)=\begin{bmatrix}
    x\\
    x^2
    \\x^3
\end{bmatrix}
$$
我们希望将得到的特征映射后的特征应用于SVM分类，而不是最初的特征。这样，我们需要将前面W(a)公式中内积从$<x^{(i)},x^{(j)}>$映射到$<\phi(x^{(i)}),\Phi(x^{(j)})>$。这意味着我们将使用映射后的高维特征来运算svm，而不是原始的一维特征。
一些情况下,$\phi(x)$的维度会非常高。问题是，维度非常高时，无法有效的计算出内积。因为计算机将这些非常高维的向量表示出来之后求它们之间的内积代价很大。

然而，计算这些向量之间的内积代价很小，表示特征向量的内积的函数就是核函数：
$$
K(\phi(x^{(i)}),\phi(x^{(j)})) = <\phi(x^{(i)}),\phi(x^{(j)})>
$$
此时我们便可用核函数K代替SVM算法之前使用的$ <\phi(x^{(i)}),\phi(x^{(j)})>$。使用函数替换后，算法的时间复杂度可以大大减少，
让我们看看具体是怎么做的。比如有两个输入:
$x,z\in \mathbb{R}^n$
假设核函数：
$$
\begin{aligned}
   k(x,z) &= (x^Tz)^2 
   \\&=(\sum_{i=1}^nx_iz_i)(\sum_{j=1}^nx_jz_j) 
   \\&= \sum_{i=1}^n\sum_{j=1}^n(x_ix_j)(z_iz_j)
   \\& = (\phi(x))^T(\phi(z))
\end{aligned}
$$
与其对应的$\phi(x)$为（n=3）:
$$
\phi(x) =\begin{bmatrix}
   x_1x_1
   \\x_1x_2 
   \\x_1x_3
   \\x_2x_1
   \\x_2x_2
   \\x_2x_3
   \\x_3x_1
   \\x_3x_2
   \\x_3x_3
\end{bmatrix}
$$
如上式所示，若先计算$\phi(x)$，然后计算$\phi(x)^T\phi(z)$是非常低效的。因为例如最初的特征是n维的，我们将其映射到$n^2$维，然后再计算，这样需要$O(n^2)$的时间复杂度。而经过核函数可以达到同样的效果，却只用计算始特征x和z内积然后再将得数平方即可，这样的时间复杂度为O(n)。
## 2. 软间隔（L1 norm soft margin）

## 3. SMO算法


https://blog.csdn.net/qrlhl/article/details/47978549