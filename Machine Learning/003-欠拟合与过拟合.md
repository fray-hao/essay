# 1.拟合
拟合是指已知某函数的若干离散函数值{f1,f2,…,fn}，通过调整该函数中若干待定系数f(λ1, λ2,…,λn)，使得该函数与已知点集的差别(最小二乘意义)最小。

## 欠拟合（under-fitting）
欠拟合也叫高偏差（high bias），是指模型拟合程度不高，数据距离拟合曲线较远，或指模型没有很好地捕捉到数据特征，不能够很好地拟合数据。
![image.png](https://upload-images.jianshu.io/upload_images/13764292-4de52abef2e0e26f.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

> 偏差：反映的是模型在样本上的期望值与真实值之间的误差，即模型本身的精准度

## 过拟合（over-fitting）
过拟合也叫高方差（high variance）是指在模型参数拟合过程中的问题，由于训练数据包含抽样误差，训练时，复杂的模型将抽样误差也考虑在内，将抽样误差也进行了很好的拟合。

具体表现就是最终模型在训练集上效果好；在测试集上效果差。模型泛化能力弱
![image.png](https://upload-images.jianshu.io/upload_images/13764292-c40c3484ac32d968.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)
> 方差：反映的是模型在新样本上的期望值与与样本真实值之间的误差，即模型的稳定性。

# 2. 非参数学习算法(non-parametric learning algorithms)

选取的特征（参数）太少会欠拟合，选取的特征太多会过拟合。

非参数学习算法可以缓解对于选取特征的需求。

我们以前学过的线性回归是参数学习算法的一个例子。参数学习算法是一类有固定数目的参数($\Theta$)，以用来数据拟合的算法。

非参数学习算法是指参数的数目会随着训练集的大小线性增长
```math
\begin{aligned}
& no \quad of\quad parameters \quad grows \quad with\quad m.
\end{aligned}
```

局部加权回归（locally weighted regression）也叫做loess，是一种非参数学习算法，可以让我们不用考虑特征的选择。它的算法如下：
```math
\begin{aligned}
& Fit\quad \Theta\quad to\quad minimize \quad\sum_iw^{(i)}(y^{(i)}-\Theta^TX^{(i)})^2
\\&\qquad where\quad w^{(i)}=exp(-\frac{(x^{(i)}-x)^2}{2})
\\&If\quad |x^{(i)}-x|\quad small,\quad then \quad w^{(i)}\approx 1
\\&If\quad |x^{(i)}-x|\quad large,\quad then \quad w^{(i)}\approx 0
\end{aligned}
```
在预测一个特定的点x时，将会给那些离得近的点赋予较大的权值，而给那些离得远的点赋予较小的权值（近似0），所以，离得远的点对于求和公式的贡献会非常小。

这种权值计算的方法就是通过使用局部加权回归拟合出一组参数向量，来更多的注重对临近点的精确拟合。如下图：

![image.png](https://upload-images.jianshu.io/upload_images/13764292-d9636f0d3472d975.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

在预测点x时，只会考虑位于x周围的固定区域内的数据点，之后对这个数据子集使用线性回归来拟合出一条直线。


权值$W^{(i)}$是一个指数衰减函数，它的形状是钟形的：
![image.png](https://upload-images.jianshu.io/upload_images/13764292-8d2181ede5af2915.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)
还可以为这个函数添加一个参数，来控制变化的速率。如：
```math
\begin{aligned}
&exp(-\frac{(x^{(i)}-x)^2}{2\tau^2})
\end{aligned}
```

这个参数叫波长函数。它会生成一个权值函数：
![image.png](https://upload-images.jianshu.io/upload_images/13764292-fa28aa2a38490fab.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)
当$\tau$的值小时，波长比较小，变化速率快；当$\tau$的值大时，波长比较大，

>对于线性回归算法，一旦拟合出适合训练数据的参数θ，保存这些参数θ，对于之后的预测，不需要再使用原始训练数据集，所以是参数学习算法。

>对于局部加权线性回归算法，每次进行预测都需要全部的训练数据（每次进行的预测得到不同的参数θ），没有固定的参数θ，所以是非参数算法。

# 3. 线性模型的概率解释

在房价的线性回归预测中，假设：

```math
\begin{aligned}
&y^{(i)}=\Theta^Tx^{(i)}+\epsilon^{(i)}
\end{aligned}
```
其中$\epsilon$是误差项，代表了对未建模的效应（unmodeled effects）的捕获，例如，也许房价还与其他特征有关（如多少壁炉），它表示了一种我们**没有捕获到的特征**，或者也可以把它看成一种**随机的噪声**，如当天玩家的心情。

我们假设误差项服从高斯概率分布：

```math
\begin{aligned}
&\epsilon^{(i)}\sim \mathcal{N}(0,\sigma^2)
\end{aligned}
```
> 高斯分布的均值是0，方差是$\sigma^2 $

所以误差项的概率密度函数为：
```math
\begin{aligned}
&P(\epsilon^{(i)})=\frac{1}{\sigma\sqrt{2\pi}}\cdot\exp(-\frac{(\epsilon^{(i)})^2}{2\sigma^2})
\end{aligned}
```
这表示在给定参数$\Theta$的情况下，房价也服从高斯分布：
```math
\begin{aligned}
&P(y^{(i)}\Big\vert x^{(i)};\Theta)=\frac{1}{\sigma\sqrt{2\pi}}\cdot\exp(-\frac{(y^{(i)}-\Theta^Tx^{(i)})^2}{2\sigma^2})
\end{aligned}
```
> $y^{(i)}\Big\vert x^{(i)};\Theta$`读作：`$y^{(i)}$ given  $x^{(i)}$parameterized by $\Theta$。上面的公式，也就是在求给定$x^{(i)}$,以$\Theta$为参数的$y^{(i)}$的概率。

换句话说：

```math
\begin{aligned}
&y^{(i)}\Big\vert x^{(i)};\Theta\quad\sim\mathcal{N}(\Theta^Tx^{(i)^2},\sigma^2)
\end{aligned}
```



> 根据中心极限定理，许多独立的随机变量之和趋向于服从高斯分布。如果误差是有许多效应共同导致的，那么这些效应的总和会服从高斯分布的。

假设不同的独立的误差项之间是独立同分布（IID：independent identically distributated）的，也就是假设它们服从均值和方差完全相同的高斯分布。

让我们来讨论如何拟合出一个模型。

```math
\begin{aligned}
L(\Theta)&=P(\vec{y}\Big|X;\Theta)
\\&=\prod_{i=1}^mP(y^{(i)}\Big\vert x^{(i)};\Theta)
\\&=\prod_{i=1}^m\frac{1}{\sigma\sqrt{2\pi}}\cdot\exp(-\frac{(y^{(i)}-\Theta^Tx^{(i)})^2}{2\sigma^2})
\end{aligned}
```
> 这里似然性和概率是相同的。

极大似然估计(maximun likelihood)解决的就是这个问题：选择参数`$\Theta$`,使得数据出现的可能性尽可能的大，也就是选择`$\Theta$`使似然性最大化：
```math
\begin{aligned}
Choose \quad \Theta \quad to \quad maxmize\quad &L(\Theta)
\\&=P(\vec{y}\Big|X;\Theta)
\end{aligned}
```

令：
```math
\begin{aligned}
\mathcal{l}(\Theta)&=\log L(\Theta)
\\&=\log\prod_{i=1}^m\frac{1}{\sigma\sqrt{2\pi}}\cdot\exp(-\frac{(y^{(i)}-\Theta^Tx^{(i)})^2}{2\sigma^2})
\\&=\sum_{i=1}^m\log\bigg( \frac{1}{\sigma\sqrt{2\pi}}\cdot\exp(-\frac{(y^{(i)}-\Theta^Tx^{(i)})^2}{2\sigma^2})\bigg)
\\&=m\cdot\log\frac{1}{\sigma\sqrt{2\pi}}+\sum_{i=1}^m-\frac{(y^{(i)}-\Theta^Tx^{(i)})^2}{2\sigma^2}
\end{aligned}
```

>在参数估计中有一类方法叫做“最大似然估计”,因为涉及到的估计函数往往是是指数型族,取对数后不影响它的单调性但会让计算过程变得简单,所以就采用了似然函数的对数,称“对数似然函数”.根据涉及的模型不同,对数函数会不尽相同,但是原理是一样的,都是从因变量的密度函数的到来,并涉及到对随机干扰项分布的假设.

所以，使$\mathcal{l}(\Theta)$最大化也就是使

```math
\begin{aligned}
&\frac{\sum_{i=1}^m(y^{(i)}-\Theta^Tx^{(i)})^2}{2}
\end{aligned}
```
最小化。

这就是损失函数的推导过程。损失函数：

```math
\begin{aligned}
&J(\Theta)=\frac{1}{2}\sum_{i=1}^m(y^{(i)}-\Theta^Tx^{(i)})^2
\end{aligned}
```

> 最小二乘法的目的：假设误差项满足高斯分布且独立同分布的情况下，使似然性最大化。