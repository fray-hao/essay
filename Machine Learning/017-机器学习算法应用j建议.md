机器学习算法中有很多算法，那么在实际问题中应该怎样使用这些算法？当算法遇到瓶颈时该选择什么样的方向对算法进行改进？这篇文章既是解决这样的问题。这些建议针对已有的算法用到具体问题上，对发明新的算法用处不大。

本文聚集在三个主要内容：
- 调试ML算法时如何进行诊断（diagnostics for debugging learning algorithms）
- 误差分析与销蚀分析（error analysis and ablative analysis）
- 如何在一个ML问题上进行研究（how to get started on a machine learning problem）

在对具体的内容分析之前，先介绍一个很有用的建议，即避免过早优化（premature optimization）。

在我们进行项目开发或课题研究时，往往会遇到一些问题，在没有弄清楚问题之前，即没有明确的证据说明问题确实出在这里，我们往往想当然的对自己认为出问题的地方进行改进或优化，运气好时能把问题解决，运气不好则浪费了时间。

比如，在项目开发时，过早的优化不是瓶颈的代码段。例如，使用汇编语言来实现。虽然该代码段被优化得很快，但对系统的性能提高微乎其微，可以说是浪费了时间。而对于研究来说，过早的去做一些不能解决问题的事情浪费的时间可能会更多。因而，我们必须有一些能够判断问题所在的方法。这就引出了我们下面的内容。

## 1. 学习算法的调试问题

比如你想建立一个垃圾邮件识别系统。目前的研究现状如下：
- 经过仔细地**筛选**，在50000个特征（词）中选择了具有100个特征的很小的集合，来建立垃圾邮件识别系统
- 使用贝叶斯逻辑回归模型**算法**(BLR)
  $$
    \max_\theta\sum_{i=1}^m\log p(y^{(i)}|x^{(i)},\theta)-\lambda||\theta||^2
  $$
- 使用梯度下降算法实现，目前的**测试**误差率为20%

那么下一步应该做什么？

为了减少误差，改进算法。可能的解决方法如下：
- 提供跟多的训练样本
- 是否使用更少的特征
- 是否使用更多的特征
- 选取的特征是不是不够好，是否能找到更好的特征
- 梯度下降还没有完全收敛，可以多迭代几次
- 尝试其他的算法，例如牛顿方法。测试是否收敛性更好
- 调整贝叶斯逻辑回归模型中的$\lambda$
-  尝试用SVM算法，测试是否比逻辑回归算法更好

实际上可能的方法数以百计，我们只列出这8项。其中有些方法肯定有效， 如果一一尝试的话非常耗时。

面对这8种方法，如何选择改进的方向呢？只能靠运气吗？当然不是，如果我们能找到几种标准，排除上面大部分的方向，只保留1、2个，那么就可以节省很多时间。
### 1.1 方差/偏差分析
第一个标准就是判定问题是出在高方差还是高偏差上。一般来说，高方差(high variance)针对的是过拟合问题，即训练误差很小但泛化误差很大。而高偏差（high bias）针对的是模型本身不适合的问题，如特征数目过少等问题，表现既是训练误差和泛化误差都很大。示意图如下：

那么如何判断是高方差还是高偏差呢？

![](https://raw.githubusercontent.com/fray-hao/images/master/20190327093905.png)
由图1的高方差学习曲线可以看到，在高方差下，随着样本数的增加测试误差会呈下降趋势，而训练误差随着样本数的增加会单调上升，因为随着测试点的增加，它越来越难拟合出完美的曲线。**高方差的诊断方法**就是：在高方差下可以比较训练误差和测试误差，如果它们之间相差很大，那么可以通过增加样本数目使得模型的过拟合程度减少，从而提高性能。

由图2的高偏差学习曲线可以看到，在高偏差下，样本增加到一定程度后，测试误差就不变了，即使增加再多的样本数，误差程度也不会进一步缩小了；而训练误差会随着样本数的增加而递增。所以，在高偏差下，当你发现训练误差超过预期误差后，即使增加再多的样本，也不能把训练误差拉倒预期误差之下。

回到前面的修正措施列表，我们可以将前4条进行分类
- 提供跟多的训练样本 ： **可以修正高方差**
- 使用更少的特征：**可以修正高方差**
- 是否使用更多的特征： **可以修正高偏差**
- 找到更好的特征： **可以修正高偏差**

更多的样本使拟合变得平衡，可以解决高方差的问题；更少的特征也可以降低过拟合的程度，解决高方差问题。更多的特征和更好的特征可以增加模型的复杂度，提高模型在数据上的拟合程度，从而解决高偏差问题。

如果能够区别是高方差还是高偏差就可以排除掉上面四项措施中的2种，从而节省大量时间。


###1.2 收敛与目标函数是否正确的问题

考虑下面的例子，仍是针对垃圾邮件判断问题。
- 使用贝叶斯逻辑回归模型可以达到正常邮件（non spam）上2%的错误率，但是判断垃圾邮件上也有2%的错误率（也就是说2%的可能正常的邮件也被过滤了）。
- 使用svm和线性核算法可以达到判断正常邮件上10%的错误率，判断垃圾邮件上有0.1%的错误率

这个例子说明的是，你建立垃圾邮件判别系统时，你特别希望使用逻辑回归实现，因为它计算效率很高，或者需要随时进行学习，或者因为逻辑回归运行更简单、更快速。但是，它的运行效果并不好，将2%的邮件误判为垃圾邮件是不可接受的。

所以，应该怎么办？伴随着这个问题的是算法的收敛性。

你可能怀疑BLR没有收敛，可以再进行多次迭代。实际上，如果你考虑逻辑回归的优化目标。比如，逻辑回归是J(θ)的话，如果将这个函数作为迭代次数的函数，迭代次数与目标函数的趋势图如下：
![](https://raw.githubusercontent.com/fray-hao/images/master/20190327122354.png)
但是这样的趋势图在通常情况下不能分辨出目标函数是否已收敛。因为在训练的后期目标函数的每步优化往往都只能提高一点点。所以相对于查看曲线的方法，我们需要更好的判断。
https://blog.csdn.net/stdcoutzyx/article/details/18500441