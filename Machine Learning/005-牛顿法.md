除了用梯度上升对logistic回归模型进行拟合外，还可以用一种名为牛顿方法的算法。

例如：对于一个函数$f(\theta)$,想要找到一个$\theta$值，使得$f(\theta)$等于0.

牛顿算法是这样的：
1. 用某些值初始化$\theta$，叫做$\theta^{(0)}$
2. 之后，在这一点上对f求值
3. 然后对这一点求导，画切线，直到它与水平轴相交，这个相交的点叫做$\theta^{(1)}$
4. 然后重复步骤2-3

![image.png](https://upload-images.jianshu.io/upload_images/13764292-723a85af4f3d3738.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

从这个算法中，我们获得：

```math
\begin{aligned}
&f'(\theta^{(0)}) =\frac{f(\theta^{(0)})}{\Delta}
\\&\Delta=\frac{f(\theta^{(0)})}{f'(\theta^{(0)})}
\\&\theta^{(1)}=\theta^{(0)}-\frac{f(\theta^{(0)})}{f'(\theta^{(0)})}
\end{aligned}
```

归纳起来：
```math
\begin{aligned}
&\theta^{(t+1)}=\theta^{(t)}-\frac{f(\theta^{(t)})}{f'(\theta^{(t)})}
\end{aligned}
```
这就是牛顿法的一次迭代，这个算法可以找到一个$\theta$值，使得$f(\theta)$等于0.

接下来，要想使得对数似然率最大化。也就是找到一个$\theta$使得$\mathscr{l}'(\theta)=0$。

使用牛顿法：
```math
\begin{aligned}
&\theta^{(t+1)}=\theta^{(t)}-\frac{\mathscr{l}'(\theta^{(t)})}{\mathscr{l}''(\theta^{(t)})}
\end{aligned}
```

牛顿迭代法是一个收敛速度非常快的算法，它是一种二次收敛，每一次迭代后，误差会成平方的小。

上面的牛顿法表示的是一个特例，$\theta$是只有一行的数字，泛化的牛顿方法中，$\theta$是一个向量，所以泛化的牛顿方法为：

```math
\begin{aligned}
&\theta^{(t+1)}=\theta^{(t)}-H^{-1}\bigtriangledown_\theta\mathcal{l}
\end{aligned}
```
其中，H是一个黑塞矩阵（Hessian Matrix）：
```math
\begin{aligned}
&H_{ij}=\frac{\partial^2\mathscr{l}}{\partial\theta_i\partial\theta_j}
\end{aligned}
```

牛顿迭代法，一般只需要迭代10几次，和梯度上升比起来，它收敛的速度非常快，但是，它的缺点是：每一次迭代都需要计算一次黑塞矩阵的逆。黑塞矩阵是一个n*n的矩阵，如果要处理的问题中有大量的特征，求黑塞矩阵的逆需要花费很大的代价。对于规模较小，特征数量合理的问题，通常情况下会是一个很快的算法。