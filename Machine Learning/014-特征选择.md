回顾上一讲的内容，
![](https://raw.githubusercontent.com/fray-hao/images/master/20190318102330.png)
ERM是一个简化的机器学习的算法模型，对于一般假设类H，ERM算法做的是从假设类中选择具有最小训练误差的假设。$\epsilon(\hat{h})$符号表示训练误差，它表示一个假设h，是同一训练集合中样本形成的分布中样本错误分类的概率。为了保证假设的一般误差，小于等于假设类中最佳的一般误差加上两倍的误差阈值，为了保证至少在$1-\delta$概率下成立，需要的m的规模。这是一个关于样本复杂度的结论，因为给出了为了满足误差率的要求，所需的训练样本的数目的界限

接下来要做的是将这个结论推广到无限假设类。将这个结果应用到逻辑回归中，比如说你的假设类H是由线性决策边界构成的，它有d个实数参数，如果回归解决的是包含n个特征的问题，那么d应该是n+1。如果假设类以d个实数为参数，那么就需要64d个位（实数一般用双精度浮点数来表示）来表示这些参数。所以假设类H的size，在计算机表示时，可以拥有的状态的数：
$$
\begin{aligned}
    k = |H| = 2^{64d}
\end{aligned}
$$

将这个公式代入上面的结论：

$$
\begin{aligned}
   m&\geq O(\frac{1}{\gamma^2}\log\frac{2^{64d}}{\delta})
   \\& = O(\frac{d}{\gamma^2}\log\frac{1}{8})
\end{aligned}
$$
这这就是，假设你需要保证ERM生成的假设和最好的假设之间的差异不超过两个$\gamma$。你需要的m的数量级。这个结果表明，你所需的训练样本的数目大致和你的假设类的参数数目呈线性关系。

上面的结论并不严谨，它与计算机的浮点运算有关，接下来，我们将更为正式的正确的论述方式，因为它包含大量的证明。

## 1. VC维(Vapnik-Chervonenkis Dimension)
首先给定一个定义：
给定一个由d个点构成的集合：
$$
S = \begin{Bmatrix}
    x^{(1)},x^{(2)},...,x^{(d)}
\end{Bmatrix}
$$
如果一个假设类H，对S的任意一种标记方式都能实现完美的预测，那么我们认为一个假设类H能够shatters（分散）S。完美的预测是指：不管对S中的数据如何标注，H中都存在假设h，使得h对S的分类误差为0.
例如，假设H是一个包含了所有二维线性分类器的假设类，而S是包含两个个训练样本的点的集合。对于这两个点，有四种可能标记方式。如下图所示，对S的任意一种标记方式，都存在一个二维的线性分类器能够对它们完美预测，使得其训练误差为0：
![](https://raw.githubusercontent.com/fray-hao/images/master/20190319082124.png)
所以，我们说假设类H能够分散这个由两个点组成的集合S。
再举一个例子，这是一个更大的训练集合。集合S由三个点构成，有8中标记方式。对于每一种标记方式，都可以从假设类中找到一个假设能够对这些样本进行完美分类。根据定义，完美说假设H能够分散集合S

但是对于所有含有四个样本的训练集S，不存在二维线性分类器能够实现对其任意标记的完美预测。

**VC维的定义**
>一个假设类H的VC维（VC(H)）是指能够被H  shatter的最大集合的大小。

例如，假设H是由所有二维线性分类器构成的假设类，那么它的VC维应该是3：
$$
VC(H)=3
$$
https://blog.csdn.net/linkin1005/article/details/43018827

https://www.cnblogs.com/madrabbit/p/7099557.html