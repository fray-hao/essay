回顾上一讲的内容，
![](https://raw.githubusercontent.com/fray-hao/images/master/20190318102330.png)
ERM是一个简化的机器学习的算法模型，对于一般假设类H，ERM算法做的是从假设类中选择具有最小训练误差的假设。$\epsilon(\hat{h})$符号表示训练误差，它表示一个假设h，是同一训练集合中样本形成的分布中样本错误分类的概率。为了保证假设的一般误差，小于等于假设类中最佳的一般误差加上两倍的误差阈值，为了保证至少在$1-\delta$概率下成立，需要的m的规模。这是一个关于样本复杂度的结论，因为给出了为了满足误差率的要求，所需的训练样本的数目的界限

接下来要做的是将这个结论推广到无限假设类。将这个结果应用到逻辑回归中，比如说你的假设类H是由线性决策边界构成的，它有d个实数参数，如果回归解决的是包含n个特征的问题，那么d应该是n+1。如果假设类以d个实数为参数，那么就需要64d个位（实数一般用双精度浮点数来表示）来表示这些参数。所以假设类H的size，在计算机表示时，可以拥有的状态的数：
$$
\begin{aligned}
    k = |H| = 2^{64d}
\end{aligned}
$$

将这个公式代入上面的结论：

$$
\begin{aligned}
   m&\geq O(\frac{1}{\gamma^2}\log\frac{2^{64d}}{\delta})
   \\& = O(\frac{d}{\gamma^2}\log\frac{1}{8})
\end{aligned}
$$
这这就是，假设你需要保证ERM生成的假设和最好的假设之间的差异不超过两个$\gamma$。你需要的m的数量级。这个结果表明，你所需的训练样本的数目大致和你的假设类的参数数目呈线性关系。

上面的结论并不严谨，它与计算机的浮点运算有关，接下来，我们将更为正式的正确的论述方式，因为它包含大量的证明。

## 1. VC维(Vapnik-Chervonenkis Dimension)
首先给定一个定义：
给定一个由d个点构成的集合：
$$
S = \begin{Bmatrix}
    x^{(1)},x^{(2)},...,x^{(d)}
\end{Bmatrix}
$$
如果一个假设类H，对S的任意一种标记方式都能实现完美的预测，那么我们认为一个假设类H能够shatters（分散）S。完美的预测是指：不管对S中的数据如何标注，H中都存在假设h，使得h对S的分类误差为0.
例如，假设H是一个包含了所有二维线性分类器的假设类，而S是包含两个个训练样本的点的集合。对于这两个点，有四种可能标记方式。如下图所示，对S的任意一种标记方式，都存在一个二维的线性分类器能够对它们完美预测，使得其训练误差为0：
![](https://raw.githubusercontent.com/fray-hao/images/master/20190319082124.png)
所以，我们说假设类H能够分散这个由两个点组成的集合S。
再举一个例子，这是一个更大的训练集合。集合S由三个点构成，有8中标记方式。对于每一种标记方式，都可以从假设类中找到一个假设能够对这些样本进行完美分类。
>根据定义，完美就是说假设H能够分散集合S

![](https://raw.githubusercontent.com/fray-hao/images/master/20190321085923.png)
但是对于所有含有四个样本的训练集S，不存在二维线性分类器能够实现对其任意标记的完美预测。

**VC维的定义**
>一个假设类H的VC维（VC(H)）是指能够被H  shatter的最大集合的大小。

例如，假设H是由所有二维线性分类器构成的假设类，那么它的VC维应该是3：
$$
VC(H)=3
$$


VC维是3并不是说所有的三个点的样本都能正确分类：
![](https://raw.githubusercontent.com/fray-hao/images/master/20190321090127.png)
只要存在大小为3，可以被分散的集合就可以说VC维是3.

推广到更一般的情形，如果你的线性分类器是n维的，那么VC维应该等于n+1。可以写出这个结论：

给定一个假设类H（可以是无限假设类）， VC(H)= d，那么至少在$1-\delta$的概率下，对所有的假设h，有:
$$
|\epsilon(h)-\hat{\epsilon}(h)|\leq O(\frac{d}{m}\log\frac{m}{d}+\frac{1}{m}\log\frac{1}{\delta})
$$
由此可以推出，至少在$1-\delta$的概率下，有:
$$
\epsilon(\hat{h})\leq\epsilon(h^*)+O(\frac{d}{m}\log\frac{m}{d}+\frac{1}{m}\log\frac{1}{\delta})
$$
> 一般误差和训练误差相差不大，那么你选选择的假设的一般误差$\epsilon(\hat{h})$小于等于最好的一般误差$\epsilon(h^*)$加上两个$\gamma$(这里用大O省略了2)

将它写出一个引理（rewrite this as a corollary）：
为了保证$\epsilon(\hat{h})\leq\epsilon(h^*)+2\gamma$这个结论至少在$1-\delta$的概率下成立，充分条件为：
$$
m= O_{\gamma,\delta}(d)
$$
> 样本复杂度和假设类的VC维呈线性关系

我们可以得出结论，如果你的学习算法尝试最小化训练误差，一种直观理解是，你需要的训练样本数大概和假设类的VC维呈线性关系。这表明样本复杂度的上界是由VC维给定的。

事实证明，对于大多数的合理的假设类。VC维大概和你模型的参数数量差不多。例如，你要用模型进行逻辑回归，维度为n为的逻辑回归，回归需要n+1个参数，而n维线性分类器的假设类的vc维是n+1。所以实际上，VC维总是和模型参数的数量成正比。因此，直观情况下，为了拟合出模型所需要的训练样本的数目大致和模型的参数呈线性关系。

对于SVM来说，即使我们将输入特征映射到无限维的特征空间，按照正常逻辑来说，SVM的VC维应该是无穷大。但是事实证明，具有较大间隔的SVM分类器，都具有较小的VC维。
例如，假如我们只考虑半径为R以内的数据点，同时只考虑那些间隔大于$\gamma$的假设，那么这样的假设类H的VC维具有如下的上界：
$$
VC(H)=\leq\Bigg\lceil\frac{R^2}{4\gamma^2}\Bigg\rceil +1
$$
> ⌈x⌉ 表示向上取整

因此包含较大间隔线性SVM分类器的假设的假设类的VC维不依赖于X的维度 。换句话说，虽然你的数据点位于无限维的空间中，但是只要考虑具有较大间隔的分类器所组成的假设类，那么vc维就存在上界。svm会自动地尝试找到一个具有较小vc维的假设类，所以它不会过拟合。


---
实际上，我们目前所学到的所有理论都是基于ERM的。我们现在只考虑一个样本。对于特定的训练样本，要么是正样本，要么是负样本。根据指示函数，如果有一个负样本（y=0），如果z大于0，那么你的预测是错误的，如果z小于0，那么你对该样本的预测是正确的。
![](https://raw.githubusercontent.com/fray-hao/images/master/20190321140624.png)
实际上，我们要做的就是选择参数，最小化整个阶梯函数。这个函数不是一个凸函数，线性分类器使训练误差最小是一个NP难的问题。
事实上，逻辑回归和svm都可以看做是这个问题的一种凸性近似。

实际上逻辑回归尝试最大化似然性，也就是对数似然性最小。如果将负样本的对数似然性图形画出来，你会发现它是一个这样的曲线：
![](https://raw.githubusercontent.com/fray-hao/images/master/20190321141248.png)
可以将这个曲线看作是对阶梯函数的近似。这是你真正最小
化的。实际上就是近似地在最小化训练误差。可以认为逻辑回归是一种ERM的近似。它没有使用阶梯函数，因为它不是凸性的，这使得优化问题的求解非常困难，它使用上面的曲线，通过这种近似，就得到了一个凸优化问题，可以找到参数使得逻辑回归的似然性最大。

实际上，SVM也可以看做是一种近似。SVM可以看做是尝试使用两段不同的线性函数进行近似
![](https://raw.githubusercontent.com/fray-hao/images/master/20190321141855.png)
逻辑回归和SVM是两种不同近似，都是为了近似最小化整个阶梯函数。

## 2. 模型选择

https://www.cnblogs.com/madrabbit/p/7099557.html

https://blog.csdn.net/linkin1005/article/details/43018827