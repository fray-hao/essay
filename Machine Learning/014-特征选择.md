回顾上一讲的内容，
![](https://raw.githubusercontent.com/fray-hao/images/master/20190318102330.png)
ERM是一个简化的机器学习的算法模型，对于一般假设类H，ERM算法做的是从假设类中选择具有最小训练误差的假设。$\epsilon(\hat{h})$符号表示训练误差，它表示一个假设h，是同一训练集合中样本形成的分布中样本错误分类的概率。为了保证假设的一般误差，小于等于假设类中最佳的一般误差加上两倍的误差阈值，为了保证至少在$1-\delta$概率下成立，需要的m的规模。这是一个关于样本复杂度的结论，因为给出了为了满足误差率的要求，所需的训练样本的数目的界限

接下来要做的是将这个结论推广到无限假设类。将这个结果应用到逻辑回归中，比如说你的假设类H是由线性决策边界构成的，它有d个实数参数，如果回归解决的是包含n个特征的问题，那么d应该是n+1。如果假设类以d个实数为参数，那么就需要64d个位（实数一般用双精度浮点数来表示）来表示这些参数。所以假设类H的size，在计算机表示时，可以拥有的状态的数：
$$
\begin{aligned}
    k = |H| = 2^{64d}
\end{aligned}
$$

将这个公式代入上面的结论：

$$
\begin{aligned}
   m&\geq O(\frac{1}{\gamma^2}\log\frac{2^{64d}}{\delta})
   \\& = O(\frac{d}{\gamma^2}\log\frac{1}{8})
\end{aligned}
$$
这这就是，假设你需要保证ERM生成的假设和最好的假设之间的差异不超过两个$\gamma$。你需要的m的数量级。这个结果表明，你所需的训练样本的数目大致和你的假设类的参数数目呈线性关系。

上面的结论并不严谨，它与计算机的浮点运算有关，接下来，我们将更为正式的正确的论述方式，因为它包含大量的证明。

## 1. VC维(Vapnik-Chervonenkis Dimension)
首先给定一个定义：
给定一个由d个点构成的集合：
$$
S = \begin{Bmatrix}
    x^{(1)},x^{(2)},...,x^{(d)}
\end{Bmatrix}
$$
如果一个假设类H，对S的任意一种标记方式都能实现完美的预测，那么我们认为一个假设类H能够shatters（分散）S。完美的预测是指：不管对S中的数据如何标注，H中都存在假设h，使得h对S的分类误差为0.
例如，假设H是一个包含了所有二维线性分类器的假设类，而S是包含两个个训练样本的点的集合。对于这两个点，有四种可能标记方式。如下图所示，对S的任意一种标记方式，都存在一个二维的线性分类器能够对它们完美预测，使得其训练误差为0：
![](https://raw.githubusercontent.com/fray-hao/images/master/20190319082124.png)
所以，我们说假设类H能够分散这个由两个点组成的集合S。
再举一个例子，这是一个更大的训练集合。集合S由三个点构成，有8中标记方式。对于每一种标记方式，都可以从假设类中找到一个假设能够对这些样本进行完美分类。
>根据定义，完美就是说假设H能够分散集合S

![](https://raw.githubusercontent.com/fray-hao/images/master/20190321085923.png)
但是对于所有含有四个样本的训练集S，不存在二维线性分类器能够实现对其任意标记的完美预测。

**VC维的定义**
>一个假设类H的VC维（VC(H)）是指能够被H  shatter的最大集合的大小。

例如，假设H是由所有二维线性分类器构成的假设类，那么它的VC维应该是3：
$$
VC(H)=3
$$


VC维是3并不是说所有的三个点的样本都能正确分类：
![](https://raw.githubusercontent.com/fray-hao/images/master/20190321090127.png)
只要存在大小为3，可以被分散的集合就可以说VC维是3.

推广到更一般的情形，如果你的线性分类器是n维的，那么VC维应该等于n+1。可以写出这个结论：

给定一个假设类H（可以是无限假设类）， VC(H)= d，那么至少在$1-\delta$的概率下，对所有的假设h，有:
$$
|\epsilon(h)-\hat{\epsilon}(h)|\leq O(\frac{d}{m}\log\frac{m}{d}+\frac{1}{m}\log\frac{1}{\delta})
$$
由此可以推出，至少在$1-\delta$的概率下，有:
$$
\epsilon(\hat{h})\leq\epsilon(h^*)+O(\frac{d}{m}\log\frac{m}{d}+\frac{1}{m}\log\frac{1}{\delta})
$$
> 一般误差和训练误差相差不大，那么你选选择的假设的一般误差$\epsilon(\hat{h})$小于等于最好的一般误差$\epsilon(h^*)$加上两个$\gamma$(这里用大O省略了2)

将它写出一个引理（rewrite this as a corollary）：
为了保证$\epsilon(\hat{h})\leq\epsilon(h^*)+2\gamma$这个结论至少在$1-\delta$的概率下成立，充分条件为：
$$
m= O_{\gamma,\delta}(d)
$$
> 样本复杂度和假设类的VC维呈线性关系

我们可以得出结论，如果你的学习算法尝试最小化训练误差，一种直观理解是，你需要的训练样本数大概和假设类的VC维呈线性关系。这表明样本复杂度的上界是由VC维给定的。

事实证明，对于大多数的合理的假设类。VC维大概和你模型的参数数量差不多。例如，你要用模型进行逻辑回归，维度为n为的逻辑回归，回归需要n+1个参数，而n维线性分类器的假设类的vc维是n+1。所以实际上，VC维总是和模型参数的数量成正比。因此，直观情况下，为了拟合出模型所需要的训练样本的数目大致和模型的参数呈线性关系。

对于SVM来说，即使我们将输入特征映射到无限维的特征空间，按照正常逻辑来说，SVM的VC维应该是无穷大。但是事实证明，具有较大间隔的SVM分类器，都具有较小的VC维。
例如，假如我们只考虑半径为R以内的数据点，同时只考虑那些间隔大于$\gamma$的假设，那么这样的假设类H的VC维具有如下的上界：
$$
VC(H)=\leq\Bigg\lceil\frac{R^2}{4\gamma^2}\Bigg\rceil +1
$$
> ⌈x⌉ 表示向上取整

因此包含较大间隔线性SVM分类器的假设的假设类的VC维不依赖于X的维度 。换句话说，虽然你的数据点位于无限维的空间中，但是只要考虑具有较大间隔的分类器所组成的假设类，那么vc维就存在上界。svm会自动地尝试找到一个具有较小vc维的假设类，所以它不会过拟合。


---
实际上，我们目前所学到的所有理论都是基于ERM的。我们现在只考虑一个样本。对于特定的训练样本，要么是正样本，要么是负样本。根据指示函数，如果有一个负样本（y=0），如果z大于0，那么你的预测是错误的，如果z小于0，那么你对该样本的预测是正确的。
![](https://raw.githubusercontent.com/fray-hao/images/master/20190321140624.png)
实际上，我们要做的就是选择参数，最小化整个阶梯函数。这个函数不是一个凸函数，线性分类器使训练误差最小是一个NP难的问题。
事实上，逻辑回归和svm都可以看做是这个问题的一种凸性近似。

实际上逻辑回归尝试最大化似然性，也就是对数似然性最小。如果将负样本的对数似然性图形画出来，你会发现它是一个这样的曲线：
![](https://raw.githubusercontent.com/fray-hao/images/master/20190321141248.png)
可以将这个曲线看作是对阶梯函数的近似。这是你真正最小
化的。实际上就是近似地在最小化训练误差。可以认为逻辑回归是一种ERM的近似。它没有使用阶梯函数，因为它不是凸性的，这使得优化问题的求解非常困难，它使用上面的曲线，通过这种近似，就得到了一个凸优化问题，可以找到参数使得逻辑回归的似然性最大。

实际上，SVM也可以看做是一种近似。SVM可以看做是尝试使用两段不同的线性函数进行近似
![](https://raw.githubusercontent.com/fray-hao/images/master/20190321141855.png)
逻辑回归和SVM是两种不同近似，都是为了近似最小化整个阶梯函数。


## 2. 模型选择

偏差和方差之间存在着权衡。如果你的数据中包含二次结构，但你用线性函数，尝试近似它，那么会导致欠拟合，所以你的假设的偏差很高；相反地，如果选择过于复杂的模型，那么方差会很高，可能会导致过拟合，模型的一般实用性不强。

模型选择算法提供了一类方法，可以自动地在偏差和方差之间进行权衡，

在上节的一般误差图中：
![](https://raw.githubusercontent.com/fray-hao/images/master/20190325081644.png)

如果模型模型过于简单，会得到一个较高的一般误差，这实际上就是欠拟合；如果模型过于复杂，例如14次的多项式来拟合，你们你会得到一个较高的一般误差，这实际上就是过拟合。
我们要做的就是抽象的进行选择。
- (A). 我们可能选择不同的多项式
  $$
  \begin{aligned}
      &\theta_0+\theta_1x
      \\&\theta_0+\theta_1x+\theta_2x^2
      \\&\theta_0+\theta_1x+...+\theta_nx^n
  \end{aligned}
  $$
- (B). 我们可能选择带宽参数（bandwidth parameter）也就是局部加权线性回归或其他局部加权 
- (C). 选择SVM中的参数C。参数C控制了样本会以多大的间隔被分割。对于错误的分类，样本惩罚力度是多少。

上述三个模型是选择问题的具体例子。我们这里将提出一个自动选择模型的方法。

假设有一个包含有限个模型的集合。
$$
\mu = \begin{Bmatrix}
    m_1,m_2,m_3,...
\end{Bmatrix}
$$
其中，$m_1$可能是$\theta_0+\theta_1x$; 而$m_2$可能是$\theta_0+\theta_1x+\theta_2x^2$。当然，这个集合也可以表示不同的带宽参数

我们要做的是，对于所有的模型，用训练集合训练它们，之后看看那个模型具有最小的训练误差。这是一个糟糕的注意。因为有可能存在过拟合。

选择模型时，有很多标准的方法。

-  Hold-out cross validation（保留交叉验证）
    1. split S into two subset :
       -  $S_{train}$子集占70% 
       -  $S_{cv}$子集占30% 
    2. 然后用训练子集$S_{train}$训练模型，之后用保留交叉验证子集$S_{cv}$进行测试
    3.  最后，选择具有最小测试误差的模型作为结果。
    4.  选择了合适的模型后，还可以用100%的数据重新训练一次模型。

在实际应用中，每个得到的训练样本都需要很大的代价来获得。比如，有些适合你的数据是通过医学实验获得的，每个样本都代表着病人的病痛或一些不好的经历。仅仅为了进行验证模型就需要保留30%的样本，这通常令人不可接受。

有几种交叉验证的变种，可以更为高效的利用数据。
- k-fold cross validation（k重交叉验证）
  - 把S分成k（通常k=10）部分，每次用k-1个部分进行训练，用剩下的一个部分用来验证。
  - 轮流交换验证的部分。最后对k个结果取平均。 

k重交叉验证的缺陷是，它需要大量的计算。 具体来说，为了验证你的模型，你需要训练10次，而不是仅训练1次。

- leave-one-out cross validation（留一交叉验证）
  根据样本数目进行划分，令k=m。它是k重交叉验证的特例。每次只留出一个样本进行验证。如果样本数量非常少，可以采用这种方法。

> 学习理论的界是一个很宽松的界。它考虑的是最坏的情况，是满足任何分布的。

> 一个经验法则： 如果你有n个参数，那么训练样本数是参数的10倍的话，你们很可能会拟合出不错的形状。

## 特征选择(feature selection)
特征选择是模型选择的一个特例。

对于许多机器学习的问题，可能会面对一个非常高维的特征空间,输入特征向量x的维数可能非常高。例如，对于文本分类，这个问题的特征空间很容易达到30000或50000。那么如果你有这么多特征，根据你用的学习算法，可能存在过拟合的风险。

如果能减少特征的数量，也许就可以减少学习算法的方差，从而降低过拟合的风险。对于文本分类这个例子，只有少量的相关特征（relevant features），所以这样会有少得多的特征是和学习问题真正相关的。
在特征选择问题中，我们会从原始的特征中，选出一个子集。我们认为这个子集对于特定的学习问题来说是最为相关的，这样我们就有了一个更为简单的假设类，可以防止过拟合的风险。

我们应该怎么做呢？
 
对n个特征来说，每个特征都存在选择和不选择的情况，因此特征选择一共存在$2^n$个子集。在进行模型选取时，我们通常会使用不同的启发式规则进行搜索，尝试在所有可能的子集构成的空间中进行搜索。这个空间太大了，不可能一一枚举

来看一个具体的例子，这个算法叫做前向搜索算法（forward search）

![](https://raw.githubusercontent.com/fray-hao/images/master/20190325101511.png)

前向搜索算法，是从空集F开始，每次迭代都考虑从那些尚未被选择的特征中选出最好的一个特征添加到F中

后向搜索算法 （Backward search）前向搜索算法的变种：
![](https://raw.githubusercontent.com/fray-hao/images/master/20190325102555.png)
当样本数远小于特征数时，该算法可能没有意义，因为会导致过拟合。

前向搜索和后向搜索 统称为 “封装（wrapper）”特征选择算法。封装这个词的意思是指前向或后向算法想一个包装一样封装在学习算法外面。这意味着，当你进行前向或反向选择的时候，需要重复使用学习算法去训练模型，根据其结果选择特征子集。这种算法在计算能力满足要求的情况下，性能是很不错的。但是对于文本分类这样的问题，特征数目特别多，前向选择的计算量会特别大。

前向搜索和后向搜索都是一种启发式搜索，不能保证它们会找到最好的特征子集。

对于特征维度非常高的样本，可以采用"过滤（Filter）"特征选择算法
$$
\begin{aligned}
   & \text{for each featuer i}
\\&\text{compute some measure of how informative }x_i\text{is about y}
\end{aligned}
$$
另外一种测量方法是计算$x_i$和y的互信息（mutual information）：
![](https://raw.githubusercontent.com/fray-hao/images/master/20190325120705.png)
当$x_i$和y相互独立时，$x_i$和y的联合概率分布与$x_i$和y的概率分布乘积得到的概率分布相同，其KL散度为0,表明它们没有关联度。当关联度很大的时候，KL值也会很大

衡量$x_i$与y的相关度后，最后选择相关度最高的k个特征。


https://www.cnblogs.com/madrabbit/p/7099557.html

https://blog.csdn.net/linkin1005/article/details/43018827