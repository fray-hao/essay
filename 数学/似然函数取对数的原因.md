## 1. 减少计算量，计算更方便

在计算一个独立同分布数据集的联合概率时，如：
$$
X=\begin{Bmatrix}
  x_1,x_2,...,x_n
\end{Bmatrix}
$$

其联合概率是每个数据点概率的连乘：
$$
p(X|\Theta) = \Pi_{i=1}^n p(x_i|\Theta)
$$

两边取对数可以将连乘转化为连加：
$$
\ln p(x|\Theta) = \sum_{i=1}^n\ln p(x_i|\Theta)
$$

乘法变成加法，从而减少了计算量；同时，如果概率中含有指数项，如高斯分布，能把指数项也化为求和形式，进一步减少计算量；另外，在对联合概率求导时，和的形式会比积的形式更方便。

但其实可能更重要的一点是，因为概率值都在[0,1]之间，因此，概率的连乘将会变成一个很小的值，可能会引起浮点数下溢，尤其是当数据集很大的时候，联合概率会趋向于0，非常不利于之后的计算。
## 取对数不影响单调性
$$
p(x∣Θ_{1})>p(x∣Θ_{2})⇔\ln p(x∣Θ_{1})>\ln p(x∣Θ_{2})
$$
![](https://raw.githubusercontent.com/fray-hao/images/master/20190404090055.png)
因为相同的单调性，它确保了概率的最大对数值出现在与原始概率函数相同的点上。因此，可以用更简单的对数似然来代替原来的似然。

<https://math.stackexchange.com/questions/892832/why-we-consider-log-likelihood-instead-of-likelihood-in-gaussian-distribution>

---
1. It is extremely useful for example when you want to calculate the joint likelihood for a set of independent and identically distributed points. Assuming that you have your points:
   $$X=\{x_1,x_2,\ldots,x_N\}$$
The total likelihood is the product of the likelihood for each point, i.e.:
$$p(X\mid\Theta)=\prod_{i=1}^Np(x_i\mid\Theta)$$
where Θ are the model parameters: vector of means μ and covariance matrix Σ. If you use the log-likelihood you will end up with sum instead of product:
$$
\ln p(X\mid\Theta)=\sum_{i=1}^N\ln
p(x_i\mid\Theta)
$$
2. Also in the case of Gaussian, it allows you to avoid computation of the exponential:
   $$
   p(x\mid\Theta) =
\dfrac{1}{(\sqrt{2\pi})^d\sqrt{\det\Sigma}}e^{-\frac{1}{2}(x-\mu)^T \Sigma^{-1}(x-\mu)}
   $$
Which becomes:
$$
\ln p(x\mid\Theta) = -\frac{d}{2}\ln(2\pi)-\frac{1}{2}\ln(\det
\Sigma)-\frac{1}{2}(x-\mu)^T\Sigma^{-1}(x-\mu)
$$
3. Like you mentioned lnx is a monotonically increasing function, thus log-likelihoods have the same relations of order as the likelihoods:
   $$p(x\mid\Theta_1)>p(x\mid\Theta_2) \Leftrightarrow \ln
p(x\mid\Theta_1)>\ln p(x\mid\Theta_2)$$
4. From a standpoint of computational complexity, you can imagine that first of all summing is less expensive than multiplication (although nowadays these are almost equal). But what is even more important, likelihoods would become very small and you will run out of your floating point precision very quickly, yielding an underflow. That's why it is way more convenient to use the logarithm of the likelihood. Simply try to calculate the likelihood by hand, using pocket calculator - almost impossible.
Additionally in the classification framework you can simplify calculations even further. The relations of order will remain valid if you drop the division by 2 and the dln(2π) term. You can do that because these are class independent. Also, as one might notice if variance of both classes is the same (Σ1=Σ2), then you can also remove the ln(detΣ) term.