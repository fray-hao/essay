```python

import  re
import  requests
import  os
from multiprocessing.dummy import Pool
url = "https://www.kanunu8.com/book3/6879/"

def get_source(url):
    """
    获取网页源代码
    Args:
        url:网页

    Returns:网页源代码
    """
    return  requests.get(url).content.decode("gb2312")

def get_toc(html):
    """
    获取每一章的链接
    Args:
        html:目录页源代码

    Returns: 列表形式存储的每章链接。

    """
    toc_url_list =[]
    toc_block = re.findall('正文(.*?)</tbody>', html, re.S)[0]
    top_url = re.findall('href="(.*?)"',toc_block,re.S)

    for href in top_url:
        toc_url_list.append(url + href)

    return  toc_url_list

def get_article(html):
    """
    获取文章的章节名和正文
    Args:
        html: 正文源代码

    Returns: 章节名，正文

    """
    chapter_name = re.search('size="4">(.*?)</font>', html, re.S).group(1)
    text_block = re.search('<p>(.*?)</p>',html,re.S).group(1)
    text = text_block.replace("<br />","")
    return  chapter_name,text
    return  chapter_name,text

def save(chapter,article):
    """
    将文章保存到本地
    Args:
        chapter: 文章名
        article: 正文

    Returns:

    """
    path = '/Users/free.hao/programe/python/crawler/动物农场'
    os.makedirs(path,exist_ok=True)
    with open(os.path.join(path,chapter+'.txt'),'w',encoding='utf_8_sig') as f:
        f.write(article)

def qurey_article(url):
    """
    根据正文网址获取正文内容并保存到本地
    Args:
        url:

    Returns:

    """
    html = get_source(url)
    charpter, article = get_article(html)
    save(charpter,article)

if __name__=='__main__':
    toc_html = get_source(url)
    # 获取章节的url
    toc_list = get_toc(toc_html)
    pool = Pool(4)
    pool.map(qurey_article,toc_list)

```